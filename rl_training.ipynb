{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Reinforcement Learning Training for AI Predator-Prey Ecosystem\n",
        "\n",
        "**üéØ End-to-End RL Training on Google Colab A100**\n",
        "\n",
        "This notebook implements reinforcement learning training for the transformer-based predator, building on the existing supervised learning foundation. Key features:\n",
        "\n",
        "- **Environment**: 100% identical Python simulation to production JavaScript\n",
        "- **Reward Design**: Sparse end-to-end rewards based on episode completion time\n",
        "- **RL Algorithm**: PPO (Proximal Policy Optimization) with proper credit assignment\n",
        "- **Checkpointing**: Comprehensive checkpoint management with unique naming\n",
        "- **GPU Optimization**: Full A100 utilization with optimized batch processing\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "Supervised Checkpoint ‚Üí RL Environment ‚Üí PPO Training ‚Üí Enhanced Model\n",
        "     ‚Üì                      ‚Üì                ‚Üì             ‚Üì\n",
        "Pre-trained Weights ‚Üí Python Simulation ‚Üí Credit Assignment ‚Üí JavaScript Export\n",
        "```\n",
        "\n",
        "**Episode Definition**: Start with N boids (default 50), episode ends when remaining boids ‚â§ threshold (tunable: 0-50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## üîß Environment Setup\n",
        "\n",
        "First, let's verify GPU availability and download the simulation code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Verification and Initial Setup\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üî• GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"üî¢ CUDA Cores: {torch.cuda.get_device_properties(0).multi_processor_count}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: GPU not available, training will be slow!\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"‚úÖ Initial setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Repository and Install Dependencies\n",
        "import subprocess\n",
        "\n",
        "def run_command(command, description):\n",
        "    \"\"\"Run shell command with error handling\"\"\"\n",
        "    print(f\"üì• {description}...\")\n",
        "    try:\n",
        "        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
        "        print(f\"‚úÖ {description} completed\")\n",
        "        return result.stdout\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error in {description}: {e}\")\n",
        "        print(f\"stderr: {e.stderr}\")\n",
        "        raise\n",
        "\n",
        "# Install required packages for RL\n",
        "packages = [\n",
        "    \"torch>=2.0.0\",\n",
        "    \"tensorboard>=2.8.0\", \n",
        "    \"matplotlib>=3.5.0\",\n",
        "    \"tqdm>=4.60.0\",\n",
        "    \"gymnasium>=0.28.0\",  # Updated gym\n",
        "    \"stable-baselines3>=2.0.0\",  # PPO implementation\n",
        "    \"sb3-contrib>=2.0.0\",  # Additional algorithms\n",
        "    \"wandb\",  # For advanced logging\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    run_command(f\"pip install {package}\", f\"Installing {package}\")\n",
        "\n",
        "# Clone the repository\n",
        "repo_url = \"https://github.com/yimianxyz/homepage.git\"\n",
        "branch = \"neuro-predator\"\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(\"homepage\"):\n",
        "    run_command(\"rm -rf homepage\", \"Removing existing repository\")\n",
        "\n",
        "# Clone the specific branch\n",
        "run_command(f\"git clone -b {branch} {repo_url}\", f\"Cloning repository ({branch} branch)\")\n",
        "\n",
        "# Add python_simulation to Python path\n",
        "simulation_path = str(Path(\"homepage/python_simulation\").absolute())\n",
        "if simulation_path not in sys.path:\n",
        "    sys.path.insert(0, simulation_path)\n",
        "\n",
        "# Add pytorch_training to Python path  \n",
        "training_path = str(Path(\"homepage/pytorch_training\").absolute())\n",
        "if training_path not in sys.path:\n",
        "    sys.path.insert(0, training_path)\n",
        "\n",
        "print(f\"üìÅ Repository cloned to: {os.path.abspath('homepage')}\")\n",
        "print(f\"üêç Python paths added:\")\n",
        "print(f\"   - {simulation_path}\")\n",
        "print(f\"   - {training_path}\")\n",
        "\n",
        "# Verify python_simulation import\n",
        "try:\n",
        "    from python_simulation import Simulation, InputProcessor, ActionProcessor, CONSTANTS\n",
        "    print(\"‚úÖ Python simulation imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import python_simulation: {e}\")\n",
        "\n",
        "# Verify pytorch_training import\n",
        "try:\n",
        "    from transformer_model import TransformerPredator\n",
        "    print(\"‚úÖ PyTorch transformer model imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import transformer model: {e}\")\n",
        "\n",
        "print(\"üéØ Environment setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéÆ RL Environment Design\n",
        "\n",
        "**Key Design Principles:**\n",
        "- **Sparse Rewards**: Only terminal rewards based on episode completion time\n",
        "- **End-to-End Learning**: No intermediate rewards to avoid bias\n",
        "- **Configurable Difficulty**: Tunable episode end threshold (remaining boids)\n",
        "- **Proper Credit Assignment**: PPO handles reward distribution to actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RL Environment Wrapper\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import random\n",
        "from typing import Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RLConfig:\n",
        "    \"\"\"RL training configuration\"\"\"\n",
        "    canvas_width: int = 800\n",
        "    canvas_height: int = 600\n",
        "    initial_boids: int = 50\n",
        "    episode_end_threshold: int = 20  # Episode ends when boids <= this value\n",
        "    max_episode_steps: int = 1000    # Maximum steps per episode\n",
        "    time_penalty_scale: float = 0.01  # Penalty for longer episodes\n",
        "    success_reward: float = 100.0     # Reward for successful completion\n",
        "    \n",
        "    # Curriculum learning parameters\n",
        "    curriculum_enabled: bool = True\n",
        "    curriculum_stages: list = None\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.curriculum_stages is None:\n",
        "            # Progressive difficulty: start easy, get harder\n",
        "            self.curriculum_stages = [\n",
        "                {\"threshold\": 35, \"max_steps\": 800},   # Easy: stop at 35 boids\n",
        "                {\"threshold\": 25, \"max_steps\": 900},   # Medium: stop at 25 boids  \n",
        "                {\"threshold\": 20, \"max_steps\": 1000},  # Hard: stop at 20 boids\n",
        "                {\"threshold\": 15, \"max_steps\": 1200},  # Expert: stop at 15 boids\n",
        "                {\"threshold\": 10, \"max_steps\": 1500},  # Master: stop at 10 boids\n",
        "            ]\n",
        "\n",
        "class PredatorPreyRL(gym.Env):\n",
        "    \"\"\"\n",
        "    Reinforcement Learning Environment for Predator-Prey Ecosystem\n",
        "    \n",
        "    **Observation Space**: Structured inputs for transformer (dict)\n",
        "    **Action Space**: Continuous steering forces [-1, 1] (Box(2,))\n",
        "    **Reward**: Sparse terminal reward based on episode completion time\n",
        "    **Episode End**: When boids count <= threshold OR max steps reached\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: RLConfig = None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.config = config or RLConfig()\n",
        "        \n",
        "        # Initialize simulation components\n",
        "        self.simulation = None\n",
        "        self.input_processor = InputProcessor()\n",
        "        self.action_processor = ActionProcessor()\n",
        "        \n",
        "        # Action space: continuous steering forces\n",
        "        self.action_space = spaces.Box(\n",
        "            low=-1.0, high=1.0, shape=(2,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Observation space: structured inputs (will be flattened for SB3)\n",
        "        # We'll define this after seeing the actual observation structure\n",
        "        self.observation_space = None\n",
        "        \n",
        "        # Episode tracking\n",
        "        self.step_count = 0\n",
        "        self.episode_start_time = 0\n",
        "        self.curriculum_stage = 0\n",
        "        \n",
        "        # Statistics tracking\n",
        "        self.episode_stats = {\n",
        "            \"total_episodes\": 0,\n",
        "            \"successful_episodes\": 0,\n",
        "            \"average_completion_time\": 0.0,\n",
        "            \"best_completion_time\": float('inf')\n",
        "        }\n",
        "        \n",
        "        print(f\"üéÆ PredatorPreyRL Environment Created:\")\n",
        "        print(f\"   Initial Boids: {self.config.initial_boids}\")\n",
        "        print(f\"   Episode End Threshold: {self.config.episode_end_threshold}\")\n",
        "        print(f\"   Max Episode Steps: {self.config.max_episode_steps}\")\n",
        "        print(f\"   Curriculum Learning: {self.config.curriculum_enabled}\")\n",
        "    \n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
        "        \"\"\"Reset environment for new episode\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        \n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        # Update curriculum stage if enabled\n",
        "        if self.config.curriculum_enabled:\n",
        "            self._update_curriculum()\n",
        "        \n",
        "        # Create new simulation\n",
        "        self.simulation = Simulation(\n",
        "            canvas_width=self.config.canvas_width,\n",
        "            canvas_height=self.config.canvas_height\n",
        "        )\n",
        "        self.simulation.initialize()\n",
        "        \n",
        "        # Reset episode tracking\n",
        "        self.step_count = 0\n",
        "        self.episode_start_time = 0\n",
        "        \n",
        "        # Get initial observation\n",
        "        observation = self._get_observation()\n",
        "        \n",
        "        # Set observation space on first reset\n",
        "        if self.observation_space is None:\n",
        "            obs_flat = self._flatten_observation(observation)\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=-np.inf, high=np.inf, \n",
        "                shape=obs_flat.shape, dtype=np.float32\n",
        "            )\n",
        "            print(f\"üìä Observation Space: {self.observation_space.shape}\")\n",
        "        \n",
        "        info = self._get_info()\n",
        "        return self._flatten_observation(observation), info\n",
        "    \n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
        "        \"\"\"Execute one step in the environment\"\"\"\n",
        "        self.step_count += 1\n",
        "        \n",
        "        # Convert action to game forces\n",
        "        actions = self.action_processor.process_action(action.tolist())\n",
        "        \n",
        "        # Apply actions to predator\n",
        "        self.simulation.set_predator_acceleration(actions[0], actions[1])\n",
        "        \n",
        "        # Step simulation\n",
        "        self.simulation.step()\n",
        "        \n",
        "        # Get new observation\n",
        "        observation = self._get_observation()\n",
        "        \n",
        "        # Check if episode is done\n",
        "        terminated, truncated = self._check_episode_end()\n",
        "        \n",
        "        # Calculate reward\n",
        "        reward = self._calculate_reward(terminated, truncated)\n",
        "        \n",
        "        # Get info\n",
        "        info = self._get_info()\n",
        "        \n",
        "        # Update statistics on episode end\n",
        "        if terminated or truncated:\n",
        "            self._update_episode_stats(terminated)\n",
        "        \n",
        "        return self._flatten_observation(observation), reward, terminated, truncated, info\n",
        "    \n",
        "    def _get_observation(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current observation from simulation\"\"\"\n",
        "        state = self.simulation.get_state()\n",
        "        \n",
        "        if state['predator'] is None:\n",
        "            # Handle case where predator doesn't exist\n",
        "            predator_pos = {'x': 0, 'y': 0}\n",
        "            predator_vel = {'x': 0, 'y': 0}\n",
        "        else:\n",
        "            predator_pos = state['predator']['position']\n",
        "            predator_vel = state['predator']['velocity']\n",
        "        \n",
        "        structured_inputs = self.input_processor.process_inputs(\n",
        "            state['boids'],\n",
        "            predator_pos,\n",
        "            predator_vel,\n",
        "            state['canvas_width'],\n",
        "            state['canvas_height']\n",
        "        )\n",
        "        \n",
        "        return structured_inputs\n",
        "    \n",
        "    def _flatten_observation(self, observation: Dict[str, Any]) -> np.ndarray:\n",
        "        \"\"\"Flatten structured observation for SB3 compatibility\"\"\"\n",
        "        # Context (2 values)\n",
        "        obs_parts = [\n",
        "            observation['context']['canvasWidth'],\n",
        "            observation['context']['canvasHeight']\n",
        "        ]\n",
        "        \n",
        "        # Predator (2 values)\n",
        "        obs_parts.extend([\n",
        "            observation['predator']['velX'],\n",
        "            observation['predator']['velY']\n",
        "        ])\n",
        "        \n",
        "        # Boids (4 values each, padded/truncated to fixed size)\n",
        "        max_boids = 60  # Slightly larger than initial to handle edge cases\n",
        "        boid_data = []\n",
        "        \n",
        "        for i in range(max_boids):\n",
        "            if i < len(observation['boids']):\n",
        "                boid = observation['boids'][i]\n",
        "                boid_data.extend([\n",
        "                    boid['relX'], boid['relY'], \n",
        "                    boid['velX'], boid['velY']\n",
        "                ])\n",
        "            else:\n",
        "                # Pad with zeros\n",
        "                boid_data.extend([0.0, 0.0, 0.0, 0.0])\n",
        "        \n",
        "        obs_parts.extend(boid_data)\n",
        "        \n",
        "        return np.array(obs_parts, dtype=np.float32)\n",
        "    \n",
        "    def _check_episode_end(self) -> Tuple[bool, bool]:\n",
        "        \"\"\"Check if episode should end\"\"\"\n",
        "        current_boids = self.simulation.get_boid_count()\n",
        "        \n",
        "        # Get current thresholds (accounting for curriculum)\n",
        "        if self.config.curriculum_enabled and self.curriculum_stage < len(self.config.curriculum_stages):\n",
        "            stage = self.config.curriculum_stages[self.curriculum_stage]\n",
        "            threshold = stage[\"threshold\"]\n",
        "            max_steps = stage[\"max_steps\"]\n",
        "        else:\n",
        "            threshold = self.config.episode_end_threshold\n",
        "            max_steps = self.config.max_episode_steps\n",
        "        \n",
        "        # Terminated: successfully reached target\n",
        "        terminated = current_boids <= threshold\n",
        "        \n",
        "        # Truncated: max steps reached\n",
        "        truncated = self.step_count >= max_steps\n",
        "        \n",
        "        return terminated, truncated\n",
        "    \n",
        "    def _calculate_reward(self, terminated: bool, truncated: bool) -> float:\n",
        "        \"\"\"Calculate sparse terminal reward\"\"\"\n",
        "        if not (terminated or truncated):\n",
        "            # No intermediate rewards - pure end-to-end learning\n",
        "            return 0.0\n",
        "        \n",
        "        if terminated:\n",
        "            # Success reward based on efficiency (fewer steps = higher reward)\n",
        "            efficiency_bonus = max(0, (500 - self.step_count) / 500.0)\n",
        "            success_reward = self.config.success_reward + (efficiency_bonus * 50)\n",
        "            return success_reward\n",
        "        else:\n",
        "            # Truncated (timeout) - small penalty\n",
        "            return -10.0\n",
        "    \n",
        "    def _update_curriculum(self):\n",
        "        \"\"\"Update curriculum stage based on performance\"\"\"\n",
        "        if not self.config.curriculum_enabled:\n",
        "            return\n",
        "        \n",
        "        # Simple curriculum: advance when success rate > 70% over last 100 episodes\n",
        "        if self.episode_stats[\"total_episodes\"] >= 100:\n",
        "            success_rate = self.episode_stats[\"successful_episodes\"] / min(100, self.episode_stats[\"total_episodes\"])\n",
        "            \n",
        "            if success_rate > 0.7 and self.curriculum_stage < len(self.config.curriculum_stages) - 1:\n",
        "                self.curriculum_stage += 1\n",
        "                print(f\"üìà Curriculum Advanced to Stage {self.curriculum_stage}\")\n",
        "                print(f\"   New Threshold: {self.config.curriculum_stages[self.curriculum_stage]['threshold']}\")\n",
        "    \n",
        "    def _update_episode_stats(self, successful: bool):\n",
        "        \"\"\"Update episode statistics\"\"\"\n",
        "        self.episode_stats[\"total_episodes\"] += 1\n",
        "        \n",
        "        if successful:\n",
        "            self.episode_stats[\"successful_episodes\"] += 1\n",
        "            \n",
        "            # Update timing stats\n",
        "            completion_time = self.step_count\n",
        "            if completion_time < self.episode_stats[\"best_completion_time\"]:\n",
        "                self.episode_stats[\"best_completion_time\"] = completion_time\n",
        "            \n",
        "            # Rolling average of completion time\n",
        "            alpha = 0.1  # Learning rate for moving average\n",
        "            if self.episode_stats[\"average_completion_time\"] == 0:\n",
        "                self.episode_stats[\"average_completion_time\"] = completion_time\n",
        "            else:\n",
        "                self.episode_stats[\"average_completion_time\"] = (\n",
        "                    alpha * completion_time + \n",
        "                    (1 - alpha) * self.episode_stats[\"average_completion_time\"]\n",
        "                )\n",
        "    \n",
        "    def _get_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get environment info\"\"\"\n",
        "        current_boids = self.simulation.get_boid_count()\n",
        "        \n",
        "        # Current stage info\n",
        "        if self.config.curriculum_enabled and self.curriculum_stage < len(self.config.curriculum_stages):\n",
        "            stage_info = self.config.curriculum_stages[self.curriculum_stage]\n",
        "        else:\n",
        "            stage_info = {\n",
        "                \"threshold\": self.config.episode_end_threshold,\n",
        "                \"max_steps\": self.config.max_episode_steps\n",
        "            }\n",
        "        \n",
        "        return {\n",
        "            \"step_count\": self.step_count,\n",
        "            \"boids_remaining\": current_boids,\n",
        "            \"boids_caught\": self.config.initial_boids - current_boids,\n",
        "            \"curriculum_stage\": self.curriculum_stage,\n",
        "            \"stage_threshold\": stage_info[\"threshold\"],\n",
        "            \"stage_max_steps\": stage_info[\"max_steps\"],\n",
        "            \"episode_stats\": self.episode_stats.copy()\n",
        "        }\n",
        "\n",
        "# Test the environment\n",
        "print(\"üß™ Testing RL Environment...\")\n",
        "config = RLConfig(\n",
        "    initial_boids=30,  # Start small for testing\n",
        "    episode_end_threshold=20,\n",
        "    max_episode_steps=100\n",
        ")\n",
        "\n",
        "env = PredatorPreyRL(config)\n",
        "obs, info = env.reset()\n",
        "\n",
        "print(f\"‚úÖ Environment created successfully\")\n",
        "print(f\"üìä Observation shape: {obs.shape}\")\n",
        "print(f\"üéØ Action space: {env.action_space}\")\n",
        "print(f\"üéÆ Initial boids: {info['boids_remaining']}\")\n",
        "\n",
        "# Test a few steps\n",
        "for i in range(3):\n",
        "    action = env.action_space.sample()  # Random action\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    print(f\"Step {i+1}: Boids={info['boids_remaining']}, Reward={reward:.2f}, Done={terminated or truncated}\")\n",
        "    \n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "print(\"üéØ Environment test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üß† Custom Transformer Policy for PPO\n",
        "\n",
        "We need to integrate our existing transformer architecture with Stable-Baselines3's PPO implementation. This allows us to:\n",
        "- Load supervised learning checkpoints\n",
        "- Use our proven transformer architecture  \n",
        "- Leverage PPO's credit assignment for RL training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Transformer Policy for SB3\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.type_aliases import Schedule\n",
        "from typing import Union\n",
        "\n",
        "class TransformerFeaturesExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Custom features extractor that converts flattened observations \n",
        "    back to structured format for transformer processing\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 48):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        \n",
        "        # Initialize our transformer model\n",
        "        self.transformer = TransformerPredator(\n",
        "            d_model=48, n_heads=4, n_layers=3, ffn_hidden=96\n",
        "        )\n",
        "        \n",
        "        self.features_dim = features_dim\n",
        "        \n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convert flattened observations back to structured format and process through transformer\n",
        "        \"\"\"\n",
        "        batch_size = observations.shape[0]\n",
        "        \n",
        "        # Convert flattened observations back to structured format\n",
        "        structured_inputs_batch = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            obs = observations[i]\n",
        "            \n",
        "            # Extract context (first 2 values)\n",
        "            context = {\n",
        "                'canvasWidth': obs[0].item(),\n",
        "                'canvasHeight': obs[1].item()\n",
        "            }\n",
        "            \n",
        "            # Extract predator (next 2 values)\n",
        "            predator = {\n",
        "                'velX': obs[2].item(),\n",
        "                'velY': obs[3].item()\n",
        "            }\n",
        "            \n",
        "            # Extract boids (remaining values, grouped by 4)\n",
        "            boids = []\n",
        "            boid_start_idx = 4\n",
        "            max_boids = (len(obs) - 4) // 4\n",
        "            \n",
        "            for j in range(max_boids):\n",
        "                idx = boid_start_idx + j * 4\n",
        "                rel_x = obs[idx].item()\n",
        "                rel_y = obs[idx + 1].item()\n",
        "                vel_x = obs[idx + 2].item()\n",
        "                vel_y = obs[idx + 3].item()\n",
        "                \n",
        "                # Only add non-zero boids (ignore padding)\n",
        "                if rel_x != 0 or rel_y != 0 or vel_x != 0 or vel_y != 0:\n",
        "                    boids.append({\n",
        "                        'relX': rel_x,\n",
        "                        'relY': rel_y, \n",
        "                        'velX': vel_x,\n",
        "                        'velY': vel_y\n",
        "                    })\n",
        "            \n",
        "            structured_inputs = {\n",
        "                'context': context,\n",
        "                'predator': predator,\n",
        "                'boids': boids\n",
        "            }\n",
        "            \n",
        "            structured_inputs_batch.append(structured_inputs)\n",
        "        \n",
        "        # Process through transformer\n",
        "        transformer_outputs = self.transformer(structured_inputs_batch)\n",
        "        \n",
        "        return transformer_outputs\n",
        "\n",
        "class TransformerActorCriticPolicy(ActorCriticPolicy):\n",
        "    \"\"\"\n",
        "    Custom ActorCriticPolicy that uses our transformer as feature extractor\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 observation_space: gym.spaces.Space,\n",
        "                 action_space: gym.spaces.Space,\n",
        "                 lr_schedule: Schedule,\n",
        "                 **kwargs):\n",
        "        \n",
        "        # Set custom features extractor\n",
        "        kwargs['features_extractor_class'] = TransformerFeaturesExtractor\n",
        "        kwargs['features_extractor_kwargs'] = {'features_dim': 48}\n",
        "        \n",
        "        super().__init__(observation_space, action_space, lr_schedule, **kwargs)\n",
        "    \n",
        "    def load_supervised_checkpoint(self, checkpoint_path: str):\n",
        "        \"\"\"Load weights from supervised learning checkpoint\"\"\"\n",
        "        print(f\"üì• Loading supervised checkpoint: {checkpoint_path}\")\n",
        "        \n",
        "        try:\n",
        "            # Load checkpoint\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "            \n",
        "            if 'model_state_dict' in checkpoint:\n",
        "                state_dict = checkpoint['model_state_dict']\n",
        "                epoch = checkpoint.get('epoch', 'unknown')\n",
        "                val_loss = checkpoint.get('best_val_loss', 'unknown')\n",
        "                print(f\"   Checkpoint from epoch {epoch}, validation loss: {val_loss}\")\n",
        "            else:\n",
        "                state_dict = checkpoint\n",
        "                print(\"   Raw state dict loaded\")\n",
        "            \n",
        "            # Load weights into transformer feature extractor\n",
        "            transformer_state_dict = {}\n",
        "            for key, value in state_dict.items():\n",
        "                # Remove any 'model.' prefix if present\n",
        "                clean_key = key.replace('model.', '')\n",
        "                transformer_state_dict[clean_key] = value\n",
        "            \n",
        "            self.features_extractor.transformer.load_state_dict(transformer_state_dict, strict=True)\n",
        "            print(\"‚úÖ Supervised weights loaded successfully\")\n",
        "            \n",
        "            return {\n",
        "                'epoch': checkpoint.get('epoch', 0),\n",
        "                'val_loss': checkpoint.get('best_val_loss', float('inf'))\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load supervised checkpoint: {e}\")\n",
        "            print(\"   Continuing with random initialization\")\n",
        "            return None\n",
        "\n",
        "# Test the custom policy\n",
        "print(\"üß™ Testing Custom Transformer Policy...\")\n",
        "\n",
        "# Create a dummy environment to get spaces\n",
        "test_config = RLConfig(initial_boids=10, max_episode_steps=50)\n",
        "test_env = PredatorPreyRL(test_config)\n",
        "obs, _ = test_env.reset()\n",
        "\n",
        "print(f\"üìä Observation space: {test_env.observation_space}\")\n",
        "print(f\"üéØ Action space: {test_env.action_space}\")\n",
        "\n",
        "# Test feature extractor\n",
        "feature_extractor = TransformerFeaturesExtractor(\n",
        "    observation_space=test_env.observation_space,\n",
        "    features_dim=48\n",
        ")\n",
        "\n",
        "# Test with batch of observations\n",
        "batch_obs = torch.FloatTensor([obs, obs])  # Batch of 2\n",
        "features = feature_extractor(batch_obs)\n",
        "\n",
        "print(f\"‚úÖ Features extracted successfully\")\n",
        "print(f\"üìè Features shape: {features.shape}\")\n",
        "print(f\"üî¢ Feature range: [{features.min().item():.3f}, {features.max().item():.3f}]\")\n",
        "\n",
        "print(\"üéØ Custom policy test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üíæ Comprehensive Checkpoint Management\n",
        "\n",
        "Robust checkpoint system for RL training with:\n",
        "- **Unique Naming**: Timestamped checkpoints to avoid conflicts\n",
        "- **Best Model Tracking**: Automatic best model updates\n",
        "- **Resume Capability**: Load from any checkpoint to continue training\n",
        "- **Export Ready**: Easy conversion to JavaScript format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Checkpoint Management\n",
        "import datetime\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from typing import Dict, Optional, List\n",
        "\n",
        "class RLCheckpointManager:\n",
        "    \"\"\"\n",
        "    Advanced checkpoint management for RL training\n",
        "    \n",
        "    Features:\n",
        "    - Unique timestamped checkpoint names\n",
        "    - Best model tracking with automatic updates\n",
        "    - Metadata storage (training stats, config, etc.)\n",
        "    - Easy resume from any checkpoint\n",
        "    - Export ready for JavaScript deployment\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 checkpoint_dir: str = \"rl_checkpoints\",\n",
        "                 max_checkpoints: int = 50,\n",
        "                 save_frequency: int = 1000):\n",
        "        \n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        self.max_checkpoints = max_checkpoints\n",
        "        self.save_frequency = save_frequency\n",
        "        \n",
        "        # Tracking\n",
        "        self.best_reward = float('-inf')\n",
        "        self.best_checkpoint_path = None\n",
        "        self.checkpoint_history = []\n",
        "        \n",
        "        # Metadata file\n",
        "        self.metadata_file = self.checkpoint_dir / \"training_metadata.json\"\n",
        "        self.load_metadata()\n",
        "        \n",
        "        print(f\"üíæ Checkpoint Manager initialized:\")\n",
        "        print(f\"   Directory: {self.checkpoint_dir}\")\n",
        "        print(f\"   Max checkpoints: {self.max_checkpoints}\")\n",
        "        print(f\"   Save frequency: {self.save_frequency} steps\")\n",
        "        print(f\"   Current best reward: {self.best_reward:.2f}\")\n",
        "    \n",
        "    def generate_checkpoint_name(self, prefix: str = \"rl_checkpoint\") -> str:\n",
        "        \"\"\"Generate unique timestamped checkpoint name\"\"\"\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "        return f\"{prefix}_{timestamp}\"\n",
        "    \n",
        "    def save_checkpoint(self, \n",
        "                       model,\n",
        "                       step: int,\n",
        "                       episode: int, \n",
        "                       reward: float,\n",
        "                       episode_stats: Dict,\n",
        "                       metadata: Dict = None,\n",
        "                       is_best: bool = False,\n",
        "                       force_save: bool = False) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Save model checkpoint with comprehensive metadata\n",
        "        \n",
        "        Args:\n",
        "            model: PPO model to save\n",
        "            step: Current training step\n",
        "            episode: Current episode number\n",
        "            reward: Current episode reward (or recent average)\n",
        "            episode_stats: Episode statistics\n",
        "            metadata: Additional metadata to save\n",
        "            is_best: Whether this is the best model so far\n",
        "            force_save: Force save regardless of frequency\n",
        "            \n",
        "        Returns:\n",
        "            Path to saved checkpoint or None if not saved\n",
        "        \"\"\"\n",
        "        \n",
        "        # Check if we should save based on frequency\n",
        "        if not force_save and not is_best and step % self.save_frequency != 0:\n",
        "            return None\n",
        "        \n",
        "        # Generate checkpoint name\n",
        "        if is_best:\n",
        "            checkpoint_name = \"best_model\"\n",
        "        else:\n",
        "            checkpoint_name = self.generate_checkpoint_name()\n",
        "        \n",
        "        checkpoint_path = self.checkpoint_dir / f\"{checkpoint_name}.zip\"\n",
        "        \n",
        "        # Save model (SB3 format)\n",
        "        model.save(checkpoint_path)\n",
        "        \n",
        "        # Prepare metadata\n",
        "        checkpoint_metadata = {\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"step\": step,\n",
        "            \"episode\": episode,\n",
        "            \"reward\": reward,\n",
        "            \"episode_stats\": episode_stats,\n",
        "            \"model_info\": {\n",
        "                \"algorithm\": \"PPO\",\n",
        "                \"policy\": \"TransformerActorCriticPolicy\",\n",
        "                \"total_parameters\": sum(p.numel() for p in model.policy.parameters()),\n",
        "            },\n",
        "            \"checkpoint_path\": str(checkpoint_path),\n",
        "            \"is_best\": is_best\n",
        "        }\n",
        "        \n",
        "        if metadata:\n",
        "            checkpoint_metadata.update(metadata)\n",
        "        \n",
        "        # Save metadata alongside model\n",
        "        metadata_path = checkpoint_path.with_suffix('.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(checkpoint_metadata, f, indent=2)\n",
        "        \n",
        "        # Update best model tracking\n",
        "        if is_best or reward > self.best_reward:\n",
        "            self.best_reward = reward\n",
        "            self.best_checkpoint_path = str(checkpoint_path)\n",
        "            \n",
        "            # Also save as best_model if not already\n",
        "            if not is_best:\n",
        "                best_path = self.checkpoint_dir / \"best_model.zip\"\n",
        "                shutil.copy2(checkpoint_path, best_path)\n",
        "                \n",
        "                best_metadata_path = best_path.with_suffix('.json')\n",
        "                checkpoint_metadata[\"is_best\"] = True\n",
        "                with open(best_metadata_path, 'w') as f:\n",
        "                    json.dump(checkpoint_metadata, f, indent=2)\n",
        "                \n",
        "                print(f\"üèÜ New best model! Reward: {reward:.2f}\")\n",
        "        \n",
        "        # Add to history\n",
        "        self.checkpoint_history.append({\n",
        "            \"path\": str(checkpoint_path),\n",
        "            \"step\": step,\n",
        "            \"episode\": episode,\n",
        "            \"reward\": reward,\n",
        "            \"timestamp\": checkpoint_metadata[\"timestamp\"],\n",
        "            \"is_best\": is_best or reward > self.best_reward\n",
        "        })\n",
        "        \n",
        "        # Clean up old checkpoints if needed\n",
        "        self._cleanup_old_checkpoints()\n",
        "        \n",
        "        # Save metadata\n",
        "        self.save_metadata()\n",
        "        \n",
        "        print(f\"üíæ Checkpoint saved: {checkpoint_name}\")\n",
        "        print(f\"   Step: {step}, Episode: {episode}, Reward: {reward:.2f}\")\n",
        "        \n",
        "        return str(checkpoint_path)\n",
        "    \n",
        "    def load_checkpoint(self, checkpoint_path: str = None) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Load checkpoint metadata (model loading is handled separately)\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to checkpoint, or None for best model\n",
        "            \n",
        "        Returns:\n",
        "            Checkpoint metadata or None if not found\n",
        "        \"\"\"\n",
        "        if checkpoint_path is None:\n",
        "            # Load best model\n",
        "            checkpoint_path = self.checkpoint_dir / \"best_model.zip\"\n",
        "        else:\n",
        "            checkpoint_path = Path(checkpoint_path)\n",
        "        \n",
        "        if not checkpoint_path.exists():\n",
        "            print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "            return None\n",
        "        \n",
        "        # Load metadata\n",
        "        metadata_path = checkpoint_path.with_suffix('.json')\n",
        "        if metadata_path.exists():\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "            \n",
        "            print(f\"üì• Checkpoint metadata loaded:\")\n",
        "            print(f\"   Path: {checkpoint_path}\")\n",
        "            print(f\"   Step: {metadata.get('step', 'unknown')}\")\n",
        "            print(f\"   Episode: {metadata.get('episode', 'unknown')}\")\n",
        "            print(f\"   Reward: {metadata.get('reward', 'unknown')}\")\n",
        "            print(f\"   Timestamp: {metadata.get('timestamp', 'unknown')}\")\n",
        "            \n",
        "            return metadata\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Metadata file not found for checkpoint: {checkpoint_path}\")\n",
        "            return {\"checkpoint_path\": str(checkpoint_path)}\n",
        "    \n",
        "    def list_checkpoints(self) -> List[Dict]:\n",
        "        \"\"\"List all available checkpoints with metadata\"\"\"\n",
        "        checkpoints = []\n",
        "        \n",
        "        for checkpoint_file in sorted(self.checkpoint_dir.glob(\"*.zip\")):\n",
        "            metadata_file = checkpoint_file.with_suffix('.json')\n",
        "            \n",
        "            if metadata_file.exists():\n",
        "                with open(metadata_file, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "                checkpoints.append(metadata)\n",
        "            else:\n",
        "                # Basic info for checkpoints without metadata\n",
        "                checkpoints.append({\n",
        "                    \"checkpoint_path\": str(checkpoint_file),\n",
        "                    \"timestamp\": \"unknown\",\n",
        "                    \"step\": \"unknown\",\n",
        "                    \"episode\": \"unknown\", \n",
        "                    \"reward\": \"unknown\"\n",
        "                })\n",
        "        \n",
        "        return sorted(checkpoints, key=lambda x: x.get('step', 0))\n",
        "    \n",
        "    def _cleanup_old_checkpoints(self):\n",
        "        \"\"\"Remove old checkpoints to maintain max_checkpoints limit\"\"\"\n",
        "        if len(self.checkpoint_history) <= self.max_checkpoints:\n",
        "            return\n",
        "        \n",
        "        # Sort by step (oldest first) but keep best models\n",
        "        non_best_checkpoints = [\n",
        "            cp for cp in self.checkpoint_history \n",
        "            if not cp.get('is_best', False)\n",
        "        ]\n",
        "        \n",
        "        if len(non_best_checkpoints) > self.max_checkpoints - 1:  # -1 for best model\n",
        "            # Remove oldest non-best checkpoints\n",
        "            to_remove = len(non_best_checkpoints) - (self.max_checkpoints - 1)\n",
        "            oldest_checkpoints = sorted(non_best_checkpoints, key=lambda x: x['step'])[:to_remove]\n",
        "            \n",
        "            for cp in oldest_checkpoints:\n",
        "                checkpoint_path = Path(cp['path'])\n",
        "                metadata_path = checkpoint_path.with_suffix('.json')\n",
        "                \n",
        "                try:\n",
        "                    if checkpoint_path.exists():\n",
        "                        checkpoint_path.unlink()\n",
        "                    if metadata_path.exists():\n",
        "                        metadata_path.unlink()\n",
        "                    \n",
        "                    self.checkpoint_history = [\n",
        "                        x for x in self.checkpoint_history \n",
        "                        if x['path'] != cp['path']\n",
        "                    ]\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Failed to remove checkpoint {checkpoint_path}: {e}\")\n",
        "    \n",
        "    def save_metadata(self):\n",
        "        \"\"\"Save training metadata to disk\"\"\"\n",
        "        metadata = {\n",
        "            \"best_reward\": self.best_reward,\n",
        "            \"best_checkpoint_path\": self.best_checkpoint_path,\n",
        "            \"checkpoint_history\": self.checkpoint_history,\n",
        "            \"last_updated\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        with open(self.metadata_file, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    def load_metadata(self):\n",
        "        \"\"\"Load training metadata from disk\"\"\"\n",
        "        if self.metadata_file.exists():\n",
        "            try:\n",
        "                with open(self.metadata_file, 'r') as f:\n",
        "                    metadata = json.load(f)\n",
        "                \n",
        "                self.best_reward = metadata.get('best_reward', float('-inf'))\n",
        "                self.best_checkpoint_path = metadata.get('best_checkpoint_path')\n",
        "                self.checkpoint_history = metadata.get('checkpoint_history', [])\n",
        "                \n",
        "                print(f\"üì• Training metadata loaded\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Failed to load metadata: {e}\")\n",
        "    \n",
        "    def export_best_for_javascript(self, output_path: str = \"model_export_rl.js\"):\n",
        "        \"\"\"Export best model for JavaScript deployment\"\"\"\n",
        "        if not self.best_checkpoint_path:\n",
        "            print(\"‚ùå No best model available for export\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"üöÄ Exporting best RL model for JavaScript...\")\n",
        "        print(f\"   Best checkpoint: {self.best_checkpoint_path}\")\n",
        "        print(f\"   Output: {output_path}\")\n",
        "        \n",
        "        # This would integrate with the existing export_to_js.py script\n",
        "        # For now, just provide instructions\n",
        "        print(\"üìã To export to JavaScript format:\")\n",
        "        print(f\"   1. Extract transformer weights from: {self.best_checkpoint_path}\")\n",
        "        print(f\"   2. Run: python homepage/export_to_js.py --checkpoint <extracted_weights> --output {output_path}\")\n",
        "        \n",
        "        return True\n",
        "\n",
        "# Test checkpoint manager\n",
        "print(\"üß™ Testing Checkpoint Manager...\")\n",
        "\n",
        "checkpoint_manager = RLCheckpointManager(\n",
        "    checkpoint_dir=\"test_rl_checkpoints\",\n",
        "    max_checkpoints=5,\n",
        "    save_frequency=100\n",
        ")\n",
        "\n",
        "# List any existing checkpoints\n",
        "checkpoints = checkpoint_manager.list_checkpoints()\n",
        "print(f\"üìã Found {len(checkpoints)} existing checkpoints\")\n",
        "\n",
        "for i, cp in enumerate(checkpoints[-3:]):  # Show last 3\n",
        "    print(f\"   {i+1}. Step {cp.get('step', '?')}, Reward {cp.get('reward', '?')}\")\n",
        "\n",
        "print(\"üéØ Checkpoint manager test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üöÄ RL Training Setup & Configuration\n",
        "\n",
        "**Training Configuration Options:**\n",
        "- **Fresh Training**: Start from supervised checkpoint or random initialization\n",
        "- **Resume Training**: Continue from any RL checkpoint\n",
        "- **Curriculum Learning**: Progressive difficulty with automatic stage advancement\n",
        "- **Advanced Logging**: TensorBoard + WandB integration for comprehensive monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main RL Training Configuration\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "@dataclass\n",
        "class RLTrainingConfig:\n",
        "    \"\"\"Complete RL training configuration\"\"\"\n",
        "    \n",
        "    # Environment settings\n",
        "    num_envs: int = 8  # Parallel environments for faster training\n",
        "    canvas_width: int = 800\n",
        "    canvas_height: int = 600\n",
        "    initial_boids: int = 50\n",
        "    episode_end_threshold: int = 20\n",
        "    max_episode_steps: int = 1000\n",
        "    \n",
        "    # Curriculum learning\n",
        "    curriculum_enabled: bool = True\n",
        "    \n",
        "    # PPO hyperparameters (optimized for A100)\n",
        "    learning_rate: float = 3e-4\n",
        "    n_steps: int = 2048  # Steps per environment per update\n",
        "    batch_size: int = 256  # Batch size for optimization\n",
        "    n_epochs: int = 10  # Number of epochs per update\n",
        "    gamma: float = 0.99  # Discount factor\n",
        "    gae_lambda: float = 0.95  # GAE parameter\n",
        "    clip_range: float = 0.2  # PPO clip range\n",
        "    ent_coef: float = 0.01  # Entropy coefficient\n",
        "    vf_coef: float = 0.5  # Value function coefficient\n",
        "    max_grad_norm: float = 0.5  # Gradient clipping\n",
        "    \n",
        "    # Training settings\n",
        "    total_timesteps: int = 5_000_000  # Total training steps\n",
        "    eval_freq: int = 10_000  # Evaluation frequency\n",
        "    checkpoint_freq: int = 50_000  # Checkpoint frequency\n",
        "    \n",
        "    # Logging\n",
        "    tensorboard_log: str = \"./tensorboard_logs\"\n",
        "    wandb_project: str = \"predator-prey-rl\"\n",
        "    wandb_enabled: bool = True\n",
        "    \n",
        "    # Resume settings\n",
        "    resume_from_checkpoint: Optional[str] = None\n",
        "    supervised_checkpoint: Optional[str] = None  # Pre-trained weights to start from\n",
        "\n",
        "class AdvancedRLCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Advanced callback for RL training with comprehensive logging and checkpointing\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 checkpoint_manager: RLCheckpointManager,\n",
        "                 config: RLTrainingConfig,\n",
        "                 eval_env=None,\n",
        "                 verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        \n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        self.config = config\n",
        "        self.eval_env = eval_env\n",
        "        \n",
        "        # Episode tracking\n",
        "        self.episode_rewards = deque(maxlen=100)\n",
        "        self.episode_lengths = deque(maxlen=100)\n",
        "        self.episode_success_rate = deque(maxlen=100)\n",
        "        \n",
        "        # Best performance tracking\n",
        "        self.best_mean_reward = float('-inf')\n",
        "        self.evaluation_rewards = []\n",
        "        \n",
        "        # Logging\n",
        "        self.last_checkpoint_step = 0\n",
        "        \n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"Called after each step\"\"\"\n",
        "        \n",
        "        # Log episode end information\n",
        "        for i, done in enumerate(self.locals.get('dones', [])):\n",
        "            if done:\n",
        "                info = self.locals['infos'][i]\n",
        "                \n",
        "                # Extract episode stats\n",
        "                if 'episode' in info:\n",
        "                    episode_reward = info['episode']['r']\n",
        "                    episode_length = info['episode']['l']\n",
        "                    \n",
        "                    self.episode_rewards.append(episode_reward)\n",
        "                    self.episode_lengths.append(episode_length)\n",
        "                    \n",
        "                    # Success: terminated due to reaching threshold (not timeout)\n",
        "                    was_successful = episode_reward > 50  # Threshold for success\n",
        "                    self.episode_success_rate.append(1.0 if was_successful else 0.0)\n",
        "                    \n",
        "                    # Log to wandb if enabled\n",
        "                    if self.config.wandb_enabled:\n",
        "                        wandb.log({\n",
        "                            \"episode/reward\": episode_reward,\n",
        "                            \"episode/length\": episode_length,\n",
        "                            \"episode/success\": was_successful,\n",
        "                            \"episode/boids_remaining\": info.get('boids_remaining', 0),\n",
        "                            \"episode/curriculum_stage\": info.get('curriculum_stage', 0),\n",
        "                        }, step=self.num_timesteps)\n",
        "        \n",
        "        # Periodic logging of aggregated stats\n",
        "        if len(self.episode_rewards) > 0 and self.num_timesteps % 1000 == 0:\n",
        "            mean_reward = np.mean(self.episode_rewards)\n",
        "            mean_length = np.mean(self.episode_lengths) \n",
        "            success_rate = np.mean(self.episode_success_rate)\n",
        "            \n",
        "            # Log aggregated metrics\n",
        "            self.logger.record(\"rollout/ep_rew_mean\", mean_reward)\n",
        "            self.logger.record(\"rollout/ep_len_mean\", mean_length)\n",
        "            self.logger.record(\"rollout/success_rate\", success_rate)\n",
        "            \n",
        "            if self.config.wandb_enabled:\n",
        "                wandb.log({\n",
        "                    \"rollout/ep_rew_mean\": mean_reward,\n",
        "                    \"rollout/ep_len_mean\": mean_length,\n",
        "                    \"rollout/success_rate\": success_rate,\n",
        "                }, step=self.num_timesteps)\n",
        "            \n",
        "            # Check for new best performance\n",
        "            if mean_reward > self.best_mean_reward:\n",
        "                self.best_mean_reward = mean_reward\n",
        "                \n",
        "                # Save best model checkpoint\n",
        "                if self.model:\n",
        "                    episode_stats = {\n",
        "                        \"mean_reward\": mean_reward,\n",
        "                        \"mean_length\": mean_length,\n",
        "                        \"success_rate\": success_rate,\n",
        "                        \"total_episodes\": len(self.episode_rewards)\n",
        "                    }\n",
        "                    \n",
        "                    self.checkpoint_manager.save_checkpoint(\n",
        "                        model=self.model,\n",
        "                        step=self.num_timesteps,\n",
        "                        episode=len(self.episode_rewards),\n",
        "                        reward=mean_reward,\n",
        "                        episode_stats=episode_stats,\n",
        "                        is_best=True\n",
        "                    )\n",
        "        \n",
        "        # Regular checkpointing\n",
        "        if (self.num_timesteps - self.last_checkpoint_step) >= self.config.checkpoint_freq:\n",
        "            if self.model and len(self.episode_rewards) > 0:\n",
        "                episode_stats = {\n",
        "                    \"mean_reward\": np.mean(self.episode_rewards),\n",
        "                    \"mean_length\": np.mean(self.episode_lengths),\n",
        "                    \"success_rate\": np.mean(self.episode_success_rate),\n",
        "                    \"total_episodes\": len(self.episode_rewards)\n",
        "                }\n",
        "                \n",
        "                self.checkpoint_manager.save_checkpoint(\n",
        "                    model=self.model,\n",
        "                    step=self.num_timesteps,\n",
        "                    episode=len(self.episode_rewards),\n",
        "                    reward=np.mean(self.episode_rewards),\n",
        "                    episode_stats=episode_stats,\n",
        "                    force_save=True\n",
        "                )\n",
        "                \n",
        "                self.last_checkpoint_step = self.num_timesteps\n",
        "        \n",
        "        return True\n",
        "\n",
        "def create_rl_environment(config: RLTrainingConfig, rank: int = 0):\n",
        "    \"\"\"\n",
        "    Create and configure RL environment\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        rl_config = RLConfig(\n",
        "            canvas_width=config.canvas_width,\n",
        "            canvas_height=config.canvas_height,\n",
        "            initial_boids=config.initial_boids,\n",
        "            episode_end_threshold=config.episode_end_threshold,\n",
        "            max_episode_steps=config.max_episode_steps,\n",
        "            curriculum_enabled=config.curriculum_enabled\n",
        "        )\n",
        "        \n",
        "        env = PredatorPreyRL(rl_config)\n",
        "        env = Monitor(env)  # For episode statistics\n",
        "        env.seed(42 + rank)  # Different seed for each environment\n",
        "        return env\n",
        "    \n",
        "    return _init\n",
        "\n",
        "def setup_rl_training(config: RLTrainingConfig) -> tuple:\n",
        "    \"\"\"\n",
        "    Set up complete RL training pipeline\n",
        "    \n",
        "    Returns:\n",
        "        model, env, checkpoint_manager, callback\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üöÄ Setting up RL Training Pipeline...\")\n",
        "    \n",
        "    # Initialize wandb if enabled\n",
        "    if config.wandb_enabled:\n",
        "        wandb.init(\n",
        "            project=config.wandb_project,\n",
        "            config=config.__dict__,\n",
        "            sync_tensorboard=True\n",
        "        )\n",
        "        print(\"‚úÖ WandB initialized\")\n",
        "    \n",
        "    # Create vectorized environment\n",
        "    if config.num_envs == 1:\n",
        "        env = DummyVecEnv([create_rl_environment(config, 0)])\n",
        "    else:\n",
        "        env = SubprocVecEnv([\n",
        "            create_rl_environment(config, i) for i in range(config.num_envs)\n",
        "        ])\n",
        "    \n",
        "    print(f\"üéÆ Created {config.num_envs} parallel environments\")\n",
        "    \n",
        "    # Create checkpoint manager\n",
        "    checkpoint_manager = RLCheckpointManager(\n",
        "        checkpoint_dir=\"rl_checkpoints\",\n",
        "        max_checkpoints=50,\n",
        "        save_frequency=config.checkpoint_freq\n",
        "    )\n",
        "    \n",
        "    # PPO model configuration\n",
        "    model_kwargs = {\n",
        "        \"policy\": TransformerActorCriticPolicy,\n",
        "        \"env\": env,\n",
        "        \"learning_rate\": config.learning_rate,\n",
        "        \"n_steps\": config.n_steps,\n",
        "        \"batch_size\": config.batch_size,\n",
        "        \"n_epochs\": config.n_epochs,\n",
        "        \"gamma\": config.gamma,\n",
        "        \"gae_lambda\": config.gae_lambda,\n",
        "        \"clip_range\": config.clip_range,\n",
        "        \"ent_coef\": config.ent_coef,\n",
        "        \"vf_coef\": config.vf_coef,\n",
        "        \"max_grad_norm\": config.max_grad_norm,\n",
        "        \"device\": device,\n",
        "        \"verbose\": 1,\n",
        "        \"tensorboard_log\": config.tensorboard_log,\n",
        "    }\n",
        "    \n",
        "    # Create or load model\n",
        "    if config.resume_from_checkpoint:\n",
        "        print(f\"üì• Resuming from RL checkpoint: {config.resume_from_checkpoint}\")\n",
        "        model = PPO.load(config.resume_from_checkpoint, env=env, device=device)\n",
        "        checkpoint_metadata = checkpoint_manager.load_checkpoint(config.resume_from_checkpoint)\n",
        "    else:\n",
        "        print(\"üÜï Creating new PPO model\")\n",
        "        model = PPO(**model_kwargs)\n",
        "        \n",
        "        # Load supervised learning weights if provided\n",
        "        if config.supervised_checkpoint:\n",
        "            try:\n",
        "                supervised_info = model.policy.load_supervised_checkpoint(config.supervised_checkpoint)\n",
        "                if supervised_info:\n",
        "                    print(f\"‚úÖ Loaded supervised weights from epoch {supervised_info['epoch']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Failed to load supervised checkpoint: {e}\")\n",
        "    \n",
        "    print(f\"üß† Model created with {sum(p.numel() for p in model.policy.parameters()):,} parameters\")\n",
        "    \n",
        "    # Create callback\n",
        "    callback = AdvancedRLCallback(\n",
        "        checkpoint_manager=checkpoint_manager,\n",
        "        config=config,\n",
        "        eval_env=None  # We'll use the same env for now\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ RL Training setup complete!\")\n",
        "    \n",
        "    return model, env, checkpoint_manager, callback\n",
        "\n",
        "# Test the setup with a minimal configuration\n",
        "print(\"üß™ Testing RL Training Setup...\")\n",
        "\n",
        "test_config = RLTrainingConfig(\n",
        "    num_envs=2,  # Small for testing\n",
        "    total_timesteps=1000,  # Very short\n",
        "    checkpoint_freq=500,\n",
        "    wandb_enabled=False  # Disable for testing\n",
        ")\n",
        "\n",
        "try:\n",
        "    model, env, checkpoint_manager, callback = setup_rl_training(test_config)\n",
        "    print(\"‚úÖ RL setup test successful!\")\n",
        "    \n",
        "    # Quick test training for a few steps\n",
        "    print(\"üèÉ Running quick training test...\")\n",
        "    model.learn(total_timesteps=100, callback=callback, progress_bar=True)\n",
        "    print(\"‚úÖ Training test successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"üéØ RL training setup test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ Training Execution\n",
        "\n",
        "**Choose your training scenario by modifying the configuration below:**\n",
        "\n",
        "1. **üÜï Fresh Training**: Start from scratch or supervised checkpoint\n",
        "2. **üîÑ Resume Training**: Continue from existing RL checkpoint  \n",
        "3. **üéì Curriculum Training**: Progressive difficulty advancement\n",
        "4. **‚ö° Quick Test**: Fast training for experimentation\n",
        "\n",
        "**A100 GPU Optimization:**\n",
        "- 8 parallel environments for maximum throughput\n",
        "- Optimized batch sizes for A100 memory\n",
        "- Mixed precision training support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ MAIN TRAINING EXECUTION\n",
        "# Modify this configuration for your training scenario\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING CONFIGURATION - MODIFY THIS SECTION\n",
        "# =============================================================================\n",
        "\n",
        "# Choose your training scenario:\n",
        "TRAINING_SCENARIO = \"fresh_training\"  # Options: \"fresh_training\", \"resume_training\", \"quick_test\"\n",
        "\n",
        "# For fresh training from supervised checkpoint\n",
        "SUPERVISED_CHECKPOINT_PATH = None  # Set to path of supervised learning checkpoint\n",
        "# Example: SUPERVISED_CHECKPOINT_PATH = \"homepage/checkpoints/best_model.pt\"\n",
        "\n",
        "# For resuming RL training\n",
        "RESUME_CHECKPOINT_PATH = None  # Set to path of RL checkpoint to resume from\n",
        "# Example: RESUME_CHECKPOINT_PATH = \"rl_checkpoints/best_model.zip\"\n",
        "\n",
        "# Training configuration based on scenario\n",
        "if TRAINING_SCENARIO == \"quick_test\":\n",
        "    # Quick test configuration for experimentation\n",
        "    training_config = RLTrainingConfig(\n",
        "        # Environment\n",
        "        num_envs=4,\n",
        "        initial_boids=30,\n",
        "        episode_end_threshold=20,\n",
        "        max_episode_steps=500,\n",
        "        curriculum_enabled=False,\n",
        "        \n",
        "        # Training (short)\n",
        "        total_timesteps=100_000,\n",
        "        checkpoint_freq=10_000,\n",
        "        eval_freq=5_000,\n",
        "        \n",
        "        # PPO (smaller batches)\n",
        "        n_steps=1024,\n",
        "        batch_size=128,\n",
        "        n_epochs=5,\n",
        "        \n",
        "        # Logging\n",
        "        wandb_enabled=True,\n",
        "        wandb_project=\"predator-prey-rl-test\",\n",
        "        \n",
        "        # Checkpoints\n",
        "        supervised_checkpoint=SUPERVISED_CHECKPOINT_PATH,\n",
        "        resume_from_checkpoint=RESUME_CHECKPOINT_PATH,\n",
        "    )\n",
        "    \n",
        "elif TRAINING_SCENARIO == \"resume_training\":\n",
        "    # Resume from existing RL checkpoint\n",
        "    if not RESUME_CHECKPOINT_PATH:\n",
        "        print(\"‚ùå Please set RESUME_CHECKPOINT_PATH for resume training\")\n",
        "        raise ValueError(\"RESUME_CHECKPOINT_PATH must be set for resume training\")\n",
        "    \n",
        "    training_config = RLTrainingConfig(\n",
        "        # Environment\n",
        "        num_envs=8,\n",
        "        initial_boids=50,\n",
        "        episode_end_threshold=20,\n",
        "        max_episode_steps=1000,\n",
        "        curriculum_enabled=True,\n",
        "        \n",
        "        # Training (full)\n",
        "        total_timesteps=5_000_000,\n",
        "        checkpoint_freq=50_000,\n",
        "        eval_freq=25_000,\n",
        "        \n",
        "        # PPO (A100 optimized)\n",
        "        n_steps=2048,\n",
        "        batch_size=512,  # Increased for A100\n",
        "        n_epochs=10,\n",
        "        learning_rate=3e-4,\n",
        "        \n",
        "        # Logging\n",
        "        wandb_enabled=True,\n",
        "        wandb_project=\"predator-prey-rl\",\n",
        "        \n",
        "        # Resume\n",
        "        resume_from_checkpoint=RESUME_CHECKPOINT_PATH,\n",
        "    )\n",
        "    \n",
        "else:  # \"fresh_training\"\n",
        "    # Fresh training configuration (A100 optimized)\n",
        "    training_config = RLTrainingConfig(\n",
        "        # Environment\n",
        "        num_envs=8,  # 8 parallel environments for A100\n",
        "        initial_boids=50,\n",
        "        episode_end_threshold=20,\n",
        "        max_episode_steps=1000,\n",
        "        curriculum_enabled=True,\n",
        "        \n",
        "        # Training (full scale)\n",
        "        total_timesteps=5_000_000,  # 5M steps\n",
        "        checkpoint_freq=50_000,     # Checkpoint every 50k steps\n",
        "        eval_freq=25_000,           # Evaluate every 25k steps\n",
        "        \n",
        "        # PPO (A100 optimized)\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,               # Steps per env per update\n",
        "        batch_size=512,             # Large batch for A100 memory\n",
        "        n_epochs=10,                # Multiple epochs per update\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,              # Exploration bonus\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        \n",
        "        # Logging\n",
        "        wandb_enabled=True,\n",
        "        wandb_project=\"predator-prey-rl\",\n",
        "        tensorboard_log=\"./tensorboard_logs\",\n",
        "        \n",
        "        # Initial weights\n",
        "        supervised_checkpoint=SUPERVISED_CHECKPOINT_PATH,\n",
        "    )\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"üéØ Starting {TRAINING_SCENARIO.replace('_', ' ').title()}\")\n",
        "print(f\"üìä Configuration:\")\n",
        "print(f\"   Total timesteps: {training_config.total_timesteps:,}\")\n",
        "print(f\"   Parallel environments: {training_config.num_envs}\")\n",
        "print(f\"   Batch size: {training_config.batch_size}\")\n",
        "print(f\"   Checkpoint frequency: {training_config.checkpoint_freq:,}\")\n",
        "print(f\"   Curriculum learning: {training_config.curriculum_enabled}\")\n",
        "print(f\"   WandB logging: {training_config.wandb_enabled}\")\n",
        "\n",
        "if training_config.supervised_checkpoint:\n",
        "    print(f\"   Starting from supervised checkpoint: {training_config.supervised_checkpoint}\")\n",
        "if training_config.resume_from_checkpoint:\n",
        "    print(f\"   Resuming from RL checkpoint: {training_config.resume_from_checkpoint}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Setup training pipeline\n",
        "try:\n",
        "    model, env, checkpoint_manager, callback = setup_rl_training(training_config)\n",
        "    \n",
        "    print(f\"\\nüöÄ Starting RL Training...\")\n",
        "    print(f\"üíæ Checkpoints will be saved to: {checkpoint_manager.checkpoint_dir}\")\n",
        "    print(f\"üìà Monitor training at: http://localhost:6006 (TensorBoard)\")\n",
        "    \n",
        "    if training_config.wandb_enabled:\n",
        "        print(f\"üåê WandB dashboard: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üèÅ TRAINING STARTED\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Start training with progress bar\n",
        "    model.learn(\n",
        "        total_timesteps=training_config.total_timesteps,\n",
        "        callback=callback,\n",
        "        progress_bar=True,\n",
        "        tb_log_name=f\"PPO_{TRAINING_SCENARIO}\",\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéâ TRAINING COMPLETED!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Final checkpoint and export\n",
        "    final_checkpoint = checkpoint_manager.save_checkpoint(\n",
        "        model=model,\n",
        "        step=training_config.total_timesteps,\n",
        "        episode=len(callback.episode_rewards),\n",
        "        reward=np.mean(callback.episode_rewards) if callback.episode_rewards else 0,\n",
        "        episode_stats={\n",
        "            \"mean_reward\": np.mean(callback.episode_rewards) if callback.episode_rewards else 0,\n",
        "            \"success_rate\": np.mean(callback.episode_success_rate) if callback.episode_success_rate else 0,\n",
        "            \"total_episodes\": len(callback.episode_rewards)\n",
        "        },\n",
        "        force_save=True,\n",
        "        metadata={\"training_completed\": True, \"scenario\": TRAINING_SCENARIO}\n",
        "    )\n",
        "    \n",
        "    print(f\"üíæ Final checkpoint saved: {final_checkpoint}\")\n",
        "    print(f\"üèÜ Best model available at: {checkpoint_manager.best_checkpoint_path}\")\n",
        "    \n",
        "    # Training summary\n",
        "    if callback.episode_rewards:\n",
        "        print(f\"\\nüìä Training Summary:\")\n",
        "        print(f\"   Total episodes: {len(callback.episode_rewards)}\")\n",
        "        print(f\"   Mean reward: {np.mean(callback.episode_rewards):.2f}\")\n",
        "        print(f\"   Best reward: {max(callback.episode_rewards):.2f}\")\n",
        "        print(f\"   Success rate: {np.mean(callback.episode_success_rate)*100:.1f}%\")\n",
        "        print(f\"   Mean episode length: {np.mean(callback.episode_lengths):.1f}\")\n",
        "    \n",
        "    # Export instructions\n",
        "    print(f\"\\nüöÄ To export for JavaScript:\")\n",
        "    print(f\"   1. Best RL model: {checkpoint_manager.best_checkpoint_path}\")\n",
        "    print(f\"   2. Use homepage/export_to_js.py to convert to JavaScript format\")\n",
        "    \n",
        "    if training_config.wandb_enabled:\n",
        "        wandb.finish()\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è  Training interrupted by user\")\n",
        "    if 'model' in locals():\n",
        "        print(\"üíæ Saving interrupt checkpoint...\")\n",
        "        checkpoint_manager.save_checkpoint(\n",
        "            model=model,\n",
        "            step=model.num_timesteps,\n",
        "            episode=len(callback.episode_rewards) if 'callback' in locals() else 0,\n",
        "            reward=np.mean(callback.episode_rewards) if 'callback' in locals() and callback.episode_rewards else 0,\n",
        "            episode_stats={\"interrupted\": True},\n",
        "            force_save=True,\n",
        "            metadata={\"interrupted\": True, \"scenario\": TRAINING_SCENARIO}\n",
        "        )\n",
        "        print(\"‚úÖ Interrupt checkpoint saved\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    if 'model' in locals():\n",
        "        print(\"üíæ Saving error checkpoint...\")\n",
        "        try:\n",
        "            checkpoint_manager.save_checkpoint(\n",
        "                model=model,\n",
        "                step=model.num_timesteps,\n",
        "                episode=len(callback.episode_rewards) if 'callback' in locals() else 0,\n",
        "                reward=np.mean(callback.episode_rewards) if 'callback' in locals() and callback.episode_rewards else 0,\n",
        "                episode_stats={\"error\": str(e)},\n",
        "                force_save=True,\n",
        "                metadata={\"error\": str(e), \"scenario\": TRAINING_SCENARIO}\n",
        "            )\n",
        "            print(\"‚úÖ Error checkpoint saved\")\n",
        "        except:\n",
        "            print(\"‚ùå Failed to save error checkpoint\")\n",
        "\n",
        "print(\"\\nüéØ Training session complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üîß Training Utilities & Analysis\n",
        "\n",
        "**Post-training utilities for:**\n",
        "- üìã Checkpoint management and analysis\n",
        "- üìä Performance visualization and metrics\n",
        "- üöÄ Model export to JavaScript format\n",
        "- üéÆ Model testing and evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Training Utilities & Analysis\n",
        "# Run this cell for post-training analysis and utilities\n",
        "\n",
        "def analyze_training_checkpoints(checkpoint_dir=\"rl_checkpoints\"):\n",
        "    \"\"\"Analyze all training checkpoints and show performance trends\"\"\"\n",
        "    manager = RLCheckpointManager(checkpoint_dir=checkpoint_dir)\n",
        "    checkpoints = manager.list_checkpoints()\n",
        "    \n",
        "    if not checkpoints:\n",
        "        print(\"‚ùå No checkpoints found\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üìä Found {len(checkpoints)} checkpoints\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{'Timestamp':<20} {'Step':<10} {'Episode':<8} {'Reward':<8} {'Best':<6} {'Path'}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for cp in checkpoints:\n",
        "        timestamp = cp.get('timestamp', 'unknown')[:19] if cp.get('timestamp') else 'unknown'\n",
        "        step = cp.get('step', '?')\n",
        "        episode = cp.get('episode', '?')\n",
        "        reward = cp.get('reward', '?')\n",
        "        is_best = 'üèÜ' if cp.get('is_best', False) else ''\n",
        "        path = Path(cp.get('checkpoint_path', '')).name\n",
        "        \n",
        "        print(f\"{timestamp:<20} {str(step):<10} {str(episode):<8} {str(reward)[:6]:<8} {is_best:<6} {path}\")\n",
        "    \n",
        "    # Plot performance if we have numeric data\n",
        "    try:\n",
        "        rewards = [cp['reward'] for cp in checkpoints if isinstance(cp.get('reward'), (int, float))]\n",
        "        steps = [cp['step'] for cp in checkpoints if isinstance(cp.get('step'), (int, float))]\n",
        "        \n",
        "        if len(rewards) > 1:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            \n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(steps, rewards, 'b-o', alpha=0.7)\n",
        "            plt.xlabel('Training Steps')\n",
        "            plt.ylabel('Episode Reward')\n",
        "            plt.title('Training Progress')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.hist(rewards, bins=20, alpha=0.7, edgecolor='black')\n",
        "            plt.xlabel('Episode Reward')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Reward Distribution')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(f\"\\nüìà Performance Summary:\")\n",
        "            print(f\"   Best reward: {max(rewards):.2f}\")\n",
        "            print(f\"   Latest reward: {rewards[-1]:.2f}\")\n",
        "            print(f\"   Mean reward: {np.mean(rewards):.2f}\")\n",
        "            print(f\"   Improvement: {((rewards[-1] - rewards[0]) / abs(rewards[0]) * 100) if rewards[0] != 0 else 0:.1f}%\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not generate plots: {e}\")\n",
        "    \n",
        "    return checkpoints\n",
        "\n",
        "def test_model_performance(checkpoint_path=None, num_episodes=10):\n",
        "    \"\"\"Test a trained model's performance\"\"\"\n",
        "    print(f\"üß™ Testing model performance...\")\n",
        "    \n",
        "    # Load checkpoint manager\n",
        "    manager = RLCheckpointManager()\n",
        "    \n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = manager.best_checkpoint_path\n",
        "        if not checkpoint_path:\n",
        "            print(\"‚ùå No best model found\")\n",
        "            return\n",
        "    \n",
        "    print(f\"üì• Loading model: {checkpoint_path}\")\n",
        "    \n",
        "    # Create test environment\n",
        "    test_config = RLConfig(\n",
        "        canvas_width=800,\n",
        "        canvas_height=600,\n",
        "        initial_boids=50,\n",
        "        episode_end_threshold=20,\n",
        "        max_episode_steps=1000,\n",
        "        curriculum_enabled=False\n",
        "    )\n",
        "    \n",
        "    env = PredatorPreyRL(test_config)\n",
        "    \n",
        "    try:\n",
        "        # Load model\n",
        "        model = PPO.load(checkpoint_path, device=device)\n",
        "        \n",
        "        # Test episodes\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        episode_success = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            done = False\n",
        "            \n",
        "            while not done:\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                \n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "                done = terminated or truncated\n",
        "            \n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "            episode_success.append(terminated)  # Success if terminated (not truncated)\n",
        "            \n",
        "            print(f\"Episode {episode+1:2d}: Reward={episode_reward:6.1f}, Length={episode_length:3d}, \"\n",
        "                  f\"Success={'‚úÖ' if terminated else '‚ùå'}, Boids={info['boids_remaining']}\")\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\nüìä Test Summary ({num_episodes} episodes):\")\n",
        "        print(f\"   Mean reward: {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\")\n",
        "        print(f\"   Success rate: {np.mean(episode_success)*100:.1f}%\")\n",
        "        print(f\"   Mean length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n",
        "        print(f\"   Best reward: {max(episode_rewards):.2f}\")\n",
        "        \n",
        "        # Plot results\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        \n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(episode_rewards, 'b-o')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.title('Episode Rewards')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(episode_lengths, 'g-o')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Steps')\n",
        "        plt.title('Episode Length')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.bar(range(len(episode_success)), episode_success, alpha=0.7)\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Success')\n",
        "        plt.title('Success Rate')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return {\n",
        "            'rewards': episode_rewards,\n",
        "            'lengths': episode_lengths,\n",
        "            'success_rate': np.mean(episode_success),\n",
        "            'mean_reward': np.mean(episode_rewards)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Model testing failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def export_best_model_to_js(output_file=\"model_export_rl.js\"):\n",
        "    \"\"\"Export the best RL model to JavaScript format\"\"\"\n",
        "    print(\"üöÄ Exporting best RL model to JavaScript...\")\n",
        "    \n",
        "    manager = RLCheckpointManager()\n",
        "    \n",
        "    if not manager.best_checkpoint_path:\n",
        "        print(\"‚ùå No best model found\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"üì• Best model: {manager.best_checkpoint_path}\")\n",
        "    print(f\"üíæ Output file: {output_file}\")\n",
        "    \n",
        "    # Load the model to extract transformer weights\n",
        "    try:\n",
        "        model = PPO.load(manager.best_checkpoint_path, device='cpu')\n",
        "        transformer = model.policy.features_extractor.transformer\n",
        "        \n",
        "        # Extract state dict\n",
        "        state_dict = transformer.state_dict()\n",
        "        \n",
        "        print(f\"üß† Transformer parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
        "        \n",
        "        # Save as PyTorch checkpoint for export script\n",
        "        temp_checkpoint = \"temp_rl_transformer.pt\"\n",
        "        torch.save({\n",
        "            'model_state_dict': state_dict,\n",
        "            'architecture': {\n",
        "                'd_model': transformer.d_model,\n",
        "                'n_heads': transformer.n_heads,\n",
        "                'n_layers': transformer.n_layers,\n",
        "                'ffn_hidden': transformer.ffn_hidden\n",
        "            },\n",
        "            'source': 'RL_training',\n",
        "            'best_checkpoint': manager.best_checkpoint_path\n",
        "        }, temp_checkpoint)\n",
        "        \n",
        "        print(f\"üíæ Temporary checkpoint saved: {temp_checkpoint}\")\n",
        "        print(f\"üîß To complete export, run:\")\n",
        "        print(f\"   python homepage/export_to_js.py --checkpoint {temp_checkpoint} --output {output_file}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Export failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS - MODIFY THESE CALLS AS NEEDED\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üîß Training Utilities Available:\")\n",
        "print(\"   analyze_training_checkpoints() - Analyze all checkpoints\")\n",
        "print(\"   test_model_performance() - Test best model\")  \n",
        "print(\"   export_best_model_to_js() - Export to JavaScript\")\n",
        "print()\n",
        "\n",
        "# Uncomment the functions you want to run:\n",
        "\n",
        "# Analyze training progress\n",
        "print(\"üìä Analyzing training checkpoints...\")\n",
        "try:\n",
        "    checkpoints = analyze_training_checkpoints()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Analysis failed: {e}\")\n",
        "\n",
        "# Test model performance (uncomment to run)\n",
        "# print(\"\\nüß™ Testing model performance...\")\n",
        "# try:\n",
        "#     results = test_model_performance(num_episodes=5)\n",
        "# except Exception as e:\n",
        "#     print(f\"‚ö†Ô∏è  Testing failed: {e}\")\n",
        "\n",
        "# Export to JavaScript (uncomment to run)\n",
        "# print(\"\\nüöÄ Exporting to JavaScript...\")\n",
        "# try:\n",
        "#     export_best_model_to_js()\n",
        "# except Exception as e:\n",
        "#     print(f\"‚ö†Ô∏è  Export failed: {e}\")\n",
        "\n",
        "print(\"\\nüéØ Utilities complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéâ Reinforcement Learning Training Complete\n",
        "\n",
        "**üìã Summary of this notebook:**\n",
        "\n",
        "‚úÖ **Environment Setup**: Downloaded repository and installed RL dependencies  \n",
        "‚úÖ **RL Environment**: Created gymnasium-compatible wrapper with sparse rewards  \n",
        "‚úÖ **Transformer Integration**: Custom policy using existing transformer architecture  \n",
        "‚úÖ **PPO Training**: Stable-Baselines3 PPO with proper credit assignment  \n",
        "‚úÖ **Checkpoint Management**: Comprehensive save/load system with unique naming  \n",
        "‚úÖ **A100 Optimization**: Configured for maximum GPU utilization  \n",
        "‚úÖ **Monitoring**: TensorBoard + WandB integration  \n",
        "‚úÖ **Curriculum Learning**: Progressive difficulty with automatic advancement  \n",
        "‚úÖ **Model Export**: Ready for JavaScript deployment  \n",
        "\n",
        "**üöÄ Next Steps:**\n",
        "1. **Train**: Run the main training cell with your preferred configuration\n",
        "2. **Monitor**: Watch training progress via TensorBoard/WandB dashboards\n",
        "3. **Analyze**: Use utilities to evaluate model performance\n",
        "4. **Deploy**: Export trained model to JavaScript for production use\n",
        "\n",
        "**üéØ Key Design Principles Achieved:**\n",
        "- **End-to-End Rewards**: No intermediate rewards to avoid bias\n",
        "- **Proper Credit Assignment**: PPO handles reward distribution to actions  \n",
        "- **Robust Checkpointing**: Can resume from any point in training\n",
        "- **Production Ready**: Seamless path from training to deployment\n",
        "\n",
        "**üí° Training Tips:**\n",
        "- Start with `\"quick_test\"` to verify everything works\n",
        "- Use `supervised_checkpoint` for better initialization\n",
        "- Monitor success rate - aim for >70% before advancing curriculum\n",
        "- A100 can handle `batch_size=512` and `num_envs=8` for maximum speed\n",
        "\n",
        "Ready to train your transformer predator with reinforcement learning! üéÆü§ñ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
