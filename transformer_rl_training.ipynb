{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Codebase from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Repository information\n",
        "REPO_URL = \"https://github.com/yimianxyz/homepage.git\"\n",
        "BRANCH = \"neuro-predator\"\n",
        "REPO_DIR = \"homepage\"\n",
        "\n",
        "def download_codebase():\n",
        "    \"\"\"Download the codebase from GitHub if not already present\"\"\"\n",
        "    \n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(f\"Repository directory '{REPO_DIR}' already exists.\")\n",
        "        \n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "            # Check current branch\n",
        "            result = subprocess.run(['git', 'branch', '--show-current'],\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            current_branch = result.stdout.strip()\n",
        "            \n",
        "            if current_branch != BRANCH:\n",
        "                print(f\"Switching to branch '{BRANCH}'...\")\n",
        "                subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "            \n",
        "            # Pull latest changes\n",
        "            print(\"Updating repository...\")\n",
        "            subprocess.run(['git', 'pull', 'origin', BRANCH], check=True)\n",
        "            \n",
        "            print(f\"‚úÖ Repository updated successfully!\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error updating repository: {e}\")\n",
        "            print(\"Repository directory exists but may not be a valid git repository.\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "        \n",
        "        try:\n",
        "            # Clone the specific branch\n",
        "            subprocess.run(['git', 'clone', '-b', BRANCH, REPO_URL, REPO_DIR], check=True)\n",
        "            \n",
        "            print(f\"‚úÖ Repository cloned successfully!\")\n",
        "            \n",
        "            # Change to repository directory\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error cloning repository: {e}\")\n",
        "            print(\"Make sure you have git installed and internet connection.\")\n",
        "            return False\n",
        "    \n",
        "    # Verify key files exist\n",
        "    key_files = [\n",
        "        'config/constants.py',\n",
        "        'simulation/processors/input_processor.py',\n",
        "        'simulation/runtime/simulation_runtime.py',\n",
        "        'simulation/state_manager/state_manager.py',\n",
        "        'policy/human_prior/closest_pursuit_policy.py'\n",
        "    ]\n",
        "    \n",
        "    missing_files = []\n",
        "    for file_path in key_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Some key files are missing:\")\n",
        "        for file_path in missing_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ All key files found!\")\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Download the codebase\n",
        "success = download_codebase()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Setup complete! Ready for RL training.\")\n",
        "else:\n",
        "    print(\"‚ùå Setup failed. Please check the errors above and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and Configuration\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import math\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# Ensure we're in the correct directory and add to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'homepage':\n",
        "    print(f\"‚ö†Ô∏è  Warning: Current directory is '{project_root.name}', expected 'homepage'\")\n",
        "    print(\"Make sure the first cell downloaded the repository correctly.\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "try:\n",
        "    from config.constants import CONSTANTS\n",
        "    from simulation.processors import InputProcessor, ActionProcessor\n",
        "    from simulation.state_manager import StateManager\n",
        "    from simulation.random_state_generator import RandomStateGenerator\n",
        "    from policy.human_prior.closest_pursuit_policy import create_closest_pursuit_policy\n",
        "    \n",
        "    print(f\"‚úÖ Successfully imported all simulation modules\")\n",
        "    print(f\"üìÅ Project root: {project_root}\")\n",
        "    print(f\"üîß Key constants: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}, BOID_MAX_SPEED={CONSTANTS.BOID_MAX_SPEED}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import modules: {e}\")\n",
        "    print(\"Make sure the repository was downloaded correctly in the first cell.\")\n",
        "    raise\n",
        "\n",
        "# RL Training Configuration\n",
        "RL_CONFIG = {\n",
        "    # PPO Hyperparameters\n",
        "    'learning_rate': 3e-4,\n",
        "    'clip_epsilon': 0.2,\n",
        "    'entropy_coef': 0.01,\n",
        "    'value_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'ppo_epochs': 4,\n",
        "    'mini_batch_size': 64,\n",
        "    'rollout_steps': 2048,\n",
        "    \n",
        "    # Environment Settings\n",
        "    'min_boids': 1,\n",
        "    'max_boids': 50,\n",
        "    'min_canvas_width': 400,\n",
        "    'max_canvas_width': 1600,\n",
        "    'min_canvas_height': 400,\n",
        "    'max_canvas_height': 1200,\n",
        "    'timeout_multiplier': 3.0,  # Adaptive timeout: canvas_area * initial_boids * multiplier / 10000\n",
        "    \n",
        "    # Reward Settings\n",
        "    'catch_reward': 1.0,\n",
        "    'reward_decay_rate': 0.1,  # For exponential decay over last 50 steps\n",
        "    'reward_window': 50,  # Number of steps before catch to attribute reward\n",
        "    \n",
        "    # Training Settings\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'num_episodes': 5000,\n",
        "    'log_interval': 100,\n",
        "    'save_interval': 500,\n",
        "    'eval_interval': 200,\n",
        "    'eval_episodes': 50,\n",
        "}\n",
        "\n",
        "print(f\"\\nüöÄ RL Training Configuration:\")\n",
        "print(f\"  Device: {RL_CONFIG['device']}\")\n",
        "print(f\"  Episodes: {RL_CONFIG['num_episodes']}\")\n",
        "print(f\"  Rollout steps: {RL_CONFIG['rollout_steps']}\")\n",
        "print(f\"  Environment: {RL_CONFIG['min_boids']}-{RL_CONFIG['max_boids']} boids, {RL_CONFIG['min_canvas_width']}x{RL_CONFIG['min_canvas_height']} to {RL_CONFIG['max_canvas_width']}x{RL_CONFIG['max_canvas_height']}\")\n",
        "print(f\"  Reward: {RL_CONFIG['catch_reward']} per catch, {RL_CONFIG['reward_window']}-step attribution window\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Supervised Learning Model and Create Architecture\n",
        "from transformer_training import GEGLU, TransformerLayer, TransformerPredictor\n",
        "\n",
        "# Load SL checkpoint and extract architecture\n",
        "def load_sl_checkpoint(checkpoint_path: str = \"checkpoints/best_model.pt\"):\n",
        "    \"\"\"Load the supervised learning checkpoint and extract architecture parameters\"\"\"\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "        print(\"Available checkpoints:\")\n",
        "        if os.path.exists(\"checkpoints/\"):\n",
        "            checkpoints = list(Path(\"checkpoints/\").glob(\"*.pt\"))\n",
        "            for cp in checkpoints:\n",
        "                print(f\"  - {cp}\")\n",
        "        else:\n",
        "            print(\"  No checkpoints directory found\")\n",
        "        return None, None\n",
        "    \n",
        "    print(f\"Loading SL checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=RL_CONFIG['device'])\n",
        "    \n",
        "    # Extract architecture parameters\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        print(f\"‚úÖ Found architecture in checkpoint:\")\n",
        "        print(f\"  d_model: {arch['d_model']}\")\n",
        "        print(f\"  n_heads: {arch['n_heads']}\")\n",
        "        print(f\"  n_layers: {arch['n_layers']}\")\n",
        "        print(f\"  ffn_hidden: {arch['ffn_hidden']}\")\n",
        "        print(f\"  max_boids: {arch['max_boids']}\")\n",
        "    else:\n",
        "        print(\"‚ùå No architecture found in checkpoint - using default values\")\n",
        "        arch = {\n",
        "            'd_model': 128,\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'ffn_hidden': 512,\n",
        "            'max_boids': 50\n",
        "        }\n",
        "    \n",
        "    # Additional info\n",
        "    epoch = checkpoint.get('epoch', 'unknown')\n",
        "    val_loss = checkpoint.get('best_val_loss', 'unknown')\n",
        "    print(f\"  Checkpoint epoch: {epoch}\")\n",
        "    print(f\"  Best validation loss: {val_loss}\")\n",
        "    \n",
        "    return checkpoint, arch\n",
        "\n",
        "# Create Actor Network (uses transformer from SL)\n",
        "class ActorNetwork(nn.Module):\n",
        "    \"\"\"Actor network using the pretrained transformer\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint, architecture):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create transformer with same architecture as SL model\n",
        "        self.transformer = TransformerPredictor(\n",
        "            d_model=architecture['d_model'],\n",
        "            n_heads=architecture['n_heads'], \n",
        "            n_layers=architecture['n_layers'],\n",
        "            ffn_hidden=architecture['ffn_hidden'],\n",
        "            max_boids=architecture['max_boids'],\n",
        "            dropout=0.1\n",
        "        )\n",
        "        \n",
        "        # Load pretrained weights\n",
        "        if checkpoint is not None:\n",
        "            self.transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(\"‚úÖ Loaded pretrained SL weights into actor\")\n",
        "        \n",
        "        # Store architecture info\n",
        "        self.architecture = architecture\n",
        "        \n",
        "    def forward(self, structured_inputs):\n",
        "        \"\"\"Forward pass through transformer\"\"\"\n",
        "        # Transformer already outputs tanh(-1, 1) range\n",
        "        return self.transformer(structured_inputs)\n",
        "    \n",
        "    def get_action_and_log_prob(self, structured_inputs):\n",
        "        \"\"\"Get action and log probability for PPO\"\"\"\n",
        "        # Get mean action from transformer\n",
        "        action_mean = self.forward(structured_inputs)\n",
        "        \n",
        "        # For continuous control, we can either:\n",
        "        # 1. Use deterministic policy (no noise)\n",
        "        # 2. Add small amount of noise for exploration\n",
        "        \n",
        "        # Option 1: Deterministic (like supervised learning)\n",
        "        action = action_mean\n",
        "        # For deterministic policy, log_prob is not meaningful, but PPO needs it\n",
        "        # We'll use a very small variance Gaussian for technical compatibility\n",
        "        std = torch.ones_like(action) * 0.01  # Very small exploration noise\n",
        "        dist = torch.distributions.Normal(action_mean, std)\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "        \n",
        "        return action, log_prob\n",
        "\n",
        "# Create Critic Network (separate value function)\n",
        "class CriticNetwork(nn.Module):\n",
        "    \"\"\"Simple critic network for value estimation\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=128, hidden_dims=[256, 256]):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "            \n",
        "        layers.append(nn.Linear(prev_dim, 1))  # Single value output\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, state_features):\n",
        "        \"\"\"Estimate value from state features\"\"\"\n",
        "        return self.network(state_features).squeeze(-1)\n",
        "\n",
        "# Feature extractor for critic (simple aggregation of structured inputs)\n",
        "def extract_state_features(structured_inputs):\n",
        "    \"\"\"Extract fixed-size features from structured inputs for critic\"\"\"\n",
        "    batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "    \n",
        "    if isinstance(structured_inputs, dict):\n",
        "        structured_inputs = [structured_inputs]\n",
        "    \n",
        "    features = []\n",
        "    \n",
        "    for sample in structured_inputs:\n",
        "        # Context features (2D)\n",
        "        context_feat = [sample['context']['canvasWidth'], sample['context']['canvasHeight']]\n",
        "        \n",
        "        # Predator features (2D) \n",
        "        predator_feat = [sample['predator']['velX'], sample['predator']['velY']]\n",
        "        \n",
        "        # Boid features (aggregate statistics)\n",
        "        boids = sample['boids']\n",
        "        if len(boids) > 0:\n",
        "            # Statistical aggregation of boids\n",
        "            rel_x = [b['relX'] for b in boids]\n",
        "            rel_y = [b['relY'] for b in boids]\n",
        "            vel_x = [b['velX'] for b in boids]\n",
        "            vel_y = [b['velY'] for b in boids]\n",
        "            \n",
        "            boid_feat = [\n",
        "                len(boids),  # Number of boids\n",
        "                np.mean(rel_x), np.std(rel_x), np.min(rel_x), np.max(rel_x),  # Position stats\n",
        "                np.mean(rel_y), np.std(rel_y), np.min(rel_y), np.max(rel_y),\n",
        "                np.mean(vel_x), np.std(vel_x), np.min(vel_x), np.max(vel_x),  # Velocity stats\n",
        "                np.mean(vel_y), np.std(vel_y), np.min(vel_y), np.max(vel_y),\n",
        "                # Distance to closest boid\n",
        "                np.min([math.sqrt(b['relX']**2 + b['relY']**2) for b in boids])\n",
        "            ]\n",
        "        else:\n",
        "            # No boids remaining\n",
        "            boid_feat = [0] + [0.0] * 16  # 17 features total\n",
        "        \n",
        "        # Combine all features (2 + 2 + 17 = 21 features)\n",
        "        sample_feat = context_feat + predator_feat + boid_feat\n",
        "        features.append(sample_feat)\n",
        "    \n",
        "    return torch.tensor(features, dtype=torch.float32, device=RL_CONFIG['device'])\n",
        "\n",
        "# Load checkpoint and create networks\n",
        "checkpoint, architecture = load_sl_checkpoint()\n",
        "\n",
        "if checkpoint is not None:\n",
        "    # Create actor and critic networks\n",
        "    actor = ActorNetwork(checkpoint, architecture).to(RL_CONFIG['device'])\n",
        "    critic = CriticNetwork(input_dim=21).to(RL_CONFIG['device'])  # 21 features from extract_state_features\n",
        "    \n",
        "    # Count parameters\n",
        "    actor_params = sum(p.numel() for p in actor.parameters())\n",
        "    critic_params = sum(p.numel() for p in critic.parameters())\n",
        "    \n",
        "    print(f\"\\nüß† Networks created:\")\n",
        "    print(f\"  Actor parameters: {actor_params:,} (pretrained transformer)\")\n",
        "    print(f\"  Critic parameters: {critic_params:,} (new MLP)\")\n",
        "    print(f\"  Total parameters: {actor_params + critic_params:,}\")\n",
        "    \n",
        "    # Test forward pass\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        test_action, test_log_prob = actor.get_action_and_log_prob(test_input)\n",
        "        test_features = extract_state_features(test_input)\n",
        "        test_value = critic(test_features)\n",
        "        \n",
        "    print(f\"\\nüß™ Test forward pass:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Actor output: [{test_action[0].item():.4f}, {test_action[1].item():.4f}]\")\n",
        "    print(f\"  Log prob: {test_log_prob.item():.4f}\")\n",
        "    print(f\"  State features shape: {test_features.shape}\")\n",
        "    print(f\"  Critic value: {test_value.item():.4f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Networks ready for RL training!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot proceed without SL checkpoint\")\n",
        "    actor = critic = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RL Environment Wrapper\n",
        "class PredatorEnvironment:\n",
        "    \"\"\"RL Environment wrapper for the predator-boids simulation\"\"\"\n",
        "    \n",
        "    def __init__(self, config=RL_CONFIG):\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialize simulation components\n",
        "        self.state_manager = StateManager()\n",
        "        self.random_generator = RandomStateGenerator()\n",
        "        \n",
        "        # Environment state\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 0\n",
        "        self.initial_boids_count = 0\n",
        "        self.episode_catches = []\n",
        "        self.step_history = []  # For reward attribution\n",
        "        \n",
        "        # Metrics tracking\n",
        "        self.reset_episode_stats()\n",
        "        \n",
        "    def reset_episode_stats(self):\n",
        "        \"\"\"Reset episode-level statistics\"\"\"\n",
        "        self.episode_catches = []\n",
        "        self.step_history = []\n",
        "        self.current_step = 0\n",
        "        \n",
        "    def calculate_adaptive_timeout(self, canvas_width, canvas_height, num_boids):\n",
        "        \"\"\"Calculate adaptive timeout based on environment size and complexity\"\"\"\n",
        "        canvas_area = canvas_width * canvas_height\n",
        "        base_timeout = (canvas_area * num_boids * self.config['timeout_multiplier']) / 10000\n",
        "        return max(int(base_timeout), 200)  # Minimum 200 steps\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment for new episode\"\"\"\n",
        "        self.reset_episode_stats()\n",
        "        \n",
        "        # Generate random environment parameters\n",
        "        num_boids = random.randint(self.config['min_boids'], self.config['max_boids'])\n",
        "        canvas_width = random.randint(self.config['min_canvas_width'], self.config['max_canvas_width'])\n",
        "        canvas_height = random.randint(self.config['min_canvas_height'], self.config['max_canvas_height'])\n",
        "        \n",
        "        # Generate random initial state\n",
        "        initial_state = self.random_generator.generate_scattered_state(\n",
        "            num_boids, canvas_width, canvas_height\n",
        "        )\n",
        "        \n",
        "        # Initialize state manager with dummy policy (we'll override actions)\n",
        "        dummy_policy = create_closest_pursuit_policy()  # Not used, just for initialization\n",
        "        self.state_manager.init(initial_state, dummy_policy)\n",
        "        \n",
        "        # Set episode parameters\n",
        "        self.initial_boids_count = num_boids\n",
        "        self.max_steps = self.calculate_adaptive_timeout(canvas_width, canvas_height, num_boids)\n",
        "        self.current_step = 0\n",
        "        \n",
        "        # Get initial observation\n",
        "        current_state = self.state_manager.get_state()\n",
        "        observation = self._state_to_structured_inputs(current_state)\n",
        "        \n",
        "        return observation, {\n",
        "            'canvas_width': canvas_width,\n",
        "            'canvas_height': canvas_height,\n",
        "            'initial_boids': num_boids,\n",
        "            'max_steps': self.max_steps\n",
        "        }\n",
        "    \n",
        "    def _state_to_structured_inputs(self, state):\n",
        "        \"\"\"Convert state to structured inputs format\"\"\"\n",
        "        # Use input processor to convert state\n",
        "        input_processor = InputProcessor()\n",
        "        \n",
        "        # Extract data from state\n",
        "        boids = state['boids_states']\n",
        "        predator_pos = state['predator_state']['position']\n",
        "        predator_vel = state['predator_state']['velocity']\n",
        "        canvas_width = state['canvas_width']\n",
        "        canvas_height = state['canvas_height']\n",
        "        \n",
        "        # Convert to structured inputs\n",
        "        structured_inputs = input_processor.process_inputs(\n",
        "            boids, predator_pos, predator_vel, canvas_width, canvas_height\n",
        "        )\n",
        "        \n",
        "        return structured_inputs\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Take one step in the environment\"\"\"\n",
        "        # Convert action to the format expected by state manager\n",
        "        action_processor = ActionProcessor()\n",
        "        game_actions = action_processor.process_action([action[0].item(), action[1].item()])\n",
        "        \n",
        "        # Store current state for reward attribution\n",
        "        current_state = self.state_manager.get_state()\n",
        "        boids_before = len(current_state['boids_states'])\n",
        "        \n",
        "        # Manual step (bypass state manager's policy)\n",
        "        predator_action = {\n",
        "            'force_x': game_actions[0],\n",
        "            'force_y': game_actions[1]\n",
        "        }\n",
        "        \n",
        "        # Import simulation step function directly\n",
        "        from simulation.runtime.simulation_runtime import simulation_step\n",
        "        \n",
        "        # Run simulation step\n",
        "        step_result = simulation_step(\n",
        "            current_state['boids_states'],\n",
        "            current_state['predator_state'],\n",
        "            predator_action,\n",
        "            current_state['canvas_width'],\n",
        "            current_state['canvas_height']\n",
        "        )\n",
        "        \n",
        "        # Update state (remove caught boids)\n",
        "        caught_boids = step_result['caught_boids']\n",
        "        new_boids_states = step_result['boids_states']\n",
        "        \n",
        "        # Remove caught boids in reverse order to maintain indices\n",
        "        for i in reversed(caught_boids):\n",
        "            new_boids_states.pop(i)\n",
        "        \n",
        "        # Update state manager's internal state\n",
        "        new_state = {\n",
        "            'boids_states': new_boids_states,\n",
        "            'predator_state': step_result['predator_state'],\n",
        "            'canvas_width': current_state['canvas_width'],\n",
        "            'canvas_height': current_state['canvas_height']\n",
        "        }\n",
        "        \n",
        "        # Manually update state manager\n",
        "        self.state_manager.current_state = new_state\n",
        "        \n",
        "        # Record step information\n",
        "        step_info = {\n",
        "            'step': self.current_step,\n",
        "            'action': action.clone(),\n",
        "            'boids_before': boids_before,\n",
        "            'boids_after': len(new_boids_states),\n",
        "            'catches': len(caught_boids)\n",
        "        }\n",
        "        self.step_history.append(step_info)\n",
        "        \n",
        "        # Track catches\n",
        "        if len(caught_boids) > 0:\n",
        "            for _ in caught_boids:\n",
        "                self.episode_catches.append(self.current_step)\n",
        "        \n",
        "        self.current_step += 1\n",
        "        \n",
        "        # Check termination conditions\n",
        "        done = False\n",
        "        termination_reason = None\n",
        "        \n",
        "        if len(new_boids_states) == 0:\n",
        "            done = True\n",
        "            termination_reason = \"all_caught\"\n",
        "        elif self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            termination_reason = \"timeout\"\n",
        "        \n",
        "        # Calculate reward\n",
        "        reward = self._calculate_reward()\n",
        "        \n",
        "        # Get next observation\n",
        "        observation = self._state_to_structured_inputs(new_state)\n",
        "        \n",
        "        # Info dictionary\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'catches_this_step': len(caught_boids),\n",
        "            'total_catches': len(self.episode_catches),\n",
        "            'boids_remaining': len(new_boids_states),\n",
        "            'done': done,\n",
        "            'termination_reason': termination_reason,\n",
        "            'reward': reward\n",
        "        }\n",
        "        \n",
        "        return observation, reward, done, info\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward based on recent catches with temporal credit assignment\"\"\"\n",
        "        total_reward = 0.0\n",
        "        \n",
        "        # Only calculate reward if we have catches\n",
        "        if len(self.episode_catches) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # For each catch, attribute reward to the last N steps\n",
        "        for catch_step in self.episode_catches:\n",
        "            # Define the reward window (steps before the catch)\n",
        "            reward_start = max(0, catch_step - self.config['reward_window'] + 1)\n",
        "            reward_end = catch_step + 1\n",
        "            \n",
        "            # Only attribute reward to current step if it's within the window\n",
        "            if reward_start <= self.current_step - 1 < reward_end:\n",
        "                # Calculate steps before catch (for decay)\n",
        "                steps_before_catch = catch_step - (self.current_step - 1)\n",
        "                \n",
        "                # Exponential decay: more recent actions get higher reward\n",
        "                decay_factor = math.exp(-self.config['reward_decay_rate'] * steps_before_catch)\n",
        "                step_reward = self.config['catch_reward'] * decay_factor\n",
        "                \n",
        "                total_reward += step_reward\n",
        "        \n",
        "        return total_reward\n",
        "\n",
        "# Test the environment\n",
        "if actor is not None:\n",
        "    print(\"üåç Testing RL Environment...\")\n",
        "    \n",
        "    # Create test environment\n",
        "    test_env = PredatorEnvironment()\n",
        "    \n",
        "    # Test reset\n",
        "    obs, info = test_env.reset()\n",
        "    print(f\"  Environment reset:\")\n",
        "    print(f\"    Canvas: {info['canvas_width']}x{info['canvas_height']}\")\n",
        "    print(f\"    Boids: {info['initial_boids']}\")\n",
        "    print(f\"    Max steps: {info['max_steps']}\")\n",
        "    print(f\"    Observation boids: {len(obs['boids'])}\")\n",
        "    \n",
        "    # Test a few steps\n",
        "    print(f\"  Testing steps:\")\n",
        "    for i in range(3):\n",
        "        with torch.no_grad():\n",
        "            action, _ = actor.get_action_and_log_prob(obs)\n",
        "            \n",
        "        obs, reward, done, info = test_env.step(action)\n",
        "        \n",
        "        print(f\"    Step {i+1}: action=[{action[0]:.3f}, {action[1]:.3f}], \"\n",
        "              f\"reward={reward:.3f}, boids={info['boids_remaining']}, \"\n",
        "              f\"catches={info['catches_this_step']}\")\n",
        "        \n",
        "        if done:\n",
        "            print(f\"    Episode done: {info['termination_reason']}\")\n",
        "            break\n",
        "    \n",
        "    print(\"‚úÖ Environment test completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Skipping environment test - actor not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO Implementation\n",
        "class PPOTrainer:\n",
        "    \"\"\"PPO trainer for the predator agent\"\"\"\n",
        "    \n",
        "    def __init__(self, actor, critic, config=RL_CONFIG):\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.config = config\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.Adam(actor.parameters(), lr=config['learning_rate'])\n",
        "        self.critic_optimizer = optim.Adam(critic.parameters(), lr=config['learning_rate'])\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.reset_buffer()\n",
        "        \n",
        "        # Training metrics\n",
        "        self.training_stats = defaultdict(list)\n",
        "        \n",
        "    def reset_buffer(self):\n",
        "        \"\"\"Reset experience buffer\"\"\"\n",
        "        self.buffer = {\n",
        "            'observations': [],\n",
        "            'actions': [],\n",
        "            'rewards': [],\n",
        "            'values': [],\n",
        "            'log_probs': [],\n",
        "            'dones': [],\n",
        "            'returns': [],\n",
        "            'advantages': []\n",
        "        }\n",
        "        \n",
        "    def collect_rollout(self, env, num_steps):\n",
        "        \"\"\"Collect a rollout of experiences\"\"\"\n",
        "        self.reset_buffer()\n",
        "        \n",
        "        # Reset environment for new rollout\n",
        "        obs, info = env.reset()\n",
        "        \n",
        "        rollout_stats = {\n",
        "            'episode_rewards': [],\n",
        "            'episode_lengths': [],\n",
        "            'catches_per_episode': [],\n",
        "            'episodes_completed': 0\n",
        "        }\n",
        "        \n",
        "        current_episode_reward = 0\n",
        "        current_episode_length = 0\n",
        "        current_episode_catches = 0\n",
        "        \n",
        "        for step in range(num_steps):\n",
        "            # Get action and value\n",
        "            with torch.no_grad():\n",
        "                action, log_prob = self.actor.get_action_and_log_prob(obs)\n",
        "                state_features = extract_state_features(obs)\n",
        "                value = self.critic(state_features)\n",
        "            \n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, step_info = env.step(action)\n",
        "            \n",
        "            # Store experience\n",
        "            self.buffer['observations'].append(obs)\n",
        "            self.buffer['actions'].append(action)\n",
        "            self.buffer['rewards'].append(reward)\n",
        "            self.buffer['values'].append(value)\n",
        "            self.buffer['log_probs'].append(log_prob)\n",
        "            self.buffer['dones'].append(done)\n",
        "            \n",
        "            # Update episode stats\n",
        "            current_episode_reward += reward\n",
        "            current_episode_length += 1\n",
        "            current_episode_catches += step_info['catches_this_step']\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                # Episode finished\n",
        "                rollout_stats['episode_rewards'].append(current_episode_reward)\n",
        "                rollout_stats['episode_lengths'].append(current_episode_length)\n",
        "                rollout_stats['catches_per_episode'].append(current_episode_catches)\n",
        "                rollout_stats['episodes_completed'] += 1\n",
        "                \n",
        "                # Reset for next episode\n",
        "                obs, info = env.reset()\n",
        "                current_episode_reward = 0\n",
        "                current_episode_length = 0\n",
        "                current_episode_catches = 0\n",
        "        \n",
        "        # Calculate returns and advantages\n",
        "        self._calculate_returns_and_advantages(obs)\n",
        "        \n",
        "        return rollout_stats\n",
        "    \n",
        "    def _calculate_returns_and_advantages(self, final_obs):\n",
        "        \"\"\"Calculate returns and advantages using GAE\"\"\"\n",
        "        # Get final value estimate\n",
        "        with torch.no_grad():\n",
        "            final_state_features = extract_state_features(final_obs)\n",
        "            final_value = self.critic(final_state_features).item()\n",
        "        \n",
        "        # Convert to tensors\n",
        "        rewards = torch.tensor(self.buffer['rewards'], dtype=torch.float32, device=self.config['device'])\n",
        "        values = torch.stack(self.buffer['values'])\n",
        "        dones = torch.tensor(self.buffer['dones'], dtype=torch.float32, device=self.config['device'])\n",
        "        \n",
        "        # Calculate returns (discounted cumulative rewards)\n",
        "        returns = []\n",
        "        advantages = []\n",
        "        \n",
        "        # Simple return calculation (no GAE for now)\n",
        "        gamma = 0.99  # Discount factor\n",
        "        gae_lambda = 0.95  # GAE lambda\n",
        "        \n",
        "        # Calculate returns\n",
        "        returns_tensor = torch.zeros_like(rewards)\n",
        "        running_return = final_value\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if dones[t]:\n",
        "                running_return = 0\n",
        "            running_return = rewards[t] + gamma * running_return\n",
        "            returns_tensor[t] = running_return\n",
        "        \n",
        "        # Calculate advantages (simple baseline)\n",
        "        advantages_tensor = returns_tensor - values.squeeze()\n",
        "        \n",
        "        # Normalize advantages\n",
        "        if len(advantages_tensor) > 1:\n",
        "            advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "        \n",
        "        self.buffer['returns'] = returns_tensor\n",
        "        self.buffer['advantages'] = advantages_tensor\n",
        "    \n",
        "    def update_policy(self):\n",
        "        \"\"\"Update actor and critic using PPO\"\"\"\n",
        "        # Convert buffer to tensors\n",
        "        observations = self.buffer['observations']\n",
        "        actions = torch.stack(self.buffer['actions'])\n",
        "        old_log_probs = torch.stack(self.buffer['log_probs'])\n",
        "        returns = self.buffer['returns']\n",
        "        advantages = self.buffer['advantages']\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(self.config['ppo_epochs']):\n",
        "            # Shuffle data\n",
        "            indices = torch.randperm(len(observations))\n",
        "            \n",
        "            # Mini-batch training\n",
        "            for i in range(0, len(observations), self.config['mini_batch_size']):\n",
        "                batch_indices = indices[i:i+self.config['mini_batch_size']]\n",
        "                \n",
        "                # Get batch data\n",
        "                batch_obs = [observations[idx] for idx in batch_indices]\n",
        "                batch_actions = actions[batch_indices]\n",
        "                batch_old_log_probs = old_log_probs[batch_indices]\n",
        "                batch_returns = returns[batch_indices]\n",
        "                batch_advantages = advantages[batch_indices]\n",
        "                \n",
        "                # Forward pass\n",
        "                _, new_log_probs = self.actor.get_action_and_log_prob(batch_obs)\n",
        "                batch_state_features = extract_state_features(batch_obs)\n",
        "                new_values = self.critic(batch_state_features)\n",
        "                \n",
        "                # PPO loss calculations\n",
        "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
        "                \n",
        "                # Actor loss (PPO clipping)\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.config['clip_epsilon'], 1.0 + self.config['clip_epsilon']) * batch_advantages\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                \n",
        "                # Critic loss\n",
        "                critic_loss = F.mse_loss(new_values, batch_returns)\n",
        "                \n",
        "                # Entropy bonus (for exploration)\n",
        "                entropy = -new_log_probs.mean()  # Simple entropy approximation\n",
        "                entropy_loss = -self.config['entropy_coef'] * entropy\n",
        "                \n",
        "                # Total losses\n",
        "                total_actor_loss = actor_loss + entropy_loss\n",
        "                total_critic_loss = critic_loss\n",
        "                \n",
        "                # Update actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                total_actor_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.config['max_grad_norm'])\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Update critic\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                total_critic_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config['max_grad_norm'])\n",
        "                self.critic_optimizer.step()\n",
        "        \n",
        "        # Store training statistics\n",
        "        self.training_stats['actor_loss'].append(actor_loss.item())\n",
        "        self.training_stats['critic_loss'].append(critic_loss.item())\n",
        "        self.training_stats['entropy'].append(entropy.item())\n",
        "        \n",
        "        return {\n",
        "            'actor_loss': actor_loss.item(),\n",
        "            'critic_loss': critic_loss.item(),\n",
        "            'entropy': entropy.item()\n",
        "        }\n",
        "    \n",
        "    def save_checkpoint(self, filepath, episode, stats):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'episode': episode,\n",
        "            'actor_state_dict': self.actor.state_dict(),\n",
        "            'critic_state_dict': self.critic.state_dict(),\n",
        "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
        "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
        "            'training_stats': dict(self.training_stats),\n",
        "            'config': self.config,\n",
        "            'architecture': self.actor.architecture,\n",
        "            'episode_stats': stats,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"‚úÖ Saved RL checkpoint: {filepath}\")\n",
        "\n",
        "# Initialize PPO trainer\n",
        "if actor is not None and critic is not None:\n",
        "    ppo_trainer = PPOTrainer(actor, critic, RL_CONFIG)\n",
        "    print(\"üéØ PPO Trainer initialized!\")\n",
        "    print(f\"  Actor learning rate: {RL_CONFIG['learning_rate']}\")\n",
        "    print(f\"  Critic learning rate: {RL_CONFIG['learning_rate']}\")\n",
        "    print(f\"  Clip epsilon: {RL_CONFIG['clip_epsilon']}\")\n",
        "    print(f\"  Mini-batch size: {RL_CONFIG['mini_batch_size']}\")\n",
        "    print(\"‚úÖ Ready for RL training!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot initialize PPO trainer - networks not available\")\n",
        "    ppo_trainer = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop with Metrics\n",
        "class TrainingMetrics:\n",
        "    \"\"\"Track and manage training metrics\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset all metrics\"\"\"\n",
        "        self.metrics = {\n",
        "            'episode_rewards': [],\n",
        "            'episode_lengths': [],\n",
        "            'catches_per_episode': [],\n",
        "            'success_rate': [],  # Episodes with at least 1 catch\n",
        "            'catch_efficiency': [],  # Catches per step\n",
        "            'steps_to_first_catch': [],\n",
        "            'total_episodes': 0,\n",
        "            'total_steps': 0,\n",
        "            'total_catches': 0,\n",
        "            \n",
        "            # Training losses\n",
        "            'actor_losses': [],\n",
        "            'critic_losses': [],\n",
        "            'entropy': [],\n",
        "            \n",
        "            # Environment stats\n",
        "            'canvas_sizes': [],\n",
        "            'initial_boids': [],\n",
        "            'timeout_rate': []\n",
        "        }\n",
        "        \n",
        "    def update_episode(self, reward, length, catches, first_catch_step, canvas_size, initial_boids, termination_reason):\n",
        "        \"\"\"Update metrics with episode data\"\"\"\n",
        "        self.metrics['episode_rewards'].append(reward)\n",
        "        self.metrics['episode_lengths'].append(length)\n",
        "        self.metrics['catches_per_episode'].append(catches)\n",
        "        \n",
        "        # Success rate (episodes with at least 1 catch)\n",
        "        success = 1 if catches > 0 else 0\n",
        "        self.metrics['success_rate'].append(success)\n",
        "        \n",
        "        # Catch efficiency (catches per step)\n",
        "        efficiency = catches / length if length > 0 else 0\n",
        "        self.metrics['catch_efficiency'].append(efficiency)\n",
        "        \n",
        "        # Steps to first catch\n",
        "        if first_catch_step is not None:\n",
        "            self.metrics['steps_to_first_catch'].append(first_catch_step)\n",
        "        \n",
        "        # Environment stats\n",
        "        self.metrics['canvas_sizes'].append(canvas_size)\n",
        "        self.metrics['initial_boids'].append(initial_boids)\n",
        "        self.metrics['timeout_rate'].append(1 if termination_reason == 'timeout' else 0)\n",
        "        \n",
        "        # Global counters\n",
        "        self.metrics['total_episodes'] += 1\n",
        "        self.metrics['total_steps'] += length\n",
        "        self.metrics['total_catches'] += catches\n",
        "    \n",
        "    def update_training(self, actor_loss, critic_loss, entropy):\n",
        "        \"\"\"Update training loss metrics\"\"\"\n",
        "        self.metrics['actor_losses'].append(actor_loss)\n",
        "        self.metrics['critic_losses'].append(critic_loss)\n",
        "        self.metrics['entropy'].append(entropy)\n",
        "    \n",
        "    def get_recent_stats(self, window=100):\n",
        "        \"\"\"Get statistics for recent episodes\"\"\"\n",
        "        if len(self.metrics['episode_rewards']) == 0:\n",
        "            return {}\n",
        "            \n",
        "        recent_rewards = self.metrics['episode_rewards'][-window:]\n",
        "        recent_lengths = self.metrics['episode_lengths'][-window:]\n",
        "        recent_catches = self.metrics['catches_per_episode'][-window:]\n",
        "        recent_success = self.metrics['success_rate'][-window:]\n",
        "        recent_efficiency = self.metrics['catch_efficiency'][-window:]\n",
        "        recent_first_catch = self.metrics['steps_to_first_catch'][-window:] if self.metrics['steps_to_first_catch'] else []\n",
        "        recent_timeout = self.metrics['timeout_rate'][-window:]\n",
        "        \n",
        "        stats = {\n",
        "            'episodes': len(recent_rewards),\n",
        "            'avg_reward': np.mean(recent_rewards),\n",
        "            'avg_length': np.mean(recent_lengths),\n",
        "            'avg_catches': np.mean(recent_catches),\n",
        "            'success_rate': np.mean(recent_success),\n",
        "            'avg_efficiency': np.mean(recent_efficiency),\n",
        "            'avg_first_catch': np.mean(recent_first_catch) if recent_first_catch else None,\n",
        "            'timeout_rate': np.mean(recent_timeout),\n",
        "            'total_catches': sum(recent_catches)\n",
        "        }\n",
        "        \n",
        "        return stats\n",
        "\n",
        "def train_rl_agent(ppo_trainer, num_updates=1000):\n",
        "    \"\"\"Main RL training loop\"\"\"\n",
        "    \n",
        "    if ppo_trainer is None:\n",
        "        print(\"‚ùå Cannot start training - PPO trainer not available\")\n",
        "        return\n",
        "        \n",
        "    print(f\"üöÄ Starting RL training for {num_updates} updates...\")\n",
        "    print(f\"  Rollout steps per update: {RL_CONFIG['rollout_steps']}\")\n",
        "    print(f\"  Total environment steps: {num_updates * RL_CONFIG['rollout_steps']:,}\")\n",
        "    \n",
        "    # Initialize environment and metrics\n",
        "    env = PredatorEnvironment(RL_CONFIG)\n",
        "    metrics = TrainingMetrics()\n",
        "    \n",
        "    # Create checkpoints directory\n",
        "    os.makedirs(\"rl_checkpoints\", exist_ok=True)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for update in range(num_updates):\n",
        "        update_start = time.time()\n",
        "        \n",
        "        # Collect rollout\n",
        "        rollout_stats = ppo_trainer.collect_rollout(env, RL_CONFIG['rollout_steps'])\n",
        "        \n",
        "        # Update policy\n",
        "        training_stats = ppo_trainer.update_policy()\n",
        "        \n",
        "        # Update metrics\n",
        "        for i, (reward, length, catches) in enumerate(zip(\n",
        "            rollout_stats['episode_rewards'],\n",
        "            rollout_stats['episode_lengths'], \n",
        "            rollout_stats['catches_per_episode']\n",
        "        )):\\n            # Estimate first catch step (simplified)\\n            first_catch = length // 2 if catches > 0 else None\\n            \\n            # Environment stats (simplified)\\n            canvas_size = 800 * 600  # Placeholder\\n            initial_boids = 25  # Placeholder\\n            termination_reason = 'timeout' if catches == 0 else 'caught'\\n            \\n            metrics.update_episode(\\n                reward, length, catches, first_catch, \\n                canvas_size, initial_boids, termination_reason\\n            )\\n        \\n        metrics.update_training(\\n            training_stats['actor_loss'],\\n            training_stats['critic_loss'], \\n            training_stats['entropy']\\n        )\\n        \\n        update_time = time.time() - update_start\\n        \\n        # Logging\\n        if (update + 1) % RL_CONFIG['log_interval'] == 0:\\n            recent_stats = metrics.get_recent_stats(RL_CONFIG['log_interval'])\\n            elapsed_time = time.time() - start_time\\n            \\n            print(f\\\"\\\\nüìä Update {update+1}/{num_updates} (Episode {metrics.metrics['total_episodes']})\\\")\\n            print(f\\\"   Time: {elapsed_time:.1f}s, Update time: {update_time:.2f}s\\\")\\n            print(f\\\"   Episodes: {rollout_stats['episodes_completed']}, Total steps: {metrics.metrics['total_steps']:,}\\\")\\n            \\n            if recent_stats['episodes'] > 0:\\n                print(f\\\"   Recent {recent_stats['episodes']} episodes:\\\")\\n                print(f\\\"     Avg reward: {recent_stats['avg_reward']:.3f}\\\")\\n                print(f\\\"     Avg length: {recent_stats['avg_length']:.1f} steps\\\")\\n                print(f\\\"     Avg catches: {recent_stats['avg_catches']:.1f}\\\")\\n                print(f\\\"     Success rate: {recent_stats['success_rate']:.1%}\\\")\\n                print(f\\\"     Catch efficiency: {recent_stats['avg_efficiency']:.4f} catches/step\\\")\\n                print(f\\\"     Timeout rate: {recent_stats['timeout_rate']:.1%}\\\")\\n            \\n            print(f\\\"   Training:\\\")\\n            print(f\\\"     Actor loss: {training_stats['actor_loss']:.4f}\\\")\\n            print(f\\\"     Critic loss: {training_stats['critic_loss']:.4f}\\\")\\n            print(f\\\"     Entropy: {training_stats['entropy']:.4f}\\\")\\n        \\n        # Save checkpoint\\n        if (update + 1) % RL_CONFIG['save_interval'] == 0:\\n            checkpoint_path = f\\\"rl_checkpoints/rl_checkpoint_{update+1}.pt\\\"\\n            ppo_trainer.save_checkpoint(checkpoint_path, update + 1, recent_stats)\\n            \\n            # Also save as latest\\n            latest_path = \\\"rl_checkpoints/latest.pt\\\"\\n            ppo_trainer.save_checkpoint(latest_path, update + 1, recent_stats)\\n    \\n    total_time = time.time() - start_time\\n    \\n    print(f\\\"\\\\nüéâ RL Training completed!\\\")\\n    print(f\\\"  Total time: {total_time:.1f}s ({total_time/60:.1f}m)\\\")\\n    print(f\\\"  Total episodes: {metrics.metrics['total_episodes']}\\\")\\n    print(f\\\"  Total steps: {metrics.metrics['total_steps']:,}\\\")\\n    print(f\\\"  Total catches: {metrics.metrics['total_catches']}\\\")\\n    \\n    final_stats = metrics.get_recent_stats(200)\\n    if final_stats['episodes'] > 0:\\n        print(f\\\"\\\\nüìà Final performance (last 200 episodes):\\\")\\n        print(f\\\"  Average reward: {final_stats['avg_reward']:.3f}\\\")\\n        print(f\\\"  Average catches: {final_stats['avg_catches']:.1f}\\\")\\n        print(f\\\"  Success rate: {final_stats['success_rate']:.1%}\\\")\\n        print(f\\\"  Catch efficiency: {final_stats['avg_efficiency']:.4f}\\\")\\n    \\n    return metrics\\n\\n# Quick training test (small scale)\\nif ppo_trainer is not None:\\n    print(\\\"üß™ Quick training test (5 updates)...\\\")\\n    \\n    # Very small test\\n    test_metrics = train_rl_agent(ppo_trainer, num_updates=5)\\n    \\n    print(\\\"‚úÖ Training test completed! Ready for full training.\\\")\\n    \\n    # Uncomment below for full training\\n    # print(\\\"\\\\nüöÄ Starting full RL training...\\\")\\n    # full_metrics = train_rl_agent(ppo_trainer, num_updates=500)\\nelse:\\n    print(\\\"‚ùå Skipping training test - PPO trainer not available\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation, Visualization and Model Export\n",
        "def evaluate_rl_agent(actor, critic, num_episodes=50, verbose=True):\n",
        "    \"\"\"Evaluate the RL-trained agent\"\"\"\n",
        "    \n",
        "    if actor is None:\n",
        "        print(\"‚ùå Cannot evaluate - actor not available\")\n",
        "        return None\n",
        "        \n",
        "    print(f\"üìä Evaluating RL agent for {num_episodes} episodes...\")\n",
        "    \n",
        "    env = PredatorEnvironment(RL_CONFIG)\n",
        "    eval_stats = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        obs, info = env.reset()\n",
        "        \n",
        "        episode_reward = 0\n",
        "        episode_catches = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                action, _ = actor.get_action_and_log_prob(obs)\n",
        "                \n",
        "            obs, reward, done, step_info = env.step(action)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_catches += step_info['catches_this_step']\n",
        "            episode_length += 1\n",
        "        \n",
        "        eval_stats.append({\n",
        "            'reward': episode_reward,\n",
        "            'length': episode_length,\n",
        "            'catches': episode_catches,\n",
        "            'success': episode_catches > 0,\n",
        "            'efficiency': episode_catches / episode_length if episode_length > 0 else 0,\n",
        "            'termination': step_info['termination_reason'],\n",
        "            'canvas_size': info['canvas_width'] * info['canvas_height'],\n",
        "            'initial_boids': info['initial_boids']\n",
        "        })\n",
        "        \n",
        "        if verbose and (episode + 1) % 10 == 0:\n",
        "            recent_stats = eval_stats[-10:]\n",
        "            avg_catches = np.mean([s['catches'] for s in recent_stats])\n",
        "            success_rate = np.mean([s['success'] for s in recent_stats])\n",
        "            print(f\"  Episodes {episode-8}-{episode+1}: {avg_catches:.1f} catches/ep, {success_rate:.1%} success\")\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    summary = {\n",
        "        'total_episodes': len(eval_stats),\n",
        "        'avg_reward': np.mean([s['reward'] for s in eval_stats]),\n",
        "        'avg_catches': np.mean([s['catches'] for s in eval_stats]),\n",
        "        'avg_length': np.mean([s['length'] for s in eval_stats]),\n",
        "        'success_rate': np.mean([s['success'] for s in eval_stats]),\n",
        "        'avg_efficiency': np.mean([s['efficiency'] for s in eval_stats]),\n",
        "        'timeout_rate': np.mean([1 if s['termination'] == 'timeout' else 0 for s in eval_stats]),\n",
        "        'total_catches': sum([s['catches'] for s in eval_stats])\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìà Evaluation Results:\")\n",
        "    print(f\"  Episodes: {summary['total_episodes']}\")\n",
        "    print(f\"  Average reward: {summary['avg_reward']:.3f}\")\n",
        "    print(f\"  Average catches: {summary['avg_catches']:.2f}\")\n",
        "    print(f\"  Success rate: {summary['success_rate']:.1%}\")\n",
        "    print(f\"  Average efficiency: {summary['avg_efficiency']:.4f} catches/step\")\n",
        "    print(f\"  Average episode length: {summary['avg_length']:.1f} steps\")\n",
        "    print(f\"  Timeout rate: {summary['timeout_rate']:.1%}\")\n",
        "    print(f\"  Total catches: {summary['total_catches']}\")\n",
        "    \n",
        "    return eval_stats, summary\n",
        "\n",
        "def plot_training_metrics(metrics):\n",
        "    \"\"\"Plot training progress\"\"\"\n",
        "    \n",
        "    if metrics is None or len(metrics.metrics['episode_rewards']) == 0:\n",
        "        print(\"‚ùå No metrics to plot\")\n",
        "        return\n",
        "        \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # Episode rewards\n",
        "    axes[0,0].plot(metrics.metrics['episode_rewards'])\n",
        "    axes[0,0].set_title('Episode Rewards')\n",
        "    axes[0,0].set_xlabel('Episode')\n",
        "    axes[0,0].set_ylabel('Reward')\n",
        "    axes[0,0].grid(True)\n",
        "    \n",
        "    # Catches per episode\n",
        "    axes[0,1].plot(metrics.metrics['catches_per_episode'])\n",
        "    axes[0,1].set_title('Catches per Episode')\n",
        "    axes[0,1].set_xlabel('Episode')\n",
        "    axes[0,1].set_ylabel('Catches')\n",
        "    axes[0,1].grid(True)\n",
        "    \n",
        "    # Success rate (rolling average)\n",
        "    if len(metrics.metrics['success_rate']) > 10:\n",
        "        window = min(50, len(metrics.metrics['success_rate']) // 10)\n",
        "        success_smooth = np.convolve(metrics.metrics['success_rate'], \n",
        "                                   np.ones(window)/window, mode='valid')\n",
        "        axes[0,2].plot(success_smooth)\n",
        "        axes[0,2].set_title(f'Success Rate (rolling avg, window={window})')\n",
        "        axes[0,2].set_xlabel('Episode')\n",
        "        axes[0,2].set_ylabel('Success Rate')\n",
        "        axes[0,2].grid(True)\n",
        "    \n",
        "    # Training losses\n",
        "    if len(metrics.metrics['actor_losses']) > 0:\n",
        "        axes[1,0].plot(metrics.metrics['actor_losses'], label='Actor Loss')\n",
        "        axes[1,0].plot(metrics.metrics['critic_losses'], label='Critic Loss')\n",
        "        axes[1,0].set_title('Training Losses')\n",
        "        axes[1,0].set_xlabel('Update')\n",
        "        axes[1,0].set_ylabel('Loss')\n",
        "        axes[1,0].legend()\n",
        "        axes[1,0].grid(True)\n",
        "    \n",
        "    # Catch efficiency\n",
        "    axes[1,1].plot(metrics.metrics['catch_efficiency'])\n",
        "    axes[1,1].set_title('Catch Efficiency')\n",
        "    axes[1,1].set_xlabel('Episode')\n",
        "    axes[1,1].set_ylabel('Catches per Step')\n",
        "    axes[1,1].grid(True)\n",
        "    \n",
        "    # Episode lengths\n",
        "    axes[1,2].plot(metrics.metrics['episode_lengths'])\n",
        "    axes[1,2].set_title('Episode Lengths')\n",
        "    axes[1,2].set_xlabel('Episode')\n",
        "    axes[1,2].set_ylabel('Steps')\n",
        "    axes[1,2].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìä Training metrics plotted!\")\n",
        "\n",
        "def export_rl_model(actor, output_path=\"policy/transformer/models/rl_model.js\"):\n",
        "    \"\"\"Export RL-trained transformer to JavaScript\"\"\"\n",
        "    \n",
        "    if actor is None:\n",
        "        print(\"‚ùå Cannot export - actor not available\")\n",
        "        return False\n",
        "        \n",
        "    print(f\"üîÑ Exporting RL-trained model to JavaScript format...\")\n",
        "    \n",
        "    # Save PyTorch checkpoint first\n",
        "    checkpoint_path = \"rl_checkpoints/export_checkpoint.pt\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': actor.transformer.state_dict(),\n",
        "        'episode': 'rl_export',\n",
        "        'architecture': actor.architecture,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'training_type': 'reinforcement_learning'\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"‚úÖ Saved RL checkpoint for export: {checkpoint_path}\")\n",
        "    \n",
        "    # Use export_to_js.py script\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"export_to_js.py\",\n",
        "        \"--checkpoint\", checkpoint_path,\n",
        "        \"--output\", output_path\n",
        "    ], capture_output=True, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ RL model exported to: {output_path}\")\n",
        "        print(\"üéâ RL-trained model ready for browser deployment!\")\n",
        "        print(result.stdout)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Export failed:\")\n",
        "        print(result.stderr)\n",
        "        return False\n",
        "\n",
        "def compare_sl_vs_rl(sl_actor, rl_actor, num_episodes=20):\n",
        "    \"\"\"Compare SL vs RL performance side by side\"\"\"\n",
        "    \n",
        "    if sl_actor is None or rl_actor is None:\n",
        "        print(\"‚ùå Cannot compare - need both SL and RL actors\")\n",
        "        return\n",
        "        \n",
        "    print(f\"‚öîÔ∏è  Comparing SL vs RL performance ({num_episodes} episodes each)...\")\n",
        "    \n",
        "    # Create fresh actors (reload SL weights)\n",
        "    sl_eval_actor = ActorNetwork(checkpoint, architecture).to(RL_CONFIG['device'])\n",
        "    \n",
        "    # Evaluate both\n",
        "    print(\"\\\\nüìä Evaluating SL model...\")\n",
        "    sl_stats, sl_summary = evaluate_rl_agent(sl_eval_actor, None, num_episodes, verbose=False)\n",
        "    \n",
        "    print(\"\\\\nüìä Evaluating RL model...\")\n",
        "    rl_stats, rl_summary = evaluate_rl_agent(rl_actor, None, num_episodes, verbose=False)\n",
        "    \n",
        "    # Comparison\n",
        "    print(f\"\\\\n‚öîÔ∏è  SL vs RL Comparison:\")\n",
        "    print(f\"  Metric                | SL Model | RL Model | Improvement\")\n",
        "    print(f\"  ---------------------|----------|----------|------------\")\n",
        "    print(f\"  Avg Catches          | {sl_summary['avg_catches']:8.2f} | {rl_summary['avg_catches']:8.2f} | {((rl_summary['avg_catches']/max(sl_summary['avg_catches'],0.001))-1)*100:+7.1f}%\")\n",
        "    print(f\"  Success Rate         | {sl_summary['success_rate']:7.1%} | {rl_summary['success_rate']:7.1%} | {((rl_summary['success_rate']/max(sl_summary['success_rate'],0.001))-1)*100:+7.1f}%\")\n",
        "    print(f\"  Catch Efficiency     | {sl_summary['avg_efficiency']:8.4f} | {rl_summary['avg_efficiency']:8.4f} | {((rl_summary['avg_efficiency']/max(sl_summary['avg_efficiency'],0.0001))-1)*100:+7.1f}%\")\n",
        "    print(f\"  Timeout Rate         | {sl_summary['timeout_rate']:7.1%} | {rl_summary['timeout_rate']:7.1%} | {((rl_summary['timeout_rate']/max(sl_summary['timeout_rate'],0.001))-1)*100:+7.1f}%\")\n",
        "    print(f\"  Avg Episode Length   | {sl_summary['avg_length']:8.1f} | {rl_summary['avg_length']:8.1f} | {((rl_summary['avg_length']/max(sl_summary['avg_length'],0.1))-1)*100:+7.1f}%\")\n",
        "    \n",
        "    return sl_stats, rl_stats\n",
        "\n",
        "# Run evaluation and visualization\n",
        "if ppo_trainer is not None and hasattr(ppo_trainer, 'training_stats'):\n",
        "    print(\"üìä Running evaluation and generating plots...\")\n",
        "    \n",
        "    # Evaluate current model\n",
        "    eval_stats, eval_summary = evaluate_rl_agent(actor, critic, num_episodes=20)\n",
        "    \n",
        "    # Plot metrics if we have training data\n",
        "    if hasattr(ppo_trainer, 'training_stats') and test_metrics is not None:\n",
        "        plot_training_metrics(test_metrics)\n",
        "    \n",
        "    print(\"‚úÖ Evaluation completed!\")\n",
        "    \n",
        "    # Uncomment below to export RL model\n",
        "    # print(\"\\\\nüîÑ Exporting RL model...\")\n",
        "    # export_success = export_rl_model(actor)\n",
        "    \n",
        "    # Uncomment below for SL vs RL comparison  \n",
        "    # print(\"\\\\n‚öîÔ∏è  Running SL vs RL comparison...\")\n",
        "    # sl_stats, rl_stats = compare_sl_vs_rl(actor, actor, num_episodes=10)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Skipping evaluation - training not completed or data not available\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"üéØ RL TRAINING PIPELINE READY!\")\n",
        "print(\"=\"*60) \n",
        "print(\"‚úÖ All components initialized and tested\")\n",
        "print(\"‚úÖ Environment, PPO trainer, and metrics ready\")\n",
        "print(\"‚úÖ Quick test completed successfully\")\n",
        "print(\"\")\n",
        "print(\"üöÄ To start full training, uncomment and run:\")\n",
        "print(\"   # full_metrics = train_rl_agent(ppo_trainer, num_updates=500)\")\n",
        "print(\"\")\n",
        "print(\"üìä Key metrics to monitor during training:\")\n",
        "print(\"  - Success rate (episodes with ‚â•1 catch)\")\n",
        "print(\"  - Average catches per episode\")\n",
        "print(\"  - Catch efficiency (catches per step)\")\n",
        "print(\"  - Episode length and timeout rate\")\n",
        "print(\"\")\n",
        "print(\"üéÆ After training, export model with:\")\n",
        "print(\"   # export_rl_model(actor)\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
