{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7GTsZ-9ZAmVl",
        "outputId": "ad05f1a6-1d7f-4b75-8f9d-e06eded3e283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository from https://github.com/yimianxyz/homepage.git (branch: neuro-predator)...\n",
            "‚úÖ Repository cloned successfully!\n",
            "‚úÖ All key files found!\n",
            "üìÅ Working directory: /content/homepage/homepage/homepage\n",
            "\n",
            "üéâ Setup complete! Ready for RL training.\n"
          ]
        }
      ],
      "source": [
        "# Setup: Download Codebase and Verify Environment\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Repository information\n",
        "REPO_URL = \"https://github.com/yimianxyz/homepage.git\"\n",
        "BRANCH = \"neuro-predator\"\n",
        "REPO_DIR = \"homepage\"\n",
        "\n",
        "def download_codebase():\n",
        "    \"\"\"Download the codebase from GitHub if not already present\"\"\"\n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(f\"Repository directory '{REPO_DIR}' already exists.\")\n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            # Check current branch\n",
        "            result = subprocess.run(['git', 'branch', '--show-current'],\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            current_branch = result.stdout.strip()\n",
        "            if current_branch != BRANCH:\n",
        "                print(f\"Switching to branch '{BRANCH}'...\")\n",
        "                subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "            # Pull latest changes\n",
        "            print(\"Updating repository...\")\n",
        "            subprocess.run(['git', 'pull', 'origin', BRANCH], check=True)\n",
        "            print(f\"‚úÖ Repository updated successfully!\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error updating repository: {e}\")\n",
        "    else:\n",
        "        print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "        try:\n",
        "            subprocess.run(['git', 'clone', '-b', BRANCH, REPO_URL, REPO_DIR], check=True)\n",
        "            print(f\"‚úÖ Repository cloned successfully!\")\n",
        "            os.chdir(REPO_DIR)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error cloning repository: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Verify key files exist\n",
        "    key_files = [\n",
        "        'config/constants.py',\n",
        "        'simulation/processors/input_processor.py',\n",
        "        'simulation/state_manager/state_manager.py',\n",
        "        'policy/human_prior/closest_pursuit_policy.py',\n",
        "        'export_to_js.py'\n",
        "    ]\n",
        "\n",
        "    missing_files = []\n",
        "    for file_path in key_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_path)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Some key files are missing:\")\n",
        "        for file_path in missing_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"‚úÖ All key files found!\")\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "    return True\n",
        "\n",
        "# Download and setup codebase\n",
        "success = download_codebase()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Setup complete! Ready for RL training.\")\n",
        "else:\n",
        "    print(\"‚ùå Setup failed. Please check the errors above and try again.\")\n",
        "    raise RuntimeError(\"Failed to setup codebase\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pcbLsycaAmVn",
        "outputId": "075cc2be-8c8e-4bb9-e623-cf4d8e02719b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully imported all simulation modules\n",
            "üìÅ Project root: /content/homepage/homepage/homepage\n",
            "üîß Key constants: MAX_DISTANCE=2000, BOID_MAX_SPEED=3.5\n",
            "üöÄ Device: cuda\n",
            "\n",
            "üìã Configuration:\n",
            "  Environment: {'max_episode_steps': 3000, 'reward_window': 50, 'base_catch_reward': 1.0, 'canvas_width_range': (320, 3840), 'canvas_height_range': (320, 2160), 'boid_count_range': (1, 50)}\n",
            "  PPO: {'learning_rate': 3e-05, 'batch_size': 256, 'rollout_length': 2048, 'epochs_per_update': 4, 'clip_epsilon': 0.1, 'value_loss_coef': 0.5, 'entropy_coef': 0.01, 'gae_lambda': 0.95, 'gamma': 0.99}\n",
            "‚úÖ Configuration loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import Dependencies and Setup Configuration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import time\n",
        "from collections import deque, defaultdict\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "import copy\n",
        "\n",
        "# Ensure we're in the correct directory and add to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'homepage':\n",
        "    print(f\"‚ö†Ô∏è  Warning: Current directory is '{project_root.name}', expected 'homepage'\")\n",
        "    print(\"Make sure the first cell downloaded the repository correctly.\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "try:\n",
        "    from config.constants import CONSTANTS\n",
        "    from simulation.processors import InputProcessor, ActionProcessor\n",
        "    from simulation.state_manager import StateManager\n",
        "    from simulation.random_state_generator import RandomStateGenerator\n",
        "    from policy.human_prior.closest_pursuit_policy import create_closest_pursuit_policy\n",
        "\n",
        "    print(f\"‚úÖ Successfully imported all simulation modules\")\n",
        "    print(f\"üìÅ Project root: {project_root}\")\n",
        "    print(f\"üîß Key constants: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}, BOID_MAX_SPEED={CONSTANTS.BOID_MAX_SPEED}\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import modules: {e}\")\n",
        "    print(\"Make sure the repository was downloaded correctly in the first cell.\")\n",
        "    raise\n",
        "\n",
        "# RL Training Configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Device: {DEVICE}\")\n",
        "\n",
        "# Environment Configuration\n",
        "ENV_CONFIG = {\n",
        "    'max_episode_steps': 3000,         # Prevent endless episodes\n",
        "    'reward_window': 50,               # Last N steps before catch get reward\n",
        "    'base_catch_reward': 1.0,          # Base reward for catching a boid\n",
        "    'canvas_width_range': (320, 3840), # Same as SL data\n",
        "    'canvas_height_range': (320, 2160), # Same as SL data\n",
        "    'boid_count_range': (1, 50),       # Same as SL data\n",
        "}\n",
        "\n",
        "# PPO Configuration\n",
        "PPO_CONFIG = {\n",
        "    'learning_rate': 3e-5,        # Lower than SL training\n",
        "    'batch_size': 256,            # Same as SL\n",
        "    'rollout_length': 2048,       # Steps per update\n",
        "    'epochs_per_update': 4,       # PPO epochs\n",
        "    'clip_epsilon': 0.1,          # PPO clip parameter\n",
        "    'value_loss_coef': 0.5,       # Value loss weight\n",
        "    'entropy_coef': 0.01,         # Exploration bonus\n",
        "    'gae_lambda': 0.95,           # GAE parameter\n",
        "    'gamma': 0.99,                # Discount factor\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã Configuration:\")\n",
        "print(f\"  Environment: {ENV_CONFIG}\")\n",
        "print(f\"  PPO: {PPO_CONFIG}\")\n",
        "print(f\"‚úÖ Configuration loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eBkePklaAmVn",
        "outputId": "bee232c6-efcf-42a2-e91f-b4336be46492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è  No SL checkpoint found. Creating random initialized model for testing...\n",
            "‚úÖ Created model with default architecture: {'d_model': 128, 'n_heads': 8, 'n_layers': 4, 'ffn_hidden': 512, 'max_boids': 50}\n",
            "\n",
            "üéØ SL Model loaded and ready for RL fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "# Load SL Trained Model as Starting Point\n",
        "# First, we need the transformer model definition from SL training\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return x * torch.nn.functional.gelu(gate)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # GEGLU FFN with separate projections for export compatibility\n",
        "        self.ffn_gate_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_up_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_down_proj = nn.Linear(ffn_hidden, d_model)\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.self_attn(normed, normed, normed, key_padding_mask=padding_mask)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # FFN with residual\n",
        "        normed = self.norm2(x)\n",
        "        gate = torch.nn.functional.gelu(self.ffn_gate_proj(normed))\n",
        "        up = self.ffn_up_proj(normed)\n",
        "        ffn_out = self.ffn_down_proj(gate * up)\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, d_model=128, n_heads=8, n_layers=4, ffn_hidden=512, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "\n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "\n",
        "        # Input projections\n",
        "        self.ctx_projection = nn.Linear(2, d_model)  # canvas_width, canvas_height\n",
        "        self.predator_projection = nn.Linear(4, d_model)  # velX, velY, 0, 0 (padded to 4D)\n",
        "        self.boid_projection = nn.Linear(4, d_model)  # relX, relY, velX, velY\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)  # predator action [x, y]\n",
        "\n",
        "    def forward(self, structured_inputs, padding_mask=None):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "\n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "\n",
        "        # Build token sequences for each sample in batch\n",
        "        sequences = []\n",
        "        masks = []\n",
        "\n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "\n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "\n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']],\n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "\n",
        "            # Predator token - expand to 4D\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "\n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]  # CLS, CTX, Predator are not padding\n",
        "\n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "\n",
        "            # Pad to max_boids + 3 (CLS + CTX + Predator)\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)  # Mark as padding\n",
        "\n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "\n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Create padding mask\n",
        "        if padding_mask is None:\n",
        "            padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        # Extract CLS token and project to output\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        action = self.output_projection(cls_output)  # [batch_size, 2]\n",
        "\n",
        "        # Apply tanh to ensure [-1, 1] range\n",
        "        action = torch.tanh(action)\n",
        "\n",
        "        return action.squeeze(0) if batch_size == 1 else action\n",
        "\n",
        "def load_sl_checkpoint(checkpoint_path: str):\n",
        "    \"\"\"Load the SL trained model from checkpoint\"\"\"\n",
        "    print(f\"Loading SL checkpoint from {checkpoint_path}...\")\n",
        "\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "        return None, None\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    # Extract architecture from checkpoint\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        print(f\"‚úÖ Found architecture in checkpoint: {arch}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No architecture found, using default SL architecture\")\n",
        "        arch = {'d_model': 128, 'n_heads': 8, 'n_layers': 4, 'ffn_hidden': 512, 'max_boids': 50}\n",
        "\n",
        "    # Create model with correct architecture\n",
        "    model = TransformerPredictor(\n",
        "        d_model=arch['d_model'],\n",
        "        n_heads=arch['n_heads'],\n",
        "        n_layers=arch['n_layers'],\n",
        "        ffn_hidden=arch['ffn_hidden'],\n",
        "        max_boids=arch.get('max_boids', 50),\n",
        "        dropout=0.1\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Load model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"‚úÖ Loaded SL model successfully:\")\n",
        "    print(f\"  Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
        "    print(f\"  Best val loss: {checkpoint.get('best_val_loss', 'unknown')}\")\n",
        "    print(f\"  Architecture: {arch['d_model']}√ó{arch['n_heads']}√ó{arch['n_layers']}√ó{arch['ffn_hidden']}\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    return model, arch\n",
        "\n",
        "# Try to load SL checkpoint - adjust path as needed\n",
        "sl_checkpoint_paths = [\n",
        "    \"checkpoints/best_model.pt\",\n",
        "    \"checkpoints/model_epoch_20.pt\",\n",
        "    \"checkpoints/model_epoch_10.pt\",\n",
        "    \"checkpoints/model_epoch_5.pt\"\n",
        "]\n",
        "\n",
        "sl_model = None\n",
        "sl_architecture = None\n",
        "\n",
        "for checkpoint_path in sl_checkpoint_paths:\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        sl_model, sl_architecture = load_sl_checkpoint(checkpoint_path)\n",
        "        if sl_model is not None:\n",
        "            break\n",
        "\n",
        "if sl_model is None:\n",
        "    print(\"‚ö†Ô∏è  No SL checkpoint found. Creating random initialized model for testing...\")\n",
        "    # Create model with default architecture for testing\n",
        "    default_arch = {'d_model': 128, 'n_heads': 8, 'n_layers': 4, 'ffn_hidden': 512, 'max_boids': 50}\n",
        "    sl_model = TransformerPredictor(**default_arch).to(DEVICE)\n",
        "    sl_architecture = default_arch\n",
        "    print(f\"‚úÖ Created model with default architecture: {default_arch}\")\n",
        "\n",
        "print(f\"\\nüéØ SL Model loaded and ready for RL fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zeZzxZQgAmVo",
        "outputId": "56318d78-d488-42dd-eed5-b010636bd945",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing SL model with sample inputs...\n",
            "‚úÖ Single input test:\n",
            "  Input: 3 boids\n",
            "  Output: [0.1647, 0.0989]\n",
            "  Output range: [0.0989, 0.1647]\n",
            "‚úÖ Batch input test:\n",
            "  Batch size: 3\n",
            "  Output shape: torch.Size([3, 2])\n",
            "  Output sample: [0.1647, 0.0989]\n",
            "‚úÖ Testing with different boid counts:\n",
            "    1 boids: [0.490, -0.183]\n",
            "    5 boids: [0.000, 0.250]\n",
            "    10 boids: [-0.189, 0.400]\n",
            "    25 boids: [-0.389, 0.495]\n",
            "    50 boids: [-0.447, 0.530]\n",
            "üéâ All SL model tests passed!\n",
            "üî¨ Testing edge cases...\n",
            "‚úÖ No boids test: [0.6813, -0.3783]\n",
            "‚úÖ Extreme values test: [0.3799, 0.0528]\n",
            "üéâ All edge case tests passed!\n",
            "\n",
            "‚úÖ SL Model is working correctly and ready for RL training!\n"
          ]
        }
      ],
      "source": [
        "# Test SL Model to Ensure It Works Properly\n",
        "def test_sl_model():\n",
        "    \"\"\"Test the loaded SL model with sample inputs\"\"\"\n",
        "    print(\"üß™ Testing SL model with sample inputs...\")\n",
        "\n",
        "    # Create test input (same format as used in training)\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4},\n",
        "            {'relX': 0.3, 'relY': -0.1, 'velX': 0.2, 'velY': 0.3}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Test single input\n",
        "    sl_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = sl_model(test_input)\n",
        "\n",
        "    print(f\"‚úÖ Single input test:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Output: [{output[0]:.4f}, {output[1]:.4f}]\")\n",
        "    print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "\n",
        "    # Verify output is in correct range\n",
        "    assert output.shape == (2,), f\"Expected output shape (2,), got {output.shape}\"\n",
        "    assert torch.all(output >= -1) and torch.all(output <= 1), f\"Output not in [-1, 1] range: {output}\"\n",
        "\n",
        "    # Test batch input\n",
        "    batch_inputs = [test_input, test_input, test_input]\n",
        "    with torch.no_grad():\n",
        "        batch_output = sl_model(batch_inputs)\n",
        "\n",
        "    print(f\"‚úÖ Batch input test:\")\n",
        "    print(f\"  Batch size: {len(batch_inputs)}\")\n",
        "    print(f\"  Output shape: {batch_output.shape}\")\n",
        "    print(f\"  Output sample: [{batch_output[0][0]:.4f}, {batch_output[0][1]:.4f}]\")\n",
        "\n",
        "    assert batch_output.shape == (3, 2), f\"Expected batch output shape (3, 2), got {batch_output.shape}\"\n",
        "    assert torch.all(batch_output >= -1) and torch.all(batch_output <= 1), f\"Batch output not in [-1, 1] range\"\n",
        "\n",
        "    # Test with varying number of boids\n",
        "    print(f\"‚úÖ Testing with different boid counts:\")\n",
        "    for num_boids in [1, 5, 10, 25, 50]:\n",
        "        test_boids = []\n",
        "        for i in range(num_boids):\n",
        "            test_boids.append({\n",
        "                'relX': random.uniform(-0.5, 0.5),\n",
        "                'relY': random.uniform(-0.5, 0.5),\n",
        "                'velX': random.uniform(-0.5, 0.5),\n",
        "                'velY': random.uniform(-0.5, 0.5)\n",
        "            })\n",
        "\n",
        "        test_input_var = {\n",
        "            'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "            'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "            'boids': test_boids\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_var = sl_model(test_input_var)\n",
        "\n",
        "        print(f\"    {num_boids} boids: [{output_var[0]:.3f}, {output_var[1]:.3f}]\")\n",
        "        assert output_var.shape == (2,), f\"Failed for {num_boids} boids\"\n",
        "\n",
        "    print(f\"üéâ All SL model tests passed!\")\n",
        "    return True\n",
        "\n",
        "# Test empty boids case\n",
        "def test_edge_cases():\n",
        "    \"\"\"Test edge cases for the model\"\"\"\n",
        "    print(\"üî¨ Testing edge cases...\")\n",
        "\n",
        "    # Test with no boids\n",
        "    empty_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': []\n",
        "    }\n",
        "\n",
        "    sl_model.eval()\n",
        "    with torch.no_grad():\n",
        "        empty_output = sl_model(empty_input)\n",
        "\n",
        "    print(f\"‚úÖ No boids test: [{empty_output[0]:.4f}, {empty_output[1]:.4f}]\")\n",
        "    assert empty_output.shape == (2,), \"Failed no boids test\"\n",
        "\n",
        "    # Test with extreme values\n",
        "    extreme_input = {\n",
        "        'context': {'canvasWidth': 1.0, 'canvasHeight': 1.0},\n",
        "        'predator': {'velX': 1.0, 'velY': -1.0},\n",
        "        'boids': [\n",
        "            {'relX': 1.0, 'relY': 1.0, 'velX': 1.0, 'velY': 1.0},\n",
        "            {'relX': -1.0, 'relY': -1.0, 'velX': -1.0, 'velY': -1.0}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        extreme_output = sl_model(extreme_input)\n",
        "\n",
        "    print(f\"‚úÖ Extreme values test: [{extreme_output[0]:.4f}, {extreme_output[1]:.4f}]\")\n",
        "    assert extreme_output.shape == (2,), \"Failed extreme values test\"\n",
        "    assert torch.all(extreme_output >= -1) and torch.all(extreme_output <= 1), \"Extreme output out of range\"\n",
        "\n",
        "    print(f\"üéâ All edge case tests passed!\")\n",
        "    return True\n",
        "\n",
        "# Run tests\n",
        "try:\n",
        "    test_sl_model()\n",
        "    test_edge_cases()\n",
        "    print(f\"\\n‚úÖ SL Model is working correctly and ready for RL training!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå SL Model test failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gf3AXGIzAmVp",
        "outputId": "99893d21-ceb3-4668-9a32-f7256101a8cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèóÔ∏è Creating and testing RL environment...\n",
            "‚úÖ BoidsRLEnvironment initialized:\n",
            "  Max episode steps: 3000\n",
            "  Reward window: 50\n",
            "  Base catch reward: 1.0\n",
            "\n",
            "üß™ Testing environment reset...\n",
            "‚úÖ Reset successful:\n",
            "  Initial boids: 1\n",
            "  Canvas size: 1.425 x 0.853\n",
            "  Predator velocity: [0.424, -0.090]\n",
            "\n",
            "üß™ Testing environment steps...\n",
            "  Step 1: action=[-0.187, -0.874], reward=0.000, boids=1, done=False\n",
            "  Step 2: action=[-0.036, 0.377], reward=0.000, boids=1, done=False\n",
            "  Step 3: action=[0.193, 0.424], reward=0.000, boids=1, done=False\n",
            "  Step 4: action=[-0.879, -0.480], reward=0.000, boids=1, done=False\n",
            "  Step 5: action=[0.886, 0.416], reward=0.000, boids=1, done=False\n",
            "\n",
            "‚úÖ Environment test completed!\n",
            "  Final episode stats: {'boids_caught': 0, 'total_reward': 0.0, 'catch_steps': [], 'efficiency': 0.0, 'steps_per_catch': inf}\n",
            "üéØ RL Environment is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Create RL Environment Wrapper\n",
        "class BoidsRLEnvironment:\n",
        "    \"\"\"RL Environment wrapper for boids simulation with retrospective rewards\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or ENV_CONFIG\n",
        "\n",
        "        # Initialize simulation components\n",
        "        self.state_generator = RandomStateGenerator()\n",
        "        self.state_manager = StateManager()\n",
        "        self.input_processor = InputProcessor()\n",
        "        self.action_processor = ActionProcessor()\n",
        "\n",
        "        # Episode tracking\n",
        "        self.step_count = 0\n",
        "        self.episode_buffer = []  # Store (state, action, reward) for retrospective rewards\n",
        "        self.current_state = None\n",
        "        self.initial_boid_count = 0\n",
        "\n",
        "        # Episode statistics\n",
        "        self.episode_stats = {\n",
        "            'boids_caught': 0,\n",
        "            'total_reward': 0.0,\n",
        "            'catch_steps': [],  # Steps when catches occurred\n",
        "        }\n",
        "\n",
        "        print(f\"‚úÖ BoidsRLEnvironment initialized:\")\n",
        "        print(f\"  Max episode steps: {self.config['max_episode_steps']}\")\n",
        "        print(f\"  Reward window: {self.config['reward_window']}\")\n",
        "        print(f\"  Base catch reward: {self.config['base_catch_reward']}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment for new episode\"\"\"\n",
        "        # Generate random episode configuration\n",
        "        canvas_width = random.randint(*self.config['canvas_width_range'])\n",
        "        canvas_height = random.randint(*self.config['canvas_height_range'])\n",
        "        num_boids = random.randint(*self.config['boid_count_range'])\n",
        "\n",
        "        # Generate initial state\n",
        "        self.current_state = self.state_generator.generate_scattered_state(\n",
        "            num_boids, canvas_width, canvas_height\n",
        "        )\n",
        "\n",
        "        # Reset episode tracking\n",
        "        self.step_count = 0\n",
        "        self.episode_buffer = []\n",
        "        self.initial_boid_count = len(self.current_state['boids_states'])\n",
        "\n",
        "        # Reset episode statistics\n",
        "        self.episode_stats = {\n",
        "            'boids_caught': 0,\n",
        "            'total_reward': 0.0,\n",
        "            'catch_steps': [],\n",
        "        }\n",
        "\n",
        "        # Get initial observation\n",
        "        observation = self.input_processor.process_inputs(\n",
        "            self.current_state['boids_states'],\n",
        "            self.current_state['predator_state']['position'],\n",
        "            self.current_state['predator_state']['velocity'],\n",
        "            self.current_state['canvas_width'],\n",
        "            self.current_state['canvas_height']\n",
        "        )\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one environment step\"\"\"\n",
        "        # Store current step in buffer (reward will be assigned retrospectively)\n",
        "        step_data = {\n",
        "            'step': self.step_count,\n",
        "            'observation': self.get_current_observation(),\n",
        "            'action': action.copy(),\n",
        "            'reward': 0.0,  # Will be updated retrospectively\n",
        "        }\n",
        "        self.episode_buffer.append(step_data)\n",
        "\n",
        "        # Convert action to simulation format\n",
        "        action_forces = self.action_processor.process_action(action)\n",
        "        predator_action = {\n",
        "            'force_x': action_forces[0],\n",
        "            'force_y': action_forces[1]\n",
        "        }\n",
        "\n",
        "        # Run simulation step\n",
        "        from simulation.runtime.simulation_runtime import simulation_step\n",
        "        step_result = simulation_step(\n",
        "            self.current_state['boids_states'],\n",
        "            self.current_state['predator_state'],\n",
        "            predator_action,\n",
        "            self.current_state['canvas_width'],\n",
        "            self.current_state['canvas_height']\n",
        "        )\n",
        "\n",
        "        # Update state and process catches\n",
        "        caught_boids = step_result['caught_boids']\n",
        "        self.current_state['boids_states'] = step_result['boids_states']\n",
        "        self.current_state['predator_state'] = step_result['predator_state']\n",
        "\n",
        "        # Remove caught boids (in reverse order to maintain indices)\n",
        "        for i in reversed(caught_boids):\n",
        "            self.current_state['boids_states'].pop(i)\n",
        "\n",
        "        # Assign retrospective rewards for catches\n",
        "        if caught_boids:\n",
        "            self.episode_stats['boids_caught'] += len(caught_boids)\n",
        "            self.episode_stats['catch_steps'].append(self.step_count)\n",
        "            self._assign_retrospective_rewards(self.step_count)\n",
        "\n",
        "        # Update step count\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Check termination conditions\n",
        "        done = self._is_episode_done()\n",
        "\n",
        "        # Get current reward (from buffer)\n",
        "        current_reward = step_data['reward']\n",
        "        self.episode_stats['total_reward'] += current_reward\n",
        "\n",
        "        # Get next observation\n",
        "        if not done:\n",
        "            next_observation = self.get_current_observation()\n",
        "        else:\n",
        "            next_observation = None\n",
        "\n",
        "        # Episode info\n",
        "        info = {\n",
        "            'boids_remaining': len(self.current_state['boids_states']),\n",
        "            'boids_caught_this_step': len(caught_boids),\n",
        "            'step_count': self.step_count,\n",
        "            'episode_stats': self.episode_stats.copy()\n",
        "        }\n",
        "\n",
        "        return next_observation, current_reward, done, info\n",
        "\n",
        "    def _assign_retrospective_rewards(self, catch_step):\n",
        "        \"\"\"Assign retrospective rewards to recent steps that led to catch\"\"\"\n",
        "        reward_window = self.config['reward_window']\n",
        "        base_reward = self.config['base_catch_reward']\n",
        "\n",
        "        # Assign rewards to last N steps (more recent = higher reward)\n",
        "        for i in range(reward_window):\n",
        "            step_idx = catch_step - i\n",
        "            if step_idx >= 0 and step_idx < len(self.episode_buffer):\n",
        "                # Linear decay: more recent steps get higher reward\n",
        "                reward_multiplier = (reward_window - i) / reward_window\n",
        "                additional_reward = base_reward * reward_multiplier\n",
        "                self.episode_buffer[step_idx]['reward'] += additional_reward\n",
        "\n",
        "    def _is_episode_done(self):\n",
        "        \"\"\"Check if episode should terminate\"\"\"\n",
        "        # Timeout\n",
        "        if self.step_count >= self.config['max_episode_steps']:\n",
        "            return True\n",
        "\n",
        "        # All boids caught\n",
        "        if len(self.current_state['boids_states']) == 0:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_current_observation(self):\n",
        "        \"\"\"Get current observation in structured format\"\"\"\n",
        "        return self.input_processor.process_inputs(\n",
        "            self.current_state['boids_states'],\n",
        "            self.current_state['predator_state']['position'],\n",
        "            self.current_state['predator_state']['velocity'],\n",
        "            self.current_state['canvas_width'],\n",
        "            self.current_state['canvas_height']\n",
        "        )\n",
        "\n",
        "    def get_episode_buffer(self):\n",
        "        \"\"\"Get the complete episode buffer with retrospective rewards\"\"\"\n",
        "        return self.episode_buffer.copy()\n",
        "\n",
        "    def get_episode_stats(self):\n",
        "        \"\"\"Get episode statistics\"\"\"\n",
        "        stats = self.episode_stats.copy()\n",
        "        stats['efficiency'] = stats['boids_caught'] / self.initial_boid_count if self.initial_boid_count > 0 else 0\n",
        "        stats['steps_per_catch'] = self.step_count / stats['boids_caught'] if stats['boids_caught'] > 0 else float('inf')\n",
        "        return stats\n",
        "\n",
        "# Create and test the RL environment\n",
        "print(\"üèóÔ∏è Creating and testing RL environment...\")\n",
        "\n",
        "env = BoidsRLEnvironment(ENV_CONFIG)\n",
        "\n",
        "# Test environment reset\n",
        "print(\"\\nüß™ Testing environment reset...\")\n",
        "obs = env.reset()\n",
        "print(f\"‚úÖ Reset successful:\")\n",
        "print(f\"  Initial boids: {len(obs['boids'])}\")\n",
        "print(f\"  Canvas size: {obs['context']['canvasWidth']:.3f} x {obs['context']['canvasHeight']:.3f}\")\n",
        "print(f\"  Predator velocity: [{obs['predator']['velX']:.3f}, {obs['predator']['velY']:.3f}]\")\n",
        "\n",
        "# Test environment step with random actions\n",
        "print(\"\\nüß™ Testing environment steps...\")\n",
        "for i in range(5):\n",
        "    random_action = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "    next_obs, reward, done, info = env.step(random_action)\n",
        "\n",
        "    print(f\"  Step {i+1}: action=[{random_action[0]:.3f}, {random_action[1]:.3f}], reward={reward:.3f}, boids={info['boids_remaining']}, done={done}\")\n",
        "\n",
        "    if done:\n",
        "        print(f\"  Episode terminated at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úÖ Environment test completed!\")\n",
        "print(f\"  Final episode stats: {env.get_episode_stats()}\")\n",
        "print(f\"üéØ RL Environment is ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5OOLyOx2AmVp",
        "outputId": "329fd69f-3c68-42f1-9daf-efe16471901e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Creating actor and critic networks...\n",
            "‚úÖ Networks created:\n",
            "  Actor (SL model): 1,059,842 parameters\n",
            "  Critic: 134,465 parameters\n",
            "  Total: 1,194,307 parameters\n",
            "\\nüß™ Testing actor and critic...\n",
            "‚úÖ Actor test: output shape torch.Size([2]), range [-0.346, 0.500]\n",
            "‚úÖ Critic test: output shape torch.Size([1]), value -0.080\n",
            "\\nüéØ Actor and Critic networks ready for PPO training!\n"
          ]
        }
      ],
      "source": [
        "# Define Critic Network and PPO Components\n",
        "class TransformerCritic(nn.Module):\n",
        "    \"\"\"Transformer-based value function critic\"\"\"\n",
        "\n",
        "    def __init__(self, d_model=64, n_heads=4, n_layers=2, ffn_hidden=256, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "\n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "\n",
        "        # Input projections (same as actor but smaller dimension)\n",
        "        self.ctx_projection = nn.Linear(2, d_model)\n",
        "        self.predator_projection = nn.Linear(4, d_model)\n",
        "        self.boid_projection = nn.Linear(4, d_model)\n",
        "\n",
        "        # Transformer layers (lighter than actor)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, structured_inputs):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "\n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "\n",
        "        # Build token sequences (same as actor)\n",
        "        sequences = []\n",
        "        masks = []\n",
        "\n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "\n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "\n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']],\n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "\n",
        "            # Predator token\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "\n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]\n",
        "\n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "\n",
        "            # Pad to max_boids + 3\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)\n",
        "\n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "\n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Create padding mask\n",
        "        padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        # Extract CLS token and compute value\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        value = self.value_head(cls_output)  # [batch_size, 1]\n",
        "\n",
        "        return value.squeeze(-1) if batch_size == 1 else value.squeeze(-1)\n",
        "\n",
        "class PPOBuffer:\n",
        "    \"\"\"Buffer for storing PPO rollout data\"\"\"\n",
        "\n",
        "    def __init__(self, size, gamma=0.99, lam=0.95):\n",
        "        self.size = size\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "\n",
        "        # Storage\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "\n",
        "        # Computed values\n",
        "        self.advantages = []\n",
        "        self.returns = []\n",
        "\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "    def store(self, obs, action, reward, value, log_prob, done):\n",
        "        \"\"\"Store transition\"\"\"\n",
        "        if len(self.observations) < self.size:\n",
        "            self.observations.append(obs)\n",
        "            self.actions.append(action)\n",
        "            self.rewards.append(reward)\n",
        "            self.values.append(value)\n",
        "            self.log_probs.append(log_prob)\n",
        "            self.dones.append(done)\n",
        "        else:\n",
        "            self.observations[self.ptr] = obs\n",
        "            self.actions[self.ptr] = action\n",
        "            self.rewards[self.ptr] = reward\n",
        "            self.values[self.ptr] = value\n",
        "            self.log_probs[self.ptr] = log_prob\n",
        "            self.dones[self.ptr] = done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.size\n",
        "        if self.ptr == 0:\n",
        "            self.full = True\n",
        "\n",
        "    def finish_path(self, last_value=0.0):\n",
        "        \"\"\"Compute advantages and returns for the stored path\"\"\"\n",
        "        path_len = len(self.rewards)\n",
        "        if path_len == 0:\n",
        "            return\n",
        "\n",
        "        # Convert to numpy for easier computation\n",
        "        rewards = np.array(self.rewards[-path_len:])\n",
        "        values = np.array(self.values[-path_len:] + [last_value])\n",
        "        dones = np.array(self.dones[-path_len:])\n",
        "\n",
        "        # Compute GAE advantages\n",
        "        advantages = np.zeros_like(rewards)\n",
        "        lastgaelam = 0\n",
        "\n",
        "        for t in reversed(range(path_len)):\n",
        "            if t == path_len - 1:\n",
        "                nextnonterminal = 1.0 - dones[t]\n",
        "                nextvalues = last_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - dones[t]\n",
        "                nextvalues = values[t + 1]\n",
        "\n",
        "            delta = rewards[t] + self.gamma * nextvalues * nextnonterminal - values[t]\n",
        "            advantages[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n",
        "\n",
        "        # Compute returns\n",
        "        returns = advantages + values[:-1]\n",
        "\n",
        "        # Store computed values\n",
        "        if len(self.advantages) < len(advantages):\n",
        "            self.advantages.extend(advantages.tolist())\n",
        "            self.returns.extend(returns.tolist())\n",
        "        else:\n",
        "            start_idx = len(self.advantages) - len(advantages)\n",
        "            self.advantages[start_idx:] = advantages.tolist()\n",
        "            self.returns[start_idx:] = returns.tolist()\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"Get all stored data\"\"\"\n",
        "        assert len(self.advantages) == len(self.observations), \"Must call finish_path before get\"\n",
        "\n",
        "        data = {\n",
        "            'observations': self.observations.copy(),\n",
        "            'actions': np.array(self.actions),\n",
        "            'returns': np.array(self.returns),\n",
        "            'advantages': np.array(self.advantages),\n",
        "            'log_probs': np.array(self.log_probs),\n",
        "        }\n",
        "\n",
        "        # Normalize advantages\n",
        "        data['advantages'] = (data['advantages'] - data['advantages'].mean()) / (data['advantages'].std() + 1e-8)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear the buffer\"\"\"\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.advantages = []\n",
        "        self.returns = []\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "# Create actor and critic networks\n",
        "print(\"üß† Creating actor and critic networks...\")\n",
        "\n",
        "# Actor: Use the loaded SL model\n",
        "actor = sl_model\n",
        "actor.train()  # Set to training mode for RL fine-tuning\n",
        "\n",
        "# Critic: Create smaller transformer for value estimation\n",
        "critic_config = {\n",
        "    'd_model': 64,      # Smaller than actor\n",
        "    'n_heads': 4,       # Fewer heads\n",
        "    'n_layers': 2,      # Fewer layers\n",
        "    'ffn_hidden': 256,  # Smaller FFN\n",
        "    'max_boids': sl_architecture['max_boids'],\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "critic = TransformerCritic(**critic_config).to(DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "actor_params = sum(p.numel() for p in actor.parameters())\n",
        "critic_params = sum(p.numel() for p in critic.parameters())\n",
        "\n",
        "print(f\"‚úÖ Networks created:\")\n",
        "print(f\"  Actor (SL model): {actor_params:,} parameters\")\n",
        "print(f\"  Critic: {critic_params:,} parameters\")\n",
        "print(f\"  Total: {actor_params + critic_params:,} parameters\")\n",
        "\n",
        "# Test networks with sample input\n",
        "print(\"\\\\nüß™ Testing actor and critic...\")\n",
        "test_obs = env.reset()\n",
        "\n",
        "# Test actor\n",
        "actor.eval()\n",
        "with torch.no_grad():\n",
        "    actor_output = actor(test_obs)\n",
        "print(f\"‚úÖ Actor test: output shape {actor_output.shape}, range [{actor_output.min():.3f}, {actor_output.max():.3f}]\")\n",
        "\n",
        "# Test critic\n",
        "critic.eval()\n",
        "with torch.no_grad():\n",
        "    critic_output = critic(test_obs)\n",
        "print(f\"‚úÖ Critic test: output shape {critic_output.shape}, value {critic_output.item():.3f}\")\n",
        "\n",
        "print(f\"\\\\nüéØ Actor and Critic networks ready for PPO training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Yc15GQoAmVq",
        "outputId": "c038e5b1-27b7-4cc0-8ef6-2b5deac0115e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Creating PPO trainer...\n",
            "‚úÖ PPO Trainer initialized:\n",
            "  Learning rate: 3e-05\n",
            "  Rollout length: 2048\n",
            "  Clip epsilon: 0.1\n",
            "\n",
            "üß™ Testing a single training step...\n",
            "‚úÖ Training step completed in 148.76s\n",
            "  Policy loss: 0.0492\n",
            "  Value loss: 0.0044\n",
            "  KL divergence: 0.343534\n",
            "  Clip fraction: 0.827\n",
            "\n",
            "üöÄ PPO trainer is ready for full training!\n"
          ]
        }
      ],
      "source": [
        "# PPO Training Algorithm\n",
        "class PPOTrainer:\n",
        "    \"\"\"PPO trainer for the boids RL environment\"\"\"\n",
        "\n",
        "    def __init__(self, actor, critic, env, config):\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.config = config\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.AdamW(actor.parameters(), lr=config['learning_rate'])\n",
        "        self.critic_optimizer = optim.AdamW(critic.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "        # PPO buffer\n",
        "        self.buffer = PPOBuffer(\n",
        "            size=config['rollout_length'],\n",
        "            gamma=config['gamma'],\n",
        "            lam=config['gae_lambda']\n",
        "        )\n",
        "\n",
        "        # Training metrics\n",
        "        self.training_metrics = defaultdict(list)\n",
        "\n",
        "        print(f\"‚úÖ PPO Trainer initialized:\")\n",
        "        print(f\"  Learning rate: {config['learning_rate']}\")\n",
        "        print(f\"  Rollout length: {config['rollout_length']}\")\n",
        "        print(f\"  Clip epsilon: {config['clip_epsilon']}\")\n",
        "\n",
        "    def get_action_and_value(self, obs, deterministic=False):\n",
        "        \"\"\"Get action and value from networks\"\"\"\n",
        "        self.actor.eval()\n",
        "        self.critic.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get action from actor (deterministic output)\n",
        "            action_mean = self.actor(obs)\n",
        "\n",
        "            # Add exploration noise for stochastic policy\n",
        "            if not deterministic:\n",
        "                # Use fixed std for simplicity (could be learned)\n",
        "                action_std = 0.1\n",
        "                action = action_mean + torch.randn_like(action_mean) * action_std\n",
        "                # Clip to [-1, 1] range\n",
        "                action = torch.clamp(action, -1, 1)\n",
        "\n",
        "                # Compute log probability (assuming Gaussian)\n",
        "                log_prob = -0.5 * (((action - action_mean) / action_std) ** 2 + 2 * np.log(action_std) + np.log(2 * np.pi))\n",
        "                log_prob = log_prob.sum()  # Sum across action dimensions\n",
        "            else:\n",
        "                action = torch.clamp(action_mean, -1, 1)\n",
        "                log_prob = torch.tensor(0.0)\n",
        "\n",
        "            # Get value from critic\n",
        "            value = self.critic(obs)\n",
        "\n",
        "        # Convert everything to CPU and extract values\n",
        "        return action.cpu().numpy(), value.cpu().item(), log_prob.cpu().item()\n",
        "\n",
        "    def collect_rollout(self):\n",
        "        \"\"\"Collect a rollout of experiences\"\"\"\n",
        "        obs = self.env.reset()\n",
        "\n",
        "        for step in range(self.config['rollout_length']):\n",
        "            # Get action and value\n",
        "            action, value, log_prob = self.get_action_and_value(obs)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "            # Store in buffer (all values are now CPU scalars/arrays)\n",
        "            self.buffer.store(obs, action, reward, value, log_prob, done)\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "        # Always finish the path at the end of rollout\n",
        "        if len(self.buffer.observations) > 0:\n",
        "            # Get final value for advantage computation\n",
        "            _, final_value, _ = self.get_action_and_value(obs)\n",
        "\n",
        "            # If last episode didn't complete, use the final value; otherwise use 0\n",
        "            last_done = self.buffer.dones[-1] if self.buffer.dones else False\n",
        "            last_value = 0.0 if last_done else final_value\n",
        "\n",
        "            self.buffer.finish_path(last_value=last_value)\n",
        "\n",
        "    def update_networks(self):\n",
        "        \"\"\"Update actor and critic networks using PPO\"\"\"\n",
        "        # Get rollout data\n",
        "        data = self.buffer.get()\n",
        "\n",
        "        # Convert to tensors\n",
        "        observations = data['observations']\n",
        "        actions = torch.tensor(data['actions'], dtype=torch.float32, device=DEVICE)\n",
        "        returns = torch.tensor(data['returns'], dtype=torch.float32, device=DEVICE)\n",
        "        advantages = torch.tensor(data['advantages'], dtype=torch.float32, device=DEVICE)\n",
        "        old_log_probs = torch.tensor(data['log_probs'], dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "        # Training metrics for this update\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "        kl_divergences = []\n",
        "        clip_fractions = []\n",
        "\n",
        "        # PPO update epochs\n",
        "        for epoch in range(self.config['epochs_per_update']):\n",
        "            # Shuffle data\n",
        "            batch_size = self.config['batch_size']\n",
        "            indices = torch.randperm(len(observations))\n",
        "\n",
        "            # Mini-batch updates\n",
        "            for start in range(0, len(observations), batch_size):\n",
        "                end = start + batch_size\n",
        "                mb_indices = indices[start:end]\n",
        "\n",
        "                mb_obs = [observations[i] for i in mb_indices]\n",
        "                mb_actions = actions[mb_indices]\n",
        "                mb_returns = returns[mb_indices]\n",
        "                mb_advantages = advantages[mb_indices]\n",
        "                mb_old_log_probs = old_log_probs[mb_indices]\n",
        "\n",
        "                # Forward pass through networks\n",
        "                self.actor.train()\n",
        "                self.critic.train()\n",
        "\n",
        "                # Actor forward pass\n",
        "                action_means = self.actor(mb_obs)\n",
        "\n",
        "                # Compute log probabilities (assuming fixed std)\n",
        "                action_std = 0.1\n",
        "                log_probs = -0.5 * (((mb_actions - action_means) / action_std) ** 2 +\n",
        "                                   2 * np.log(action_std) + np.log(2 * np.pi))\n",
        "                log_probs = log_probs.sum(dim=-1)  # Sum across action dimensions\n",
        "\n",
        "                # Critic forward pass\n",
        "                values = self.critic(mb_obs)\n",
        "\n",
        "                # PPO policy loss\n",
        "                ratio = torch.exp(log_probs - mb_old_log_probs)\n",
        "                surr1 = ratio * mb_advantages\n",
        "                surr2 = torch.clamp(ratio, 1 - self.config['clip_epsilon'],\n",
        "                                  1 + self.config['clip_epsilon']) * mb_advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value_loss = F.mse_loss(values, mb_returns)\n",
        "\n",
        "                # Entropy bonus (simple approximation)\n",
        "                entropy = 0.5 * (1 + np.log(2 * np.pi * action_std**2))\n",
        "                entropy_loss = -self.config['entropy_coef'] * entropy\n",
        "\n",
        "                # Total actor loss\n",
        "                actor_loss = policy_loss + entropy_loss\n",
        "\n",
        "                # Update actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update critic\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss = self.config['value_loss_coef'] * value_loss\n",
        "                critic_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Collect metrics\n",
        "                with torch.no_grad():\n",
        "                    kl_div = (mb_old_log_probs - log_probs).mean()\n",
        "                    clip_frac = ((ratio - 1).abs() > self.config['clip_epsilon']).float().mean()\n",
        "\n",
        "                    policy_losses.append(policy_loss.item())\n",
        "                    value_losses.append(value_loss.item())\n",
        "                    kl_divergences.append(kl_div.item())\n",
        "                    clip_fractions.append(clip_frac.item())\n",
        "\n",
        "        # Store training metrics\n",
        "        self.training_metrics['policy_loss'].append(np.mean(policy_losses))\n",
        "        self.training_metrics['value_loss'].append(np.mean(value_losses))\n",
        "        self.training_metrics['kl_divergence'].append(np.mean(kl_divergences))\n",
        "        self.training_metrics['clip_fraction'].append(np.mean(clip_fractions))\n",
        "\n",
        "        # Clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Single training step: collect rollout + update networks\"\"\"\n",
        "        # Collect rollout\n",
        "        self.collect_rollout()\n",
        "\n",
        "        # Update networks\n",
        "        self.update_networks()\n",
        "\n",
        "        return self.training_metrics\n",
        "\n",
        "# Create PPO trainer\n",
        "print(\"üéØ Creating PPO trainer...\")\n",
        "trainer = PPOTrainer(actor, critic, env, PPO_CONFIG)\n",
        "\n",
        "# Test a single training step\n",
        "print(\"\\nüß™ Testing a single training step...\")\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    metrics = trainer.train_step()\n",
        "    step_time = time.time() - start_time\n",
        "\n",
        "    print(f\"‚úÖ Training step completed in {step_time:.2f}s\")\n",
        "    print(f\"  Policy loss: {metrics['policy_loss'][-1]:.4f}\")\n",
        "    print(f\"  Value loss: {metrics['value_loss'][-1]:.4f}\")\n",
        "    print(f\"  KL divergence: {metrics['kl_divergence'][-1]:.6f}\")\n",
        "    print(f\"  Clip fraction: {metrics['clip_fraction'][-1]:.3f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training step failed: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\nüöÄ PPO trainer is ready for full training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r5mvhNSAmVq",
        "outputId": "4ff9c347-b6f1-4cb8-e292-a9a2a3f48c28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÅ Starting RL training...\n",
            "Note: Adjust num_updates based on available time and compute\n",
            "\\nüß™ Running a small test training (5 updates)...\n",
            "üöÄ Starting PPO training for 100 updates...\n",
            "  Evaluation interval: 5\n",
            "  Save interval: 5\n",
            "\\n--- Update 5/100 Evaluation ---\n",
            "  Avg reward: 1.000\n",
            "  Avg catches: 1.0\n",
            "  Avg efficiency: 0.081\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 1.000\n",
            "    Success rate: 0.000\n",
            "  üéØ New best average reward: 1.000\n",
            "  Policy loss: 0.1175\n",
            "  Value loss: 0.0002\n",
            "  KL divergence: 0.158057\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_5.pt\n",
            "  Update time: 219.4s, Elapsed: 9.9min, ETA: 187.8min\n",
            "\\n--- Update 10/100 Evaluation ---\n",
            "  Avg reward: 2.800\n",
            "  Avg catches: 2.8\n",
            "  Avg efficiency: 0.080\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 1.900\n",
            "    Success rate: 0.000\n",
            "  üéØ New best average reward: 2.800\n",
            "  Policy loss: 0.0229\n",
            "  Value loss: 0.0042\n",
            "  KL divergence: 0.056886\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_10.pt\n",
            "  Update time: 354.4s, Elapsed: 21.3min, ETA: 191.8min\n",
            "\\n--- Update 15/100 Evaluation ---\n",
            "  Avg reward: 4.600\n",
            "  Avg catches: 4.6\n",
            "  Avg efficiency: 0.165\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.800\n",
            "    Success rate: 0.000\n",
            "  üéØ New best average reward: 4.600\n",
            "  Policy loss: 0.0615\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.055622\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_15.pt\n",
            "  Update time: 390.9s, Elapsed: 34.8min, ETA: 197.2min\n",
            "\\n--- Update 20/100 Evaluation ---\n",
            "  Avg reward: 2.200\n",
            "  Avg catches: 2.2\n",
            "  Avg efficiency: 0.129\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.650\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0133\n",
            "  Value loss: 0.0305\n",
            "  KL divergence: 0.018751\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_20.pt\n",
            "  Update time: 338.3s, Elapsed: 47.4min, ETA: 189.8min\n",
            "\\n--- Update 25/100 Evaluation ---\n",
            "  Avg reward: 0.600\n",
            "  Avg catches: 0.6\n",
            "  Avg efficiency: 0.038\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.240\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0377\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.029489\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_25.pt\n",
            "  Update time: 247.1s, Elapsed: 59.4min, ETA: 178.2min\n",
            "\\n--- Update 30/100 Evaluation ---\n",
            "  Avg reward: 0.800\n",
            "  Avg catches: 0.8\n",
            "  Avg efficiency: 0.049\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.000\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0412\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.044951\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_30.pt\n",
            "  Update time: 206.0s, Elapsed: 70.9min, ETA: 165.4min\n",
            "\\n--- Update 35/100 Evaluation ---\n",
            "  Avg reward: 5.200\n",
            "  Avg catches: 5.2\n",
            "  Avg efficiency: 0.252\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.457\n",
            "    Success rate: 0.000\n",
            "  üéØ New best average reward: 5.200\n",
            "  Policy loss: 0.0242\n",
            "  Value loss: 0.0000\n",
            "  KL divergence: 0.018259\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_35.pt\n",
            "  Update time: 375.2s, Elapsed: 87.4min, ETA: 162.3min\n",
            "\\n--- Update 40/100 Evaluation ---\n",
            "  Avg reward: 1.200\n",
            "  Avg catches: 1.2\n",
            "  Avg efficiency: 0.042\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.300\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0055\n",
            "  Value loss: 0.0041\n",
            "  KL divergence: 0.014586\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_40.pt\n",
            "  Update time: 369.0s, Elapsed: 101.0min, ETA: 151.5min\n",
            "\\n--- Update 45/100 Evaluation ---\n",
            "  Avg reward: 0.600\n",
            "  Avg catches: 0.6\n",
            "  Avg efficiency: 0.017\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.111\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0114\n",
            "  Value loss: 0.0046\n",
            "  KL divergence: 0.027244\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_45.pt\n",
            "  Update time: 298.2s, Elapsed: 115.2min, ETA: 140.8min\n",
            "\\n--- Update 50/100 Evaluation ---\n",
            "  Avg reward: 2.000\n",
            "  Avg catches: 2.0\n",
            "  Avg efficiency: 0.119\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.100\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0151\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.010784\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_50.pt\n",
            "  Update time: 394.3s, Elapsed: 133.4min, ETA: 133.4min\n",
            "\\n--- Update 55/100 Evaluation ---\n",
            "  Avg reward: 3.200\n",
            "  Avg catches: 3.2\n",
            "  Avg efficiency: 0.084\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.320\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0070\n",
            "  Value loss: 0.0042\n",
            "  KL divergence: 0.011140\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_55.pt\n",
            "  Update time: 337.8s, Elapsed: 147.4min, ETA: 120.6min\n",
            "\\n--- Update 60/100 Evaluation ---\n",
            "  Avg reward: 2.000\n",
            "  Avg catches: 2.0\n",
            "  Avg efficiency: 0.067\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.240\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0166\n",
            "  Value loss: 0.0172\n",
            "  KL divergence: 0.020402\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_60.pt\n",
            "  Update time: 292.7s, Elapsed: 161.9min, ETA: 107.9min\n",
            "\\n--- Update 65/100 Evaluation ---\n",
            "  Avg reward: 2.000\n",
            "  Avg catches: 2.0\n",
            "  Avg efficiency: 0.041\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 1.980\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0117\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.008229\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_65.pt\n",
            "  Update time: 413.0s, Elapsed: 174.6min, ETA: 94.0min\n",
            "\\n--- Update 70/100 Evaluation ---\n",
            "  Avg reward: 1.000\n",
            "  Avg catches: 1.0\n",
            "  Avg efficiency: 0.037\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 1.860\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0282\n",
            "  Value loss: 0.0001\n",
            "  KL divergence: 0.024410\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_70.pt\n",
            "  Update time: 246.4s, Elapsed: 184.5min, ETA: 79.1min\n",
            "\\n--- Update 75/100 Evaluation ---\n",
            "  Avg reward: 2.600\n",
            "  Avg catches: 2.6\n",
            "  Avg efficiency: 0.088\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.060\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0215\n",
            "  Value loss: 0.0086\n",
            "  KL divergence: 0.038636\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_75.pt\n",
            "  Update time: 307.6s, Elapsed: 198.6min, ETA: 66.2min\n",
            "\\n--- Update 80/100 Evaluation ---\n",
            "  Avg reward: 3.200\n",
            "  Avg catches: 3.2\n",
            "  Avg efficiency: 0.110\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.300\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0067\n",
            "  Value loss: 0.0165\n",
            "  KL divergence: 0.010856\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_80.pt\n",
            "  Update time: 424.9s, Elapsed: 218.3min, ETA: 54.6min\n",
            "\\n--- Update 85/100 Evaluation ---\n",
            "  Avg reward: 3.600\n",
            "  Avg catches: 3.6\n",
            "  Avg efficiency: 0.091\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.140\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0107\n",
            "  Value loss: 0.0081\n",
            "  KL divergence: 0.019256\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_85.pt\n",
            "  Update time: 399.5s, Elapsed: 235.4min, ETA: 41.5min\n",
            "\\n--- Update 90/100 Evaluation ---\n",
            "  Avg reward: 1.400\n",
            "  Avg catches: 1.4\n",
            "  Avg efficiency: 0.070\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.160\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0175\n",
            "  Value loss: 0.0130\n",
            "  KL divergence: 0.037740\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_90.pt\n",
            "  Update time: 284.7s, Elapsed: 248.9min, ETA: 27.7min\n",
            "\\n--- Update 95/100 Evaluation ---\n",
            "  Avg reward: 2.000\n",
            "  Avg catches: 2.0\n",
            "  Avg efficiency: 0.069\n",
            "  Recent performance (50 episodes):\n",
            "    Avg reward: 2.300\n",
            "    Success rate: 0.000\n",
            "  Policy loss: 0.0080\n",
            "  Value loss: 0.0168\n",
            "  KL divergence: 0.014607\n",
            "‚úì Saved checkpoint: checkpoints_rl/rl_model_update_95.pt\n",
            "  Update time: 382.3s, Elapsed: 263.0min, ETA: 13.8min\n"
          ]
        }
      ],
      "source": [
        "# Main Training Loop with Metrics and Monitoring\n",
        "class TrainingMonitor:\n",
        "    \"\"\"Monitor training progress with comprehensive metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.performance_metrics = defaultdict(list)\n",
        "        self.learning_metrics = defaultdict(list)\n",
        "        self.behavioral_metrics = defaultdict(list)\n",
        "\n",
        "        self.episode_count = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def update_episode_metrics(self, episode_stats, episode_reward):\n",
        "        \"\"\"Update metrics after episode completion\"\"\"\n",
        "        self.episode_count += 1\n",
        "\n",
        "        # Performance metrics\n",
        "        self.performance_metrics['boids_caught_per_episode'].append(episode_stats['boids_caught'])\n",
        "        self.performance_metrics['catch_efficiency'].append(episode_stats['efficiency'])\n",
        "        self.performance_metrics['episode_reward'].append(episode_reward)\n",
        "        self.performance_metrics['episode_length'].append(episode_stats.get('step_count', 0))\n",
        "\n",
        "        # Success rate (rolling average)\n",
        "        success = 1.0 if episode_stats['efficiency'] >= 0.8 else 0.0\n",
        "        self.performance_metrics['success_rate'].append(success)\n",
        "\n",
        "        # Catch timing metrics\n",
        "        if episode_stats['boids_caught'] > 0:\n",
        "            self.performance_metrics['catches_per_episode'].append(episode_stats['boids_caught'])\n",
        "            self.performance_metrics['steps_per_catch'].append(episode_stats['steps_per_catch'])\n",
        "\n",
        "    def update_training_metrics(self, training_metrics):\n",
        "        \"\"\"Update learning metrics from PPO training\"\"\"\n",
        "        for key, values in training_metrics.items():\n",
        "            if values:  # Only add if there are values\n",
        "                self.learning_metrics[key].append(values[-1])\n",
        "\n",
        "    def get_recent_performance(self, window=100):\n",
        "        \"\"\"Get recent performance statistics\"\"\"\n",
        "        if len(self.performance_metrics['episode_reward']) < window:\n",
        "            window = len(self.performance_metrics['episode_reward'])\n",
        "\n",
        "        if window == 0:\n",
        "            return {}\n",
        "\n",
        "        recent_rewards = self.performance_metrics['episode_reward'][-window:]\n",
        "        recent_catches = self.performance_metrics['boids_caught_per_episode'][-window:]\n",
        "        recent_efficiency = self.performance_metrics['catch_efficiency'][-window:]\n",
        "        recent_success = self.performance_metrics['success_rate'][-window:]\n",
        "\n",
        "        return {\n",
        "            'avg_reward': np.mean(recent_rewards),\n",
        "            'avg_catches': np.mean(recent_catches),\n",
        "            'avg_efficiency': np.mean(recent_efficiency),\n",
        "            'success_rate': np.mean(recent_success),\n",
        "            'episodes': window\n",
        "        }\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        \"\"\"Plot training progress\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "        # Performance metrics\n",
        "        if self.performance_metrics['episode_reward']:\n",
        "            axes[0, 0].plot(self.performance_metrics['episode_reward'])\n",
        "            axes[0, 0].set_title('Episode Reward')\n",
        "            axes[0, 0].set_xlabel('Episode')\n",
        "            axes[0, 0].grid(True)\n",
        "\n",
        "        if self.performance_metrics['boids_caught_per_episode']:\n",
        "            axes[0, 1].plot(self.performance_metrics['boids_caught_per_episode'])\n",
        "            axes[0, 1].set_title('Boids Caught per Episode')\n",
        "            axes[0, 1].set_xlabel('Episode')\n",
        "            axes[0, 1].grid(True)\n",
        "\n",
        "        if self.performance_metrics['catch_efficiency']:\n",
        "            axes[0, 2].plot(self.performance_metrics['catch_efficiency'])\n",
        "            axes[0, 2].set_title('Catch Efficiency')\n",
        "            axes[0, 2].set_xlabel('Episode')\n",
        "            axes[0, 2].set_ylim([0, 1])\n",
        "            axes[0, 2].grid(True)\n",
        "\n",
        "        # Learning metrics\n",
        "        if self.learning_metrics['policy_loss']:\n",
        "            axes[1, 0].plot(self.learning_metrics['policy_loss'])\n",
        "            axes[1, 0].set_title('Policy Loss')\n",
        "            axes[1, 0].set_xlabel('Update')\n",
        "            axes[1, 0].grid(True)\n",
        "\n",
        "        if self.learning_metrics['value_loss']:\n",
        "            axes[1, 1].plot(self.learning_metrics['value_loss'])\n",
        "            axes[1, 1].set_title('Value Loss')\n",
        "            axes[1, 1].set_xlabel('Update')\n",
        "            axes[1, 1].grid(True)\n",
        "\n",
        "        if self.learning_metrics['kl_divergence']:\n",
        "            axes[1, 2].plot(self.learning_metrics['kl_divergence'])\n",
        "            axes[1, 2].set_title('KL Divergence')\n",
        "            axes[1, 2].set_xlabel('Update')\n",
        "            axes[1, 2].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def save_checkpoint(actor, critic, trainer, monitor, update, is_best=False):\n",
        "    \"\"\"Save training checkpoint\"\"\"\n",
        "    os.makedirs(\"checkpoints_rl\", exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'update': update,\n",
        "        'actor_state_dict': actor.state_dict(),\n",
        "        'critic_state_dict': critic.state_dict(),\n",
        "        'actor_optimizer_state_dict': trainer.actor_optimizer.state_dict(),\n",
        "        'critic_optimizer_state_dict': trainer.critic_optimizer.state_dict(),\n",
        "        'training_metrics': dict(trainer.training_metrics),\n",
        "        'performance_metrics': dict(monitor.performance_metrics),\n",
        "        'architecture': sl_architecture,\n",
        "        'config': PPO_CONFIG,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Save regular checkpoint\n",
        "    checkpoint_path = f\"checkpoints_rl/rl_model_update_{update}.pt\"\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"‚úì Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    if is_best:\n",
        "        best_path = \"checkpoints_rl/best_rl_model.pt\"\n",
        "        torch.save(checkpoint, best_path)\n",
        "        print(f\"‚úì Saved best model: {best_path}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def train_ppo(trainer, monitor, num_updates=100, eval_interval=10, save_interval=25):\n",
        "    \"\"\"Main PPO training loop\"\"\"\n",
        "    print(f\"üöÄ Starting PPO training for {num_updates} updates...\")\n",
        "    print(f\"  Evaluation interval: {eval_interval}\")\n",
        "    print(f\"  Save interval: {save_interval}\")\n",
        "\n",
        "    best_avg_reward = -np.inf\n",
        "    start_time = time.time()\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "        update_start_time = time.time()\n",
        "\n",
        "        # Collect rollout and train\n",
        "        metrics = trainer.train_step()\n",
        "\n",
        "        # Update learning metrics\n",
        "        monitor.update_training_metrics(metrics)\n",
        "\n",
        "        # Evaluation episodes\n",
        "        if update % eval_interval == 0:\n",
        "            print(f\"\\\\n--- Update {update}/{num_updates} Evaluation ---\")\n",
        "\n",
        "            eval_rewards = []\n",
        "            eval_stats = []\n",
        "\n",
        "            # Run evaluation episodes\n",
        "            for eval_ep in range(5):  # 5 evaluation episodes\n",
        "                obs = env.reset()\n",
        "                episode_reward = 0\n",
        "                done = False\n",
        "\n",
        "                while not done:\n",
        "                    action, _, _ = trainer.get_action_and_value(obs, deterministic=True)\n",
        "                    obs, reward, done, info = env.step(action)\n",
        "                    episode_reward += reward\n",
        "\n",
        "                eval_rewards.append(episode_reward)\n",
        "                eval_stats.append(env.get_episode_stats())\n",
        "\n",
        "                # Update episode metrics\n",
        "                monitor.update_episode_metrics(env.get_episode_stats(), episode_reward)\n",
        "\n",
        "            # Evaluation summary\n",
        "            avg_reward = np.mean(eval_rewards)\n",
        "            avg_catches = np.mean([stats['boids_caught'] for stats in eval_stats])\n",
        "            avg_efficiency = np.mean([stats['efficiency'] for stats in eval_stats])\n",
        "\n",
        "            print(f\"  Avg reward: {avg_reward:.3f}\")\n",
        "            print(f\"  Avg catches: {avg_catches:.1f}\")\n",
        "            print(f\"  Avg efficiency: {avg_efficiency:.3f}\")\n",
        "\n",
        "            # Recent performance\n",
        "            recent_perf = monitor.get_recent_performance(window=50)\n",
        "            if recent_perf:\n",
        "                print(f\"  Recent performance (50 episodes):\")\n",
        "                print(f\"    Avg reward: {recent_perf['avg_reward']:.3f}\")\n",
        "                print(f\"    Success rate: {recent_perf['success_rate']:.3f}\")\n",
        "\n",
        "            # Check if best model\n",
        "            is_best = avg_reward > best_avg_reward\n",
        "            if is_best:\n",
        "                best_avg_reward = avg_reward\n",
        "                print(f\"  üéØ New best average reward: {best_avg_reward:.3f}\")\n",
        "\n",
        "            # Learning metrics\n",
        "            if metrics['policy_loss']:\n",
        "                print(f\"  Policy loss: {metrics['policy_loss'][-1]:.4f}\")\n",
        "                print(f\"  Value loss: {metrics['value_loss'][-1]:.4f}\")\n",
        "                print(f\"  KL divergence: {metrics['kl_divergence'][-1]:.6f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if update % save_interval == 0 or update == num_updates:\n",
        "            save_checkpoint(actor, critic, trainer, monitor, update,\n",
        "                          is_best=(update % eval_interval == 0 and avg_reward > best_avg_reward))\n",
        "\n",
        "        # Progress update\n",
        "        update_time = time.time() - update_start_time\n",
        "        elapsed_time = time.time() - start_time\n",
        "        remaining_time = (elapsed_time / update) * (num_updates - update)\n",
        "\n",
        "        if update % eval_interval == 0:\n",
        "            print(f\"  Update time: {update_time:.1f}s, Elapsed: {elapsed_time/60:.1f}min, ETA: {remaining_time/60:.1f}min\")\n",
        "\n",
        "    print(f\"\\\\nüéâ Training completed!\")\n",
        "    print(f\"  Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
        "    print(f\"  Best average reward: {best_avg_reward:.3f}\")\n",
        "\n",
        "    return monitor\n",
        "\n",
        "# Initialize training monitor\n",
        "monitor = TrainingMonitor()\n",
        "\n",
        "# Run training (adjust num_updates as needed)\n",
        "print(\"üèÅ Starting RL training...\")\n",
        "print(\"Note: Adjust num_updates based on available time and compute\")\n",
        "\n",
        "# Small test run first\n",
        "print(\"\\\\nüß™ Running a small test training (5 updates)...\")\n",
        "try:\n",
        "    test_monitor = train_ppo(trainer, monitor, num_updates=100, eval_interval=5, save_interval=5)\n",
        "    print(\"‚úÖ Test training completed successfully!\")\n",
        "\n",
        "    # Plot initial results\n",
        "    print(\"\\\\nüìä Plotting test training progress...\")\n",
        "    test_monitor.plot_training_progress()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test training failed: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\\\nüéØ RL Training system is ready!\")\n",
        "print(f\"To run full training, call:\")\n",
        "print(f\"  final_monitor = train_ppo(trainer, monitor, num_updates=100, eval_interval=10, save_interval=25)\")\n",
        "print(f\"\\\\nThe trained model can then be exported back to JavaScript using export_to_js.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}