{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Transformer RL Training - Fixed Implementation\n",
        "# Download Codebase from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Repository information\n",
        "REPO_URL = \"https://github.com/yimianxyz/homepage.git\"\n",
        "BRANCH = \"neuro-predator\"  # Using the main branch with transformer implementation\n",
        "REPO_DIR = \"homepage\"\n",
        "\n",
        "def download_codebase():\n",
        "    \"\"\"Download the codebase from GitHub if not already present\"\"\"\n",
        "    \n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(f\"Repository directory '{REPO_DIR}' already exists.\")\n",
        "        \n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "            # Check current branch\n",
        "            result = subprocess.run(['git', 'branch', '--show-current'],\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            current_branch = result.stdout.strip()\n",
        "            \n",
        "            if current_branch != BRANCH:\n",
        "                print(f\"Switching to branch '{BRANCH}'...\")\n",
        "                subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "            \n",
        "            # Pull latest changes\n",
        "            print(\"Updating repository...\")\n",
        "            subprocess.run(['git', 'pull', 'origin', BRANCH], check=True)\n",
        "            \n",
        "            print(f\"‚úÖ Repository updated successfully!\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error updating repository: {e}\")\n",
        "            print(\"Repository directory exists but may not be a valid git repository.\")\n",
        "    \n",
        "    else:\n",
        "        print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "        \n",
        "        try:\n",
        "            # Clone the specific branch\n",
        "            subprocess.run(['git', 'clone', '-b', BRANCH, REPO_URL, REPO_DIR], check=True)\n",
        "            print(f\"‚úÖ Repository cloned successfully!\")\n",
        "            \n",
        "            # Change to repository directory\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error cloning repository: {e}\")\n",
        "            print(\"Make sure you have git installed and internet connection.\")\n",
        "            return False\n",
        "    \n",
        "    # Verify key files exist\n",
        "    key_files = [\n",
        "        'config/constants.py',\n",
        "        'simulation/processors/input_processor.py',\n",
        "        'simulation/state_manager/state_manager.py',\n",
        "        'simulation/random_state_generator/random_state_generator.py',\n",
        "        'policy/human_prior/closest_pursuit_policy.py',\n",
        "        'export_to_js.py'\n",
        "    ]\n",
        "    \n",
        "    missing_files = []\n",
        "    for file_path in key_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Some key files are missing:\")\n",
        "        for file_path in missing_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ All key files found!\")\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Download the codebase\n",
        "success = download_codebase()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Setup complete! Ready for enhanced RL training.\")\n",
        "else:\n",
        "    print(\"‚ùå Setup failed. Please check the errors above and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and Imports - Single Source of Truth\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "import math\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# Ensure we're in the correct directory and add to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'homepage':\n",
        "    print(f\"‚ö†Ô∏è  Warning: Current directory is '{project_root.name}', expected 'homepage'\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "try:\n",
        "    from config.constants import CONSTANTS\n",
        "    from simulation.processors import InputProcessor, ActionProcessor\n",
        "    from simulation.state_manager import StateManager\n",
        "    from simulation.random_state_generator import RandomStateGenerator\n",
        "    from policy.human_prior.closest_pursuit_policy import create_closest_pursuit_policy\n",
        "    \n",
        "    print(f\"‚úÖ Successfully imported all simulation modules\")\n",
        "    print(f\"üìÅ Project root: {project_root}\")\n",
        "    print(f\"üîß Key constants: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}, BOID_MAX_SPEED={CONSTANTS.BOID_MAX_SPEED}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import modules: {e}\")\n",
        "    print(\"Make sure the repository was downloaded correctly in the first cell.\")\n",
        "    raise\n",
        "\n",
        "# ===== SINGLE SOURCE OF TRUTH CONFIGURATION =====\n",
        "class RLConfig:\n",
        "    \"\"\"Centralized configuration for RL training - single source of truth\"\"\"\n",
        "    \n",
        "    # Device Configuration\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Model Architecture (loaded from checkpoint)\n",
        "    MODEL_ARCHITECTURE = {\n",
        "        'd_model': 128,\n",
        "        'n_heads': 8,\n",
        "        'n_layers': 4,\n",
        "        'ffn_hidden': 512,\n",
        "        'max_boids': 50\n",
        "    }\n",
        "    \n",
        "    # PPO Hyperparameters\n",
        "    LEARNING_RATE = 3e-4        # Increased from 5e-5 for better fine-tuning\n",
        "    CLIP_EPSILON = 0.15         # Slightly conservative for fine-tuning\n",
        "    ENTROPY_COEF = 0.01         # Reduced for pre-trained deterministic policy\n",
        "    VALUE_COEF = 0.5           # Standard value loss coefficient\n",
        "    MAX_GRAD_NORM = 0.5        # Gradient clipping\n",
        "    PPO_EPOCHS = 4             # Number of PPO optimization epochs\n",
        "    MINI_BATCH_SIZE = 64       # FIXED: Consistent mini-batch size\n",
        "    ROLLOUT_STEPS = 2048       # Steps per rollout\n",
        "    GAMMA = 0.99               # Discount factor\n",
        "    GAE_LAMBDA = 0.95          # GAE lambda parameter\n",
        "    \n",
        "    # Actor Network Parameters\n",
        "    EXPLORATION_STD_INIT = 0.3  # Initial exploration standard deviation\n",
        "    EXPLORATION_STD_MIN = 0.05  # Minimum exploration standard deviation\n",
        "    EXPLORATION_STD_MAX = 0.8   # Maximum exploration standard deviation\n",
        "    \n",
        "    # Environment Settings\n",
        "    MIN_BOIDS = 1\n",
        "    MAX_BOIDS = 50\n",
        "    MIN_CANVAS_WIDTH = 400\n",
        "    MAX_CANVAS_WIDTH = 1600\n",
        "    MIN_CANVAS_HEIGHT = 400\n",
        "    MAX_CANVAS_HEIGHT = 1200\n",
        "    TIMEOUT_MULTIPLIER = 3.0    # Adaptive timeout calculation\n",
        "    \n",
        "    # Reward Settings (keeping multi-step attribution as requested)\n",
        "    CATCH_REWARD = 1.0\n",
        "    REWARD_DECAY_RATE = 0.05    # For exponential decay over reward window\n",
        "    REWARD_WINDOW = 50          # Steps before catch to attribute reward\n",
        "    \n",
        "    # Training Settings\n",
        "    NUM_EPISODES = 5000\n",
        "    LOG_INTERVAL = 50           # Log every N rollouts\n",
        "    SAVE_INTERVAL = 200         # Save every N rollouts\n",
        "    EVAL_INTERVAL = 100         # Evaluate every N rollouts\n",
        "    EVAL_EPISODES = 30          # Episodes per evaluation\n",
        "    \n",
        "    # Checkpoints\n",
        "    SL_CHECKPOINT_PATH = \"checkpoints/best_model.pt\"\n",
        "    RL_CHECKPOINT_DIR = \"rl_checkpoints\"\n",
        "    \n",
        "    @classmethod\n",
        "    def get_dict(cls):\n",
        "        \"\"\"Get configuration as dictionary for compatibility\"\"\"\n",
        "        return {attr: getattr(cls, attr) for attr in dir(cls) \n",
        "                if not attr.startswith('_') and not callable(getattr(cls, attr))}\n",
        "    \n",
        "    @classmethod\n",
        "    def print_config(cls):\n",
        "        \"\"\"Print current configuration\"\"\"\n",
        "        print(\"üöÄ RL Training Configuration:\")\n",
        "        print(f\"  Device: {cls.DEVICE}\")\n",
        "        print(f\"  Learning Rate: {cls.LEARNING_RATE}\")\n",
        "        print(f\"  Mini-batch Size: {cls.MINI_BATCH_SIZE}\")\n",
        "        print(f\"  Rollout Steps: {cls.ROLLOUT_STEPS}\")\n",
        "        print(f\"  Environment: {cls.MIN_BOIDS}-{cls.MAX_BOIDS} boids\")\n",
        "        print(f\"  Canvas: {cls.MIN_CANVAS_WIDTH}x{cls.MIN_CANVAS_HEIGHT} to {cls.MAX_CANVAS_WIDTH}x{cls.MAX_CANVAS_HEIGHT}\")\n",
        "        print(f\"  Reward: {cls.CATCH_REWARD} per catch, {cls.REWARD_WINDOW}-step attribution window\")\n",
        "        print(f\"  Exploration: std [{cls.EXPLORATION_STD_MIN}, {cls.EXPLORATION_STD_MAX}], init {cls.EXPLORATION_STD_INIT}\")\n",
        "\n",
        "# Initialize configuration\n",
        "config = RLConfig()\n",
        "config.print_config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Model Architectures - Fixed Implementation\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    \"\"\"Gated Linear Unit with GELU activation\"\"\"\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return x * torch.nn.functional.gelu(gate)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    \"\"\"Enhanced transformer layer with proper initialization\"\"\"\n",
        "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # GEGLU FFN with separate projections for export compatibility\n",
        "        self.ffn_gate_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_up_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_down_proj = nn.Linear(ffn_hidden, d_model)\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.self_attn(normed, normed, normed, key_padding_mask=padding_mask)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # FFN with residual\n",
        "        normed = self.norm2(x)\n",
        "        gate = torch.nn.functional.gelu(self.ffn_gate_proj(normed))\n",
        "        up = self.ffn_up_proj(normed)\n",
        "        ffn_out = self.ffn_down_proj(gate * up)\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    \"\"\"Enhanced transformer predictor with better initialization\"\"\"\n",
        "    def __init__(self, d_model=128, n_heads=8, n_layers=4, ffn_hidden=512, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "\n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "\n",
        "        # Input projections\n",
        "        self.ctx_projection = nn.Linear(2, d_model)\n",
        "        self.predator_projection = nn.Linear(4, d_model)\n",
        "        self.boid_projection = nn.Linear(4, d_model)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, structured_inputs, padding_mask=None):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "\n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "\n",
        "        # Build token sequences for each sample in batch\n",
        "        sequences = []\n",
        "        masks = []\n",
        "\n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "\n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "\n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']],\n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "\n",
        "            # Predator token - expand to 4D\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "\n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]  # CLS, CTX, Predator are not padding\n",
        "\n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "\n",
        "            # Pad to max_boids + 3 (CLS + CTX + Predator)\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)  # Mark as padding\n",
        "\n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "\n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Create padding mask\n",
        "        if padding_mask is None:\n",
        "            padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        # Extract CLS token and project to output\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        action = self.output_projection(cls_output)  # [batch_size, 2]\n",
        "\n",
        "        # Apply tanh to ensure [-1, 1] range\n",
        "        action = torch.tanh(action)\n",
        "\n",
        "        return action.squeeze(0) if batch_size == 1 else action\n",
        "\n",
        "class EnhancedActorNetwork(nn.Module):\n",
        "    \"\"\"FIXED: Enhanced actor network with proper stochastic policy\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint, architecture):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create transformer with same architecture as SL model\n",
        "        self.transformer = TransformerPredictor(\n",
        "            d_model=architecture['d_model'],\n",
        "            n_heads=architecture['n_heads'],\n",
        "            n_layers=architecture['n_layers'],\n",
        "            ffn_hidden=architecture['ffn_hidden'],\n",
        "            max_boids=architecture['max_boids'],\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # FIXED: Learnable exploration parameters\n",
        "        self.log_std = nn.Parameter(\n",
        "            torch.ones(2) * math.log(RLConfig.EXPLORATION_STD_INIT)\n",
        "        )\n",
        "\n",
        "        # Load pretrained weights\n",
        "        if checkpoint is not None:\n",
        "            self.transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(\"‚úÖ Loaded pretrained SL weights into actor\")\n",
        "\n",
        "        # Store architecture info\n",
        "        self.architecture = architecture\n",
        "\n",
        "    def forward(self, structured_inputs):\n",
        "        \"\"\"Forward pass through transformer\"\"\"\n",
        "        return self.transformer(structured_inputs)\n",
        "\n",
        "    def get_action_and_log_prob(self, structured_inputs):\n",
        "        \"\"\"FIXED: Get stochastic action and log probability for proper RL exploration\"\"\"\n",
        "        # Get mean action from transformer\n",
        "        action_mean = self.forward(structured_inputs)\n",
        "\n",
        "        # FIXED: Proper learnable exploration with clamping\n",
        "        log_std = torch.clamp(\n",
        "            self.log_std, \n",
        "            min=math.log(RLConfig.EXPLORATION_STD_MIN),\n",
        "            max=math.log(RLConfig.EXPLORATION_STD_MAX)\n",
        "        )\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        # Create distribution and sample\n",
        "        dist = torch.distributions.Normal(action_mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "        # Apply tanh to ensure bounded output and adjust log_prob\n",
        "        action_tanh = torch.tanh(action)\n",
        "        \n",
        "        # Adjust log probability for tanh transformation\n",
        "        log_prob = log_prob - torch.log(1 - action_tanh.pow(2) + 1e-7).sum(dim=-1)\n",
        "\n",
        "        return action_tanh, log_prob\n",
        "\n",
        "class EnhancedCriticNetwork(nn.Module):\n",
        "    \"\"\"FIXED: Enhanced critic network with better feature processing\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=22, hidden_dims=[256, 256, 128]):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(hidden_dim),  # Added layer norm for stability\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))  # Single value output\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, state_features):\n",
        "        \"\"\"Estimate value from state features\"\"\"\n",
        "        return self.network(state_features).squeeze(-1)\n",
        "\n",
        "def extract_state_features(structured_inputs):\n",
        "    \"\"\"FIXED: Enhanced feature extraction with better error handling\"\"\"\n",
        "    batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "\n",
        "    if isinstance(structured_inputs, dict):\n",
        "        structured_inputs = [structured_inputs]\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for sample in structured_inputs:\n",
        "        # Context features (2D)\n",
        "        context_feat = [sample['context']['canvasWidth'], sample['context']['canvasHeight']]\n",
        "\n",
        "        # Predator features (2D)\n",
        "        predator_feat = [sample['predator']['velX'], sample['predator']['velY']]\n",
        "\n",
        "        # FIXED: Robust boid features with error handling\n",
        "        boids = sample['boids']\n",
        "        if len(boids) > 0:\n",
        "            # Statistical aggregation of boids with safe operations\n",
        "            rel_x = [b['relX'] for b in boids]\n",
        "            rel_y = [b['relY'] for b in boids]\n",
        "            vel_x = [b['velX'] for b in boids]\n",
        "            vel_y = [b['velY'] for b in boids]\n",
        "\n",
        "            # Safe statistics calculation\n",
        "            boid_feat = [\n",
        "                len(boids),  # Number of boids\n",
        "                np.mean(rel_x), max(np.std(rel_x), 1e-8), np.min(rel_x), np.max(rel_x),\n",
        "                np.mean(rel_y), max(np.std(rel_y), 1e-8), np.min(rel_y), np.max(rel_y),\n",
        "                np.mean(vel_x), max(np.std(vel_x), 1e-8), np.min(vel_x), np.max(vel_x),\n",
        "                np.mean(vel_y), max(np.std(vel_y), 1e-8), np.min(vel_y), np.max(vel_y),\n",
        "                # Distance to closest boid\n",
        "                np.min([math.sqrt(b['relX']**2 + b['relY']**2) for b in boids])\n",
        "            ]\n",
        "        else:\n",
        "            # No boids remaining - use zeros\n",
        "            boid_feat = [0] + [0.0] * 16  # 17 features total\n",
        "\n",
        "        # Combine all features (2 + 2 + 17 = 21 features) + 1 extra = 22\n",
        "        sample_feat = context_feat + predator_feat + boid_feat + [len(boids) / RLConfig.MAX_BOIDS]  # Normalized boid count\n",
        "        features.append(sample_feat)\n",
        "\n",
        "    return torch.tensor(features, dtype=torch.float32, device=RLConfig.DEVICE)\n",
        "\n",
        "print(\"‚úÖ Enhanced model architectures defined with fixes:\")\n",
        "print(\"  - Fixed stochastic actor policy with learnable exploration\")\n",
        "print(\"  - Enhanced critic network with layer normalization\")  \n",
        "print(\"  - Robust feature extraction with error handling\")\n",
        "print(\"  - Proper log probability computation with tanh correction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Supervised Learning Model and Initialize Networks\n",
        "\n",
        "def load_sl_checkpoint(checkpoint_path: str = None):\n",
        "    \"\"\"Load supervised learning checkpoint with better error handling\"\"\"\n",
        "    \n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = RLConfig.SL_CHECKPOINT_PATH\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
        "        print(\"Available checkpoints:\")\n",
        "        if os.path.exists(\"checkpoints/\"):\n",
        "            checkpoints = list(Path(\"checkpoints/\").glob(\"*.pt\"))\n",
        "            for cp in checkpoints:\n",
        "                print(f\"  - {cp}\")\n",
        "        else:\n",
        "            print(\"  No checkpoints directory found\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loading SL checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=RLConfig.DEVICE)\n",
        "\n",
        "    # Extract and validate architecture parameters\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        print(f\"‚úÖ Found architecture in checkpoint:\")\n",
        "        print(f\"  d_model: {arch['d_model']}\")\n",
        "        print(f\"  n_heads: {arch['n_heads']}\")\n",
        "        print(f\"  n_layers: {arch['n_layers']}\")\n",
        "        print(f\"  ffn_hidden: {arch['ffn_hidden']}\")\n",
        "        print(f\"  max_boids: {arch['max_boids']}\")\n",
        "        \n",
        "        # Update config with loaded architecture\n",
        "        RLConfig.MODEL_ARCHITECTURE.update(arch)\n",
        "    else:\n",
        "        print(\"‚ùå No architecture found in checkpoint - using default values\")\n",
        "        arch = RLConfig.MODEL_ARCHITECTURE\n",
        "\n",
        "    # Additional info\n",
        "    epoch = checkpoint.get('epoch', 'unknown')\n",
        "    val_loss = checkpoint.get('best_val_loss', 'unknown')\n",
        "    print(f\"  Checkpoint epoch: {epoch}\")\n",
        "    print(f\"  Best validation loss: {val_loss}\")\n",
        "\n",
        "    return checkpoint, arch\n",
        "\n",
        "def initialize_networks(checkpoint, architecture):\n",
        "    \"\"\"Initialize actor and critic networks with proper error handling\"\"\"\n",
        "    \n",
        "    print(\"üß† Initializing networks...\")\n",
        "    \n",
        "    # Create actor and critic networks\n",
        "    actor = EnhancedActorNetwork(checkpoint, architecture).to(RLConfig.DEVICE)\n",
        "    critic = EnhancedCriticNetwork(input_dim=22).to(RLConfig.DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    actor_params = sum(p.numel() for p in actor.parameters())\n",
        "    critic_params = sum(p.numel() for p in critic.parameters())\n",
        "    actor_trainable = sum(p.numel() for p in actor.parameters() if p.requires_grad)\n",
        "    critic_trainable = sum(p.numel() for p in critic.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"üìä Network Statistics:\")\n",
        "    print(f\"  Actor parameters: {actor_params:,} ({actor_trainable:,} trainable)\")\n",
        "    print(f\"  Critic parameters: {critic_params:,} ({critic_trainable:,} trainable)\")\n",
        "    print(f\"  Total parameters: {actor_params + critic_params:,}\")\n",
        "    \n",
        "    # Exploration info\n",
        "    with torch.no_grad():\n",
        "        current_std = torch.exp(actor.log_std).mean().item()\n",
        "    print(f\"  Initial exploration std: {current_std:.3f}\")\n",
        "    \n",
        "    return actor, critic\n",
        "\n",
        "def test_networks(actor, critic):\n",
        "    \"\"\"Test network forward passes with comprehensive validation\"\"\"\n",
        "    \n",
        "    print(\"üß™ Testing network forward passes...\")\n",
        "    \n",
        "    # Create test input with validation\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # Test with different batch sizes\n",
        "    test_cases = [\n",
        "        (\"Single sample\", test_input),\n",
        "        (\"Empty boids\", {\n",
        "            'context': {'canvasWidth': 0.5, 'canvasHeight': 0.4},\n",
        "            'predator': {'velX': 0.0, 'velY': 0.0},\n",
        "            'boids': []\n",
        "        }),\n",
        "        (\"Many boids\", {\n",
        "            'context': {'canvasWidth': 1.0, 'canvasHeight': 1.0},\n",
        "            'predator': {'velX': 0.2, 'velY': -0.1},\n",
        "            'boids': [\n",
        "                {'relX': 0.1, 'relY': 0.1, 'velX': 0.1, 'velY': 0.1},\n",
        "                {'relX': -0.1, 'relY': -0.1, 'velX': -0.1, 'velY': -0.1},\n",
        "                {'relX': 0.2, 'relY': -0.2, 'velX': 0.2, 'velY': -0.2}\n",
        "            ]\n",
        "        })\n",
        "    ]\n",
        "    \n",
        "    for test_name, test_case in test_cases:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # Test actor\n",
        "                action_mean = actor.forward(test_case)\n",
        "                action, log_prob = actor.get_action_and_log_prob(test_case)\n",
        "                \n",
        "                # Test critic\n",
        "                features = extract_state_features(test_case)\n",
        "                value = critic(features)\n",
        "                \n",
        "                print(f\"  ‚úÖ {test_name}:\")\n",
        "                print(f\"    Input boids: {len(test_case['boids'])}\")\n",
        "                print(f\"    Action mean: [{action_mean[0].item():.3f}, {action_mean[1].item():.3f}]\")\n",
        "                print(f\"    Sampled action: [{action[0].item():.3f}, {action[1].item():.3f}]\")\n",
        "                print(f\"    Log prob: {log_prob.item():.3f}\")\n",
        "                print(f\"    Features shape: {features.shape}\")\n",
        "                print(f\"    Value: {value.item():.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå {test_name} failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    print(\"‚úÖ All network tests passed!\")\n",
        "    return True\n",
        "\n",
        "# Load checkpoint and initialize networks\n",
        "print(\"üöÄ Loading SL checkpoint and initializing networks...\")\n",
        "\n",
        "try:\n",
        "    checkpoint, architecture = load_sl_checkpoint()\n",
        "    \n",
        "    if checkpoint is not None:\n",
        "        actor, critic = initialize_networks(checkpoint, architecture)\n",
        "        test_success = test_networks(actor, critic)\n",
        "        \n",
        "        if test_success:\n",
        "            print(\"\\n‚úÖ Networks successfully initialized and tested!\")\n",
        "            print(\"üì± Ready for RL environment setup and training.\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Network tests failed\")\n",
        "    else:\n",
        "        print(\"‚ùå Cannot proceed without SL checkpoint\")\n",
        "        actor = critic = None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during network initialization: {e}\")\n",
        "    print(\"Please ensure the supervised learning checkpoint exists and is valid.\")\n",
        "    actor = critic = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED: Enhanced RL Environment Wrapper\n",
        "\n",
        "class CustomRLPolicy:\n",
        "    \"\"\"Custom policy wrapper for RL agent integration with StateManager\"\"\"\n",
        "    \n",
        "    def __init__(self, action):\n",
        "        self.action = action if isinstance(action, list) else [action[0].item(), action[1].item()]\n",
        "    \n",
        "    def get_action(self, structured_inputs):\n",
        "        \"\"\"Return the precomputed action\"\"\"\n",
        "        return self.action\n",
        "\n",
        "class EnhancedPredatorEnvironment:\n",
        "    \"\"\"FIXED: Enhanced RL environment wrapper with proper StateManager integration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize simulation components using proper infrastructure\n",
        "        self.state_manager = StateManager()\n",
        "        self.random_generator = RandomStateGenerator()\n",
        "        self.input_processor = InputProcessor()\n",
        "        self.action_processor = ActionProcessor()\n",
        "\n",
        "        # Environment state tracking\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 0\n",
        "        self.initial_boids_count = 0\n",
        "        self.episode_catches = []  # Steps when catches occurred\n",
        "        self.step_rewards = []     # Rewards for each step\n",
        "        \n",
        "        # Episode tracking\n",
        "        self.episode_count = 0\n",
        "        self.reset_episode_stats()\n",
        "\n",
        "    def reset_episode_stats(self):\n",
        "        \"\"\"Reset episode-level statistics\"\"\"\n",
        "        self.episode_catches = []\n",
        "        self.step_rewards = []\n",
        "        self.current_step = 0\n",
        "\n",
        "    def calculate_adaptive_timeout(self, canvas_width, canvas_height, num_boids):\n",
        "        \"\"\"Calculate adaptive timeout based on environment complexity\"\"\"\n",
        "        canvas_area = canvas_width * canvas_height\n",
        "        base_timeout = (canvas_area * num_boids * RLConfig.TIMEOUT_MULTIPLIER) / 10000\n",
        "        return max(int(base_timeout), 300)  # Minimum 300 steps for better learning\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment for new episode with enhanced randomization\"\"\"\n",
        "        self.reset_episode_stats()\n",
        "        self.episode_count += 1\n",
        "\n",
        "        # Generate random environment parameters\n",
        "        num_boids = random.randint(RLConfig.MIN_BOIDS, RLConfig.MAX_BOIDS)\n",
        "        canvas_width = random.randint(RLConfig.MIN_CANVAS_WIDTH, RLConfig.MAX_CANVAS_WIDTH)\n",
        "        canvas_height = random.randint(RLConfig.MIN_CANVAS_HEIGHT, RLConfig.MAX_CANVAS_HEIGHT)\n",
        "\n",
        "        # Generate random initial state with better distribution\n",
        "        initial_state = self.random_generator.generate_scattered_state(\n",
        "            num_boids, canvas_width, canvas_height\n",
        "        )\n",
        "\n",
        "        # FIXED: Use StateManager properly with dummy policy for initialization\n",
        "        dummy_policy = create_closest_pursuit_policy()\n",
        "        self.state_manager.init(initial_state, dummy_policy)\n",
        "\n",
        "        # Set episode parameters\n",
        "        self.initial_boids_count = num_boids\n",
        "        self.max_steps = self.calculate_adaptive_timeout(canvas_width, canvas_height, num_boids)\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Get initial observation using proper conversion\n",
        "        current_state = self.state_manager.get_state()\n",
        "        observation = self._state_to_structured_inputs(current_state)\n",
        "\n",
        "        info = {\n",
        "            'episode': self.episode_count,\n",
        "            'canvas_width': canvas_width,\n",
        "            'canvas_height': canvas_height,\n",
        "            'initial_boids': num_boids,\n",
        "            'max_steps': self.max_steps,\n",
        "            'episode_reset': True\n",
        "        }\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _state_to_structured_inputs(self, state):\n",
        "        \"\"\"FIXED: Convert state to structured inputs with validation\"\"\"\n",
        "        try:\n",
        "            # Extract data from state with validation\n",
        "            boids = state.get('boids_states', [])\n",
        "            predator_state = state.get('predator_state', {})\n",
        "            predator_pos = predator_state.get('position', {'x': 0, 'y': 0})\n",
        "            predator_vel = predator_state.get('velocity', {'x': 0, 'y': 0})\n",
        "            canvas_width = state.get('canvas_width', 800)\n",
        "            canvas_height = state.get('canvas_height', 600)\n",
        "\n",
        "            # Use input processor for proper conversion\n",
        "            structured_inputs = self.input_processor.process_inputs(\n",
        "                boids, predator_pos, predator_vel, canvas_width, canvas_height\n",
        "            )\n",
        "\n",
        "            return structured_inputs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error converting state: {e}\")\n",
        "            # Return safe default\n",
        "            return {\n",
        "                'context': {'canvasWidth': 0.5, 'canvasHeight': 0.5},\n",
        "                'predator': {'velX': 0.0, 'velY': 0.0},\n",
        "                'boids': []\n",
        "            }\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"FIXED: Take environment step using StateManager properly\"\"\"\n",
        "        # Convert action to proper format\n",
        "        if torch.is_tensor(action):\n",
        "            action_list = [action[0].item(), action[1].item()]\n",
        "        else:\n",
        "            action_list = list(action)\n",
        "\n",
        "        # Store current state for comparison\n",
        "        prev_state = self.state_manager.get_state()\n",
        "        boids_before = len(prev_state.get('boids_states', []))\n",
        "\n",
        "        # FIXED: Use StateManager with custom policy instead of direct simulation access\n",
        "        try:\n",
        "            custom_policy = CustomRLPolicy(action_list)\n",
        "            \n",
        "            # Let StateManager handle the step properly\n",
        "            # Replace the policy temporarily for this step\n",
        "            old_policy = self.state_manager.policy\n",
        "            self.state_manager.policy = custom_policy\n",
        "            \n",
        "            # Execute step using StateManager's proper infrastructure\n",
        "            new_state = self.state_manager.step()\n",
        "            \n",
        "            # Restore original policy\n",
        "            self.state_manager.policy = old_policy\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in step execution: {e}\")\n",
        "            # Fallback: use previous state\n",
        "            new_state = prev_state\n",
        "\n",
        "        # Calculate catches by comparing boid counts\n",
        "        boids_after = len(new_state.get('boids_states', []))\n",
        "        catches_this_step = max(0, boids_before - boids_after)\n",
        "\n",
        "        # Track catches for reward calculation\n",
        "        if catches_this_step > 0:\n",
        "            for _ in range(catches_this_step):\n",
        "                self.episode_catches.append(self.current_step)\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate reward using the multi-step attribution system (as requested)\n",
        "        reward = self._calculate_reward()\n",
        "        self.step_rewards.append(reward)\n",
        "\n",
        "        # Check termination conditions\n",
        "        done = False\n",
        "        termination_reason = 'ongoing'\n",
        "\n",
        "        if boids_after == 0:\n",
        "            done = True\n",
        "            termination_reason = 'all_caught'\n",
        "        elif self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            termination_reason = 'timeout'\n",
        "\n",
        "        # Get next observation\n",
        "        observation = self._state_to_structured_inputs(new_state)\n",
        "\n",
        "        # Comprehensive info dictionary\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'catches_this_step': catches_this_step,\n",
        "            'total_catches': len(self.episode_catches),\n",
        "            'boids_remaining': boids_after,\n",
        "            'boids_before': boids_before,\n",
        "            'done': done,\n",
        "            'termination_reason': termination_reason,\n",
        "            'reward': reward,\n",
        "            'episode_reward': sum(self.step_rewards),\n",
        "            'max_steps': self.max_steps,\n",
        "            'episode': self.episode_count\n",
        "        }\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Multi-step reward attribution as requested (keeping the original design)\"\"\"\n",
        "        total_reward = 0.0\n",
        "\n",
        "        # Only calculate reward if we have catches\n",
        "        if len(self.episode_catches) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # For each catch, attribute reward to the last N steps (as per original design)\n",
        "        for catch_step in self.episode_catches:\n",
        "            # Define the reward window (steps before the catch)\n",
        "            reward_start = max(0, catch_step - RLConfig.REWARD_WINDOW + 1)\n",
        "            reward_end = catch_step + 1\n",
        "\n",
        "            # Only attribute reward to current step if it's within the window\n",
        "            if reward_start <= self.current_step - 1 < reward_end:\n",
        "                # Calculate steps before catch (for decay)\n",
        "                steps_before_catch = catch_step - (self.current_step - 1)\n",
        "\n",
        "                # Exponential decay: more recent actions get higher reward\n",
        "                decay_factor = math.exp(-RLConfig.REWARD_DECAY_RATE * steps_before_catch)\n",
        "                step_reward = RLConfig.CATCH_REWARD * decay_factor\n",
        "\n",
        "                total_reward += step_reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "def test_environment():\n",
        "    \"\"\"Test the enhanced environment with comprehensive validation\"\"\"\n",
        "    print(\"üåç Testing enhanced RL environment...\")\n",
        "    \n",
        "    if actor is None:\n",
        "        print(\"‚ùå Cannot test environment - actor not available\")\n",
        "        return False\n",
        "\n",
        "    # Create test environment\n",
        "    test_env = EnhancedPredatorEnvironment()\n",
        "\n",
        "    try:\n",
        "        # Test reset\n",
        "        obs, info = test_env.reset()\n",
        "        print(f\"  ‚úÖ Environment reset successful:\")\n",
        "        print(f\"    Canvas: {info['canvas_width']}x{info['canvas_height']}\")\n",
        "        print(f\"    Boids: {info['initial_boids']}\")\n",
        "        print(f\"    Max steps: {info['max_steps']}\")\n",
        "        print(f\"    Observation boids: {len(obs['boids'])}\")\n",
        "\n",
        "        # Test multiple steps\n",
        "        print(f\"  üîÑ Testing environment steps:\")\n",
        "        for i in range(5):\n",
        "            with torch.no_grad():\n",
        "                action, log_prob = actor.get_action_and_log_prob(obs)\n",
        "\n",
        "            obs, reward, done, step_info = test_env.step(action)\n",
        "\n",
        "            print(f\"    Step {i+1}: action=[{action[0]:.3f}, {action[1]:.3f}], \"\n",
        "                  f\"reward={reward:.3f}, boids={step_info['boids_remaining']}, \"\n",
        "                  f\"catches={step_info['catches_this_step']}\")\n",
        "\n",
        "            if done:\n",
        "                print(f\"    Episode done: {step_info['termination_reason']}\")\n",
        "                break\n",
        "\n",
        "        print(\"  ‚úÖ Environment test completed successfully!\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Environment test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Test the enhanced environment\n",
        "if actor is not None:\n",
        "    env_test_success = test_environment()\n",
        "    if env_test_success:\n",
        "        print(\"\\n‚úÖ Enhanced environment ready for training!\")\n",
        "        print(\"üîß Key fixes implemented:\")\n",
        "        print(\"  - Proper StateManager integration instead of direct simulation access\")\n",
        "        print(\"  - Robust error handling and fallback mechanisms\")  \n",
        "        print(\"  - Enhanced state conversion with validation\")\n",
        "        print(\"  - Comprehensive step information tracking\")\n",
        "        print(\"  - Multi-step reward attribution system maintained\")\n",
        "    else:\n",
        "        print(\"‚ùå Environment test failed - check implementation\")\n",
        "else:\n",
        "    print(\"‚ùå Skipping environment test - actor not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED: Enhanced PPO Implementation with Proper GAE\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    \"\"\"FIXED: Memory-efficient experience buffer with proper trajectory handling\"\"\"\n",
        "    \n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the buffer\"\"\"\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.episode_starts = []  # FIXED: Track episode boundaries\n",
        "        self.size = 0\n",
        "    \n",
        "    def add(self, obs, action, reward, value, log_prob, done, episode_start=False):\n",
        "        \"\"\"Add experience to buffer\"\"\"\n",
        "        self.observations.append(obs)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.values.append(value)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.episode_starts.append(episode_start)\n",
        "        self.size += 1\n",
        "    \n",
        "    def get_batch(self):\n",
        "        \"\"\"Get all experiences as tensors\"\"\"\n",
        "        return {\n",
        "            'observations': self.observations,\n",
        "            'actions': torch.stack(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards, dtype=torch.float32, device=RLConfig.DEVICE),\n",
        "            'values': torch.stack(self.values),\n",
        "            'log_probs': torch.stack(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones, dtype=torch.float32, device=RLConfig.DEVICE),\n",
        "            'episode_starts': torch.tensor(self.episode_starts, dtype=torch.bool, device=RLConfig.DEVICE)\n",
        "        }\n",
        "    \n",
        "    def is_full(self):\n",
        "        \"\"\"Check if buffer is full\"\"\"\n",
        "        return self.size >= self.buffer_size\n",
        "\n",
        "class EnhancedPPOTrainer:\n",
        "    \"\"\"FIXED: Enhanced PPO trainer with proper GAE and trajectory handling\"\"\"\n",
        "\n",
        "    def __init__(self, actor, critic):\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "\n",
        "        # Optimizers with different learning rates for fine-tuning\n",
        "        self.actor_optimizer = optim.AdamW(\n",
        "            actor.parameters(), \n",
        "            lr=RLConfig.LEARNING_RATE,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "        self.critic_optimizer = optim.AdamW(\n",
        "            critic.parameters(), \n",
        "            lr=RLConfig.LEARNING_RATE * 2.0,  # Slightly higher for critic\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "\n",
        "        # Learning rate schedulers\n",
        "        self.actor_scheduler = optim.lr_scheduler.LinearLR(\n",
        "            self.actor_optimizer, start_factor=1.0, end_factor=0.1, total_iters=1000\n",
        "        )\n",
        "        self.critic_scheduler = optim.lr_scheduler.LinearLR(\n",
        "            self.critic_optimizer, start_factor=1.0, end_factor=0.1, total_iters=1000\n",
        "        )\n",
        "\n",
        "        # Experience buffer\n",
        "        self.buffer = ExperienceBuffer(RLConfig.ROLLOUT_STEPS)\n",
        "\n",
        "        # Training metrics\n",
        "        self.training_stats = defaultdict(list)\n",
        "        self.episode_stats = []\n",
        "\n",
        "    def collect_rollout(self, env):\n",
        "        \"\"\"FIXED: Collect rollout with proper episode boundary tracking\"\"\"\n",
        "        self.buffer.reset()\n",
        "        \n",
        "        # Initialize environment\n",
        "        obs, info = env.reset()\n",
        "        episode_stats = []\n",
        "        current_episode = {\n",
        "            'reward': 0,\n",
        "            'length': 0,\n",
        "            'catches': 0,\n",
        "            'termination': None\n",
        "        }\n",
        "\n",
        "        episode_start = True\n",
        "\n",
        "        while not self.buffer.is_full():\n",
        "            # Get action and value\n",
        "            with torch.no_grad():\n",
        "                action, log_prob = self.actor.get_action_and_log_prob(obs)\n",
        "                state_features = extract_state_features(obs)\n",
        "                value = self.critic(state_features)\n",
        "\n",
        "            # Take step in environment\n",
        "            next_obs, reward, done, step_info = env.step(action)\n",
        "\n",
        "            # Add to buffer\n",
        "            self.buffer.add(obs, action, reward, value, log_prob, done, episode_start)\n",
        "            episode_start = False\n",
        "\n",
        "            # Update episode stats\n",
        "            current_episode['reward'] += reward\n",
        "            current_episode['length'] += 1\n",
        "            current_episode['catches'] += step_info['catches_this_step']\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            if done:\n",
        "                # Episode finished\n",
        "                current_episode['termination'] = step_info['termination_reason']\n",
        "                episode_stats.append(current_episode.copy())\n",
        "                \n",
        "                # Reset for next episode\n",
        "                obs, info = env.reset()\n",
        "                episode_start = True\n",
        "                current_episode = {\n",
        "                    'reward': 0,\n",
        "                    'length': 0,\n",
        "                    'catches': 0,\n",
        "                    'termination': None\n",
        "                }\n",
        "\n",
        "        # FIXED: Calculate returns and advantages with proper GAE\n",
        "        self._calculate_gae(next_obs)\n",
        "\n",
        "        return episode_stats\n",
        "\n",
        "    def _calculate_gae(self, final_obs):\n",
        "        \"\"\"FIXED: Proper Generalized Advantage Estimation implementation\"\"\"\n",
        "        batch = self.buffer.get_batch()\n",
        "        \n",
        "        # Get final value estimate\n",
        "        with torch.no_grad():\n",
        "            final_state_features = extract_state_features(final_obs)\n",
        "            final_value = self.critic(final_state_features).item()\n",
        "\n",
        "        # Convert to numpy for easier computation\n",
        "        rewards = batch['rewards'].cpu().numpy()\n",
        "        values = batch['values'].squeeze().cpu().numpy()\n",
        "        dones = batch['dones'].cpu().numpy()\n",
        "        episode_starts = batch['episode_starts'].cpu().numpy()\n",
        "\n",
        "        # FIXED: Proper GAE computation with episode boundary handling\n",
        "        advantages = np.zeros_like(rewards)\n",
        "        returns = np.zeros_like(rewards)\n",
        "        \n",
        "        # Calculate GAE\n",
        "        gae = 0\n",
        "        for step in reversed(range(len(rewards))):\n",
        "            # Handle episode boundaries\n",
        "            if step == len(rewards) - 1:\n",
        "                next_value = final_value if not dones[step] else 0\n",
        "            else:\n",
        "                next_value = values[step + 1] if not dones[step] else 0\n",
        "            \n",
        "            # Reset GAE at episode boundaries\n",
        "            if episode_starts[step] and step > 0:\n",
        "                gae = 0\n",
        "            \n",
        "            # TD error\n",
        "            delta = rewards[step] + RLConfig.GAMMA * next_value - values[step]\n",
        "            \n",
        "            # GAE computation\n",
        "            gae = delta + RLConfig.GAMMA * RLConfig.GAE_LAMBDA * gae * (1 - dones[step])\n",
        "            advantages[step] = gae\n",
        "\n",
        "        # Calculate returns\n",
        "        returns = advantages + values\n",
        "\n",
        "        # Convert back to tensors\n",
        "        self.buffer.returns = torch.tensor(returns, dtype=torch.float32, device=RLConfig.DEVICE)\n",
        "        self.buffer.advantages = torch.tensor(advantages, dtype=torch.float32, device=RLConfig.DEVICE)\n",
        "\n",
        "        # Normalize advantages\n",
        "        if len(advantages) > 1:\n",
        "            self.buffer.advantages = (self.buffer.advantages - self.buffer.advantages.mean()) / (\n",
        "                self.buffer.advantages.std() + 1e-8\n",
        "            )\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\"FIXED: Update policy with proper mini-batch handling\"\"\"\n",
        "        batch = self.buffer.get_batch()\n",
        "        batch_size = self.buffer.size\n",
        "\n",
        "        # Training metrics for this update\n",
        "        update_stats = {\n",
        "            'actor_losses': [],\n",
        "            'critic_losses': [],\n",
        "            'entropies': [],\n",
        "            'clip_fractions': [],\n",
        "            'kl_divs': []\n",
        "        }\n",
        "\n",
        "        # FIXED: Multiple PPO epochs with proper batching\n",
        "        for epoch in range(RLConfig.PPO_EPOCHS):\n",
        "            # Shuffle indices\n",
        "            indices = torch.randperm(batch_size)\n",
        "            \n",
        "            # Mini-batch training\n",
        "            for i in range(0, batch_size, RLConfig.MINI_BATCH_SIZE):\n",
        "                batch_indices = indices[i:i+RLConfig.MINI_BATCH_SIZE]\n",
        "                \n",
        "                if len(batch_indices) < RLConfig.MINI_BATCH_SIZE // 2:\n",
        "                    continue  # Skip small batches\n",
        "                \n",
        "                # Get mini-batch data\n",
        "                mb_obs = [batch['observations'][idx] for idx in batch_indices]\n",
        "                mb_actions = batch['actions'][batch_indices]\n",
        "                mb_old_log_probs = batch['log_probs'][batch_indices]\n",
        "                mb_returns = self.buffer.returns[batch_indices]\n",
        "                mb_advantages = self.buffer.advantages[batch_indices]\n",
        "                mb_values = batch['values'][batch_indices]\n",
        "\n",
        "                # Forward passes\n",
        "                _, new_log_probs = self.actor.get_action_and_log_prob(mb_obs)\n",
        "                mb_state_features = extract_state_features(mb_obs)\n",
        "                new_values = self.critic(mb_state_features)\n",
        "\n",
        "                # FIXED: Proper PPO loss computation\n",
        "                # Actor loss\n",
        "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
        "                surr1 = ratio * mb_advantages\n",
        "                surr2 = torch.clamp(\n",
        "                    ratio, \n",
        "                    1.0 - RLConfig.CLIP_EPSILON, \n",
        "                    1.0 + RLConfig.CLIP_EPSILON\n",
        "                ) * mb_advantages\n",
        "                \n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Critic loss (clipped value function)\n",
        "                value_pred_clipped = mb_values.squeeze() + torch.clamp(\n",
        "                    new_values - mb_values.squeeze(),\n",
        "                    -RLConfig.CLIP_EPSILON,\n",
        "                    RLConfig.CLIP_EPSILON\n",
        "                )\n",
        "                value_loss1 = F.mse_loss(new_values, mb_returns)\n",
        "                value_loss2 = F.mse_loss(value_pred_clipped, mb_returns)\n",
        "                critic_loss = torch.max(value_loss1, value_loss2)\n",
        "\n",
        "                # Entropy loss (for exploration)\n",
        "                entropy = -(new_log_probs).mean()  # Approximation for entropy\n",
        "                entropy_loss = -RLConfig.ENTROPY_COEF * entropy\n",
        "\n",
        "                # Combined losses\n",
        "                total_actor_loss = actor_loss + entropy_loss\n",
        "\n",
        "                # Update actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                total_actor_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), RLConfig.MAX_GRAD_NORM)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Update critic\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), RLConfig.MAX_GRAD_NORM)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Metrics\n",
        "                with torch.no_grad():\n",
        "                    clip_frac = ((ratio - 1.0).abs() > RLConfig.CLIP_EPSILON).float().mean()\n",
        "                    kl_div = (mb_old_log_probs - new_log_probs).mean()\n",
        "\n",
        "                update_stats['actor_losses'].append(actor_loss.item())\n",
        "                update_stats['critic_losses'].append(critic_loss.item())\n",
        "                update_stats['entropies'].append(entropy.item())\n",
        "                update_stats['clip_fractions'].append(clip_frac.item())\n",
        "                update_stats['kl_divs'].append(kl_div.item())\n",
        "\n",
        "        # Update learning rate schedulers\n",
        "        self.actor_scheduler.step()\n",
        "        self.critic_scheduler.step()\n",
        "\n",
        "        # Average metrics\n",
        "        return {\n",
        "            'actor_loss': np.mean(update_stats['actor_losses']),\n",
        "            'critic_loss': np.mean(update_stats['critic_losses']),\n",
        "            'entropy': np.mean(update_stats['entropies']),\n",
        "            'clip_fraction': np.mean(update_stats['clip_fractions']),\n",
        "            'kl_div': np.mean(update_stats['kl_divs']),\n",
        "            'actor_lr': self.actor_scheduler.get_last_lr()[0],\n",
        "            'critic_lr': self.critic_scheduler.get_last_lr()[0]\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, filepath, episode, stats):\n",
        "        \"\"\"Save comprehensive training checkpoint\"\"\"\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        \n",
        "        checkpoint = {\n",
        "            'episode': episode,\n",
        "            'actor_state_dict': self.actor.state_dict(),\n",
        "            'critic_state_dict': self.critic.state_dict(),\n",
        "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
        "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
        "            'actor_scheduler_state_dict': self.actor_scheduler.state_dict(),\n",
        "            'critic_scheduler_state_dict': self.critic_scheduler.state_dict(),\n",
        "            'training_stats': dict(self.training_stats),\n",
        "            'episode_stats': self.episode_stats,\n",
        "            'config': RLConfig.get_dict(),\n",
        "            'architecture': self.actor.architecture,\n",
        "            'recent_stats': stats,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, filepath)\n",
        "        print(f\"‚úÖ Saved comprehensive RL checkpoint: {filepath}\")\n",
        "\n",
        "# Initialize the enhanced PPO trainer\n",
        "if actor is not None and critic is not None:\n",
        "    ppo_trainer = EnhancedPPOTrainer(actor, critic)\n",
        "    \n",
        "    print(\"üéØ Enhanced PPO Trainer initialized!\")\n",
        "    print(\"üîß Key improvements:\")\n",
        "    print(f\"  - Proper GAE implementation with Œª={RLConfig.GAE_LAMBDA}\")\n",
        "    print(f\"  - Episode boundary handling for trajectory segmentation\")\n",
        "    print(f\"  - Memory-efficient experience buffer\")\n",
        "    print(f\"  - Clipped value function loss\")\n",
        "    print(f\"  - Learning rate scheduling\")\n",
        "    print(f\"  - Comprehensive metrics tracking\")\n",
        "    print(f\"  - Actor LR: {RLConfig.LEARNING_RATE}, Critic LR: {RLConfig.LEARNING_RATE * 2.0}\")\n",
        "    print(\"\\n‚úÖ Ready for enhanced RL training!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot initialize PPO trainer - networks not available\")\n",
        "    ppo_trainer = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXED: Enhanced Training Loop and Metrics\n",
        "\n",
        "class ComprehensiveMetrics:\n",
        "    \"\"\"Enhanced metrics tracking with detailed analysis\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset all metrics\"\"\"\n",
        "        self.rollout_stats = []\n",
        "        self.training_stats = []\n",
        "        self.episode_data = []\n",
        "        \n",
        "        # Aggregate statistics\n",
        "        self.total_episodes = 0\n",
        "        self.total_steps = 0\n",
        "        self.total_catches = 0\n",
        "        self.start_time = time.time()\n",
        "    \n",
        "    def add_rollout(self, episode_stats, training_stats, rollout_num):\n",
        "        \"\"\"Add rollout statistics\"\"\"\n",
        "        rollout_data = {\n",
        "            'rollout': rollout_num,\n",
        "            'episodes': len(episode_stats),\n",
        "            'episode_stats': episode_stats,\n",
        "            'training_stats': training_stats,\n",
        "            'timestamp': time.time()\n",
        "        }\n",
        "        \n",
        "        self.rollout_stats.append(rollout_data)\n",
        "        self.episode_data.extend(episode_stats)\n",
        "        self.training_stats.append(training_stats)\n",
        "        \n",
        "        # Update aggregates\n",
        "        for ep in episode_stats:\n",
        "            self.total_episodes += 1\n",
        "            self.total_steps += ep['length']\n",
        "            self.total_catches += ep['catches']\n",
        "    \n",
        "    def get_recent_stats(self, window=100):\n",
        "        \"\"\"Get statistics for recent episodes\"\"\"\n",
        "        if len(self.episode_data) == 0:\n",
        "            return {}\n",
        "        \n",
        "        recent_episodes = self.episode_data[-window:]\n",
        "        \n",
        "        rewards = [ep['reward'] for ep in recent_episodes]\n",
        "        lengths = [ep['length'] for ep in recent_episodes]\n",
        "        catches = [ep['catches'] for ep in recent_episodes]\n",
        "        successes = [1 if ep['catches'] > 0 else 0 for ep in recent_episodes]\n",
        "        timeouts = [1 if ep['termination'] == 'timeout' else 0 for ep in recent_episodes]\n",
        "        \n",
        "        stats = {\n",
        "            'episodes': len(recent_episodes),\n",
        "            'avg_reward': np.mean(rewards) if rewards else 0,\n",
        "            'std_reward': np.std(rewards) if len(rewards) > 1 else 0,\n",
        "            'avg_length': np.mean(lengths) if lengths else 0,\n",
        "            'avg_catches': np.mean(catches) if catches else 0,\n",
        "            'total_catches': sum(catches),\n",
        "            'success_rate': np.mean(successes) if successes else 0,\n",
        "            'timeout_rate': np.mean(timeouts) if timeouts else 0,\n",
        "            'catch_efficiency': sum(catches) / sum(lengths) if sum(lengths) > 0 else 0\n",
        "        }\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    def print_progress(self, rollout_num, training_stats):\n",
        "        \"\"\"Print comprehensive progress information\"\"\"\n",
        "        recent_stats = self.get_recent_stats(50)\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        \n",
        "        print(f\"\\nüìä Rollout {rollout_num} Summary:\")\n",
        "        print(f\"  ‚è±Ô∏è  Time: {elapsed_time:.1f}s ({elapsed_time/60:.1f}m)\")\n",
        "        print(f\"  üìà Total: {self.total_episodes} episodes, {self.total_steps:,} steps\")\n",
        "        \n",
        "        if recent_stats['episodes'] > 0:\n",
        "            print(f\"  üéØ Recent Performance (last {recent_stats['episodes']} episodes):\")\n",
        "            print(f\"    Avg Reward: {recent_stats['avg_reward']:.3f} ¬± {recent_stats['std_reward']:.3f}\")\n",
        "            print(f\"    Avg Length: {recent_stats['avg_length']:.1f} steps\")\n",
        "            print(f\"    Avg Catches: {recent_stats['avg_catches']:.2f}\")\n",
        "            print(f\"    Success Rate: {recent_stats['success_rate']:.1%}\")\n",
        "            print(f\"    Timeout Rate: {recent_stats['timeout_rate']:.1%}\")\n",
        "            print(f\"    Catch Efficiency: {recent_stats['catch_efficiency']:.4f}\")\n",
        "        \n",
        "        print(f\"  üß† Training Metrics:\")\n",
        "        print(f\"    Actor Loss: {training_stats['actor_loss']:.4f}\")\n",
        "        print(f\"    Critic Loss: {training_stats['critic_loss']:.4f}\")\n",
        "        print(f\"    Entropy: {training_stats['entropy']:.4f}\")\n",
        "        print(f\"    Clip Fraction: {training_stats['clip_fraction']:.3f}\")\n",
        "        print(f\"    KL Divergence: {training_stats['kl_div']:.4f}\")\n",
        "        print(f\"    Learning Rates: Actor {training_stats['actor_lr']:.2e}, Critic {training_stats['critic_lr']:.2e}\")\n",
        "        \n",
        "        # Exploration info\n",
        "        with torch.no_grad():\n",
        "            current_std = torch.exp(actor.log_std).mean().item()\n",
        "        print(f\"    Exploration Std: {current_std:.3f}\")\n",
        "\n",
        "def enhanced_rl_training(ppo_trainer, num_rollouts=100, test_mode=False):\n",
        "    \"\"\"FIXED: Enhanced RL training loop with comprehensive monitoring\"\"\"\n",
        "    \n",
        "    if ppo_trainer is None:\n",
        "        print(\"‚ùå Cannot start training - PPO trainer not available\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üöÄ Starting Enhanced RL Training!\")\n",
        "    print(f\"  Rollouts: {num_rollouts}\")\n",
        "    print(f\"  Steps per rollout: {RLConfig.ROLLOUT_STEPS}\")\n",
        "    print(f\"  Total steps: {num_rollouts * RLConfig.ROLLOUT_STEPS:,}\")\n",
        "    print(f\"  Test mode: {test_mode}\")\n",
        "    \n",
        "    # Initialize environment and metrics\n",
        "    env = EnhancedPredatorEnvironment()\n",
        "    metrics = ComprehensiveMetrics()\n",
        "    \n",
        "    # Create checkpoints directory\n",
        "    os.makedirs(RLConfig.RL_CHECKPOINT_DIR, exist_ok=True)\n",
        "    \n",
        "    # Training loop\n",
        "    best_performance = 0.0\n",
        "    \n",
        "    for rollout in range(1, num_rollouts + 1):\n",
        "        rollout_start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Collect rollout\n",
        "            episode_stats = ppo_trainer.collect_rollout(env)\n",
        "            \n",
        "            # Update policy\n",
        "            training_stats = ppo_trainer.update_policy()\n",
        "            \n",
        "            # Track metrics\n",
        "            metrics.add_rollout(episode_stats, training_stats, rollout)\n",
        "            ppo_trainer.episode_stats.extend(episode_stats)\n",
        "            \n",
        "            # Progress reporting\n",
        "            if rollout % RLConfig.LOG_INTERVAL == 0:\n",
        "                metrics.print_progress(rollout, training_stats)\n",
        "                \n",
        "                # Performance tracking\n",
        "                recent_stats = metrics.get_recent_stats(100)\n",
        "                current_performance = recent_stats.get('avg_catches', 0) * recent_stats.get('success_rate', 0)\n",
        "                \n",
        "                if current_performance > best_performance:\n",
        "                    best_performance = current_performance\n",
        "                    print(f\"    üèÜ New best performance: {best_performance:.3f}\")\n",
        "            \n",
        "            # Save checkpoints\n",
        "            if rollout % RLConfig.SAVE_INTERVAL == 0:\n",
        "                checkpoint_path = f\"{RLConfig.RL_CHECKPOINT_DIR}/checkpoint_rollout_{rollout}.pt\"\n",
        "                recent_stats = metrics.get_recent_stats(100)\n",
        "                ppo_trainer.save_checkpoint(checkpoint_path, rollout, recent_stats)\n",
        "                \n",
        "                # Save as latest\n",
        "                latest_path = f\"{RLConfig.RL_CHECKPOINT_DIR}/latest_checkpoint.pt\"\n",
        "                ppo_trainer.save_checkpoint(latest_path, rollout, recent_stats)\n",
        "            \n",
        "            # Early stopping for test mode\n",
        "            if test_mode and rollout >= 5:\n",
        "                print(f\"üß™ Test mode: stopping early after {rollout} rollouts\")\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in rollout {rollout}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            break\n",
        "    \n",
        "    # Final statistics\n",
        "    total_time = time.time() - metrics.start_time\n",
        "    final_stats = metrics.get_recent_stats(200)\n",
        "    \n",
        "    print(f\"\\nüéâ Training Completed!\")\n",
        "    print(f\"  Duration: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
        "    print(f\"  Total Episodes: {metrics.total_episodes}\")\n",
        "    print(f\"  Total Steps: {metrics.total_steps:,}\")\n",
        "    print(f\"  Total Catches: {metrics.total_catches}\")\n",
        "    print(f\"  Best Performance: {best_performance:.3f}\")\n",
        "    \n",
        "    if final_stats['episodes'] > 0:\n",
        "        print(f\"\\nüìà Final Performance (last {final_stats['episodes']} episodes):\")\n",
        "        print(f\"  Average Reward: {final_stats['avg_reward']:.3f}\")\n",
        "        print(f\"  Average Catches: {final_stats['avg_catches']:.2f}\")\n",
        "        print(f\"  Success Rate: {final_stats['success_rate']:.1%}\")\n",
        "        print(f\"  Catch Efficiency: {final_stats['catch_efficiency']:.4f}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def quick_training_test():\n",
        "    \"\"\"Quick test of the training system\"\"\"\n",
        "    if ppo_trainer is None:\n",
        "        print(\"‚ùå Cannot run training test - PPO trainer not available\")\n",
        "        return None\n",
        "    \n",
        "    print(\"üß™ Running quick training test (5 rollouts)...\")\n",
        "    \n",
        "    # Temporarily reduce rollout size for quick test\n",
        "    original_rollout_steps = RLConfig.ROLLOUT_STEPS\n",
        "    RLConfig.ROLLOUT_STEPS = 256  # Smaller for quick test\n",
        "    ppo_trainer.buffer = ExperienceBuffer(RLConfig.ROLLOUT_STEPS)  # Update buffer\n",
        "    \n",
        "    try:\n",
        "        test_metrics = enhanced_rl_training(ppo_trainer, num_rollouts=5, test_mode=True)\n",
        "        \n",
        "        if test_metrics:\n",
        "            print(\"‚úÖ Training test completed successfully!\")\n",
        "            return test_metrics\n",
        "        else:\n",
        "            print(\"‚ùå Training test failed\")\n",
        "            return None\n",
        "            \n",
        "    finally:\n",
        "        # Restore original settings\n",
        "        RLConfig.ROLLOUT_STEPS = original_rollout_steps\n",
        "        ppo_trainer.buffer = ExperienceBuffer(RLConfig.ROLLOUT_STEPS)\n",
        "\n",
        "# Run quick test\n",
        "if ppo_trainer is not None:\n",
        "    print(\"üß™ Testing enhanced training system...\")\n",
        "    test_results = quick_training_test()\n",
        "    \n",
        "    if test_results:\n",
        "        print(\"\\n‚úÖ All systems ready for full training!\")\n",
        "        print(\"üöÄ To start full training, run:\")\n",
        "        print(\"   metrics = enhanced_rl_training(ppo_trainer, num_rollouts=500)\")\n",
        "    else:\n",
        "        print(\"‚ùå Training test failed - check implementation\")\n",
        "else:\n",
        "    print(\"‚ùå Skipping training test - PPO trainer not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation, Visualization, and Export Tools\n",
        "\n",
        "def evaluate_rl_agent(actor, critic, num_episodes=50, verbose=True):\n",
        "    \"\"\"Comprehensive evaluation of the RL-trained agent\"\"\"\n",
        "    \n",
        "    if actor is None:\n",
        "        print(\"‚ùå Cannot evaluate - actor not available\")\n",
        "        return None, None\n",
        "    \n",
        "    print(f\"üìä Evaluating RL agent over {num_episodes} episodes...\")\n",
        "    \n",
        "    env = EnhancedPredatorEnvironment()\n",
        "    eval_stats = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        obs, info = env.reset()\n",
        "        \n",
        "        episode_reward = 0\n",
        "        episode_catches = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                action, _ = actor.get_action_and_log_prob(obs)\n",
        "            \n",
        "            obs, reward, done, step_info = env.step(action)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_catches += step_info['catches_this_step']\n",
        "            episode_length += 1\n",
        "        \n",
        "        eval_stats.append({\n",
        "            'episode': episode + 1,\n",
        "            'reward': episode_reward,\n",
        "            'length': episode_length,\n",
        "            'catches': episode_catches,\n",
        "            'success': episode_catches > 0,\n",
        "            'efficiency': episode_catches / episode_length if episode_length > 0 else 0,\n",
        "            'termination': step_info['termination_reason'],\n",
        "            'canvas_size': info['canvas_width'] * info['canvas_height'],\n",
        "            'initial_boids': info['initial_boids']\n",
        "        })\n",
        "        \n",
        "        if verbose and (episode + 1) % 10 == 0:\n",
        "            recent_stats = eval_stats[-10:]\n",
        "            avg_catches = np.mean([s['catches'] for s in recent_stats])\n",
        "            success_rate = np.mean([s['success'] for s in recent_stats])\n",
        "            print(f\"  Episodes {episode-8}-{episode+1}: {avg_catches:.1f} catches/ep, {success_rate:.1%} success\")\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    summary = {\n",
        "        'total_episodes': len(eval_stats),\n",
        "        'avg_reward': np.mean([s['reward'] for s in eval_stats]),\n",
        "        'std_reward': np.std([s['reward'] for s in eval_stats]),\n",
        "        'avg_catches': np.mean([s['catches'] for s in eval_stats]),\n",
        "        'avg_length': np.mean([s['length'] for s in eval_stats]),\n",
        "        'success_rate': np.mean([s['success'] for s in eval_stats]),\n",
        "        'avg_efficiency': np.mean([s['efficiency'] for s in eval_stats]),\n",
        "        'timeout_rate': np.mean([1 if s['termination'] == 'timeout' else 0 for s in eval_stats]),\n",
        "        'total_catches': sum([s['catches'] for s in eval_stats])\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nüìà Evaluation Results:\")\n",
        "    print(f\"  Episodes: {summary['total_episodes']}\")\n",
        "    print(f\"  Average Reward: {summary['avg_reward']:.3f} ¬± {summary['std_reward']:.3f}\")\n",
        "    print(f\"  Average Catches: {summary['avg_catches']:.2f}\")\n",
        "    print(f\"  Success Rate: {summary['success_rate']:.1%}\")\n",
        "    print(f\"  Average Efficiency: {summary['avg_efficiency']:.4f} catches/step\")\n",
        "    print(f\"  Average Episode Length: {summary['avg_length']:.1f} steps\")\n",
        "    print(f\"  Timeout Rate: {summary['timeout_rate']:.1%}\")\n",
        "    print(f\"  Total Catches: {summary['total_catches']}\")\n",
        "    \n",
        "    return eval_stats, summary\n",
        "\n",
        "def plot_training_metrics(metrics, save_path=None):\n",
        "    \"\"\"Enhanced training metrics visualization\"\"\"\n",
        "    \n",
        "    if metrics is None or len(metrics.episode_data) == 0:\n",
        "        print(\"‚ùå No metrics to plot\")\n",
        "        return\n",
        "    \n",
        "    # Create comprehensive plots\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "    fig.suptitle('Enhanced RL Training Metrics', fontsize=16)\n",
        "    \n",
        "    # Episode rewards\n",
        "    rewards = [ep['reward'] for ep in metrics.episode_data]\n",
        "    axes[0,0].plot(rewards, alpha=0.7)\n",
        "    if len(rewards) > 50:\n",
        "        window = min(50, len(rewards) // 10)\n",
        "        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        axes[0,0].plot(range(window//2, len(smoothed)+window//2), smoothed, 'r-', linewidth=2)\n",
        "    axes[0,0].set_title('Episode Rewards')\n",
        "    axes[0,0].set_xlabel('Episode')\n",
        "    axes[0,0].set_ylabel('Reward')\n",
        "    axes[0,0].grid(True)\n",
        "    \n",
        "    # Catches per episode\n",
        "    catches = [ep['catches'] for ep in metrics.episode_data]\n",
        "    axes[0,1].plot(catches, alpha=0.7)\n",
        "    if len(catches) > 50:\n",
        "        window = min(50, len(catches) // 10)\n",
        "        smoothed = np.convolve(catches, np.ones(window)/window, mode='valid')\n",
        "        axes[0,1].plot(range(window//2, len(smoothed)+window//2), smoothed, 'r-', linewidth=2)\n",
        "    axes[0,1].set_title('Catches per Episode')\n",
        "    axes[0,1].set_xlabel('Episode')\n",
        "    axes[0,1].set_ylabel('Catches')\n",
        "    axes[0,1].grid(True)\n",
        "    \n",
        "    # Success rate (rolling average)\n",
        "    successes = [1 if ep['catches'] > 0 else 0 for ep in metrics.episode_data]\n",
        "    if len(successes) > 20:\n",
        "        window = min(50, len(successes) // 5)\n",
        "        success_rate = np.convolve(successes, np.ones(window)/window, mode='valid')\n",
        "        axes[0,2].plot(range(window//2, len(success_rate)+window//2), success_rate)\n",
        "        axes[0,2].set_title(f'Success Rate (rolling avg, window={window})')\n",
        "        axes[0,2].set_xlabel('Episode')\n",
        "        axes[0,2].set_ylabel('Success Rate')\n",
        "        axes[0,2].grid(True)\n",
        "    \n",
        "    # Training losses\n",
        "    if len(metrics.training_stats) > 0:\n",
        "        actor_losses = [stat['actor_loss'] for stat in metrics.training_stats]\n",
        "        critic_losses = [stat['critic_loss'] for stat in metrics.training_stats]\n",
        "        axes[1,0].plot(actor_losses, label='Actor Loss')\n",
        "        axes[1,0].plot(critic_losses, label='Critic Loss')\n",
        "        axes[1,0].set_title('Training Losses')\n",
        "        axes[1,0].set_xlabel('Update')\n",
        "        axes[1,0].set_ylabel('Loss')\n",
        "        axes[1,0].legend()\n",
        "        axes[1,0].grid(True)\n",
        "        \n",
        "        # Entropy and exploration\n",
        "        entropies = [stat['entropy'] for stat in metrics.training_stats]\n",
        "        axes[1,1].plot(entropies, 'g-', label='Entropy')\n",
        "        axes[1,1].set_title('Policy Entropy')\n",
        "        axes[1,1].set_xlabel('Update')\n",
        "        axes[1,1].set_ylabel('Entropy')\n",
        "        axes[1,1].grid(True)\n",
        "        \n",
        "        # KL divergence and clip fraction\n",
        "        kl_divs = [stat['kl_div'] for stat in metrics.training_stats]\n",
        "        clip_fracs = [stat['clip_fraction'] for stat in metrics.training_stats]\n",
        "        ax1 = axes[1,2]\n",
        "        ax2 = ax1.twinx()\n",
        "        ax1.plot(kl_divs, 'b-', label='KL Divergence')\n",
        "        ax2.plot(clip_fracs, 'r-', label='Clip Fraction')\n",
        "        ax1.set_xlabel('Update')\n",
        "        ax1.set_ylabel('KL Divergence', color='b')\n",
        "        ax2.set_ylabel('Clip Fraction', color='r')\n",
        "        ax1.set_title('KL Divergence & Clip Fraction')\n",
        "        ax1.grid(True)\n",
        "    \n",
        "    # Episode lengths\n",
        "    lengths = [ep['length'] for ep in metrics.episode_data]\n",
        "    axes[2,0].plot(lengths, alpha=0.7)\n",
        "    if len(lengths) > 50:\n",
        "        window = min(50, len(lengths) // 10)\n",
        "        smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
        "        axes[2,0].plot(range(window//2, len(smoothed)+window//2), smoothed, 'r-', linewidth=2)\n",
        "    axes[2,0].set_title('Episode Lengths')\n",
        "    axes[2,0].set_xlabel('Episode')\n",
        "    axes[2,0].set_ylabel('Steps')\n",
        "    axes[2,0].grid(True)\n",
        "    \n",
        "    # Catch efficiency\n",
        "    efficiency = [ep['catches'] / max(ep['length'], 1) for ep in metrics.episode_data]\n",
        "    axes[2,1].plot(efficiency, alpha=0.7)\n",
        "    if len(efficiency) > 50:\n",
        "        window = min(50, len(efficiency) // 10)\n",
        "        smoothed = np.convolve(efficiency, np.ones(window)/window, mode='valid')\n",
        "        axes[2,1].plot(range(window//2, len(smoothed)+window//2), smoothed, 'r-', linewidth=2)\n",
        "    axes[2,1].set_title('Catch Efficiency')\n",
        "    axes[2,1].set_xlabel('Episode')\n",
        "    axes[2,1].set_ylabel('Catches per Step')\n",
        "    axes[2,1].grid(True)\n",
        "    \n",
        "    # Learning rates\n",
        "    if len(metrics.training_stats) > 0:\n",
        "        actor_lrs = [stat['actor_lr'] for stat in metrics.training_stats]\n",
        "        critic_lrs = [stat['critic_lr'] for stat in metrics.training_stats]\n",
        "        axes[2,2].plot(actor_lrs, label='Actor LR')\n",
        "        axes[2,2].plot(critic_lrs, label='Critic LR')\n",
        "        axes[2,2].set_title('Learning Rates')\n",
        "        axes[2,2].set_xlabel('Update')\n",
        "        axes[2,2].set_ylabel('Learning Rate')\n",
        "        axes[2,2].legend()\n",
        "        axes[2,2].grid(True)\n",
        "        axes[2,2].set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"üìä Plots saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    print(\"üìä Training metrics plotted!\")\n",
        "\n",
        "def export_rl_model(actor, output_path=\"policy/transformer/models/rl_trained_model.js\"):\n",
        "    \"\"\"Export RL-trained transformer to JavaScript with comprehensive validation\"\"\"\n",
        "    \n",
        "    if actor is None:\n",
        "        print(\"‚ùå Cannot export - actor not available\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"üîÑ Exporting RL-trained model to JavaScript...\")\n",
        "    \n",
        "    # Create export checkpoint\n",
        "    checkpoint_path = f\"{RLConfig.RL_CHECKPOINT_DIR}/export_checkpoint.pt\"\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "    \n",
        "    # Get current exploration parameters\n",
        "    with torch.no_grad():\n",
        "        exploration_std = torch.exp(actor.log_std).cpu().numpy().tolist()\n",
        "    \n",
        "    checkpoint = {\n",
        "        'model_state_dict': actor.transformer.state_dict(),\n",
        "        'episode': 'rl_export',\n",
        "        'architecture': actor.architecture,\n",
        "        'exploration_std': exploration_std,\n",
        "        'training_type': 'reinforcement_learning_fine_tuned',\n",
        "        'base_model': 'supervised_learning_transformer',\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"‚úÖ Saved RL export checkpoint: {checkpoint_path}\")\n",
        "    \n",
        "    # Use export_to_js.py script\n",
        "    try:\n",
        "        import subprocess\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"export_to_js.py\",\n",
        "            \"--checkpoint\", checkpoint_path,\n",
        "            \"--output\", output_path,\n",
        "            \"--info\"\n",
        "        ], capture_output=True, text=True)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ RL model exported successfully!\")\n",
        "            print(f\"üìÅ Output: {output_path}\")\n",
        "            print(\"üéâ RL-trained model ready for browser deployment!\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"üöÄ EXPORT DETAILS:\")\n",
        "            print(\"=\"*60)\n",
        "            print(result.stdout)\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Export failed:\")\n",
        "            print(result.stderr)\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Export error: {e}\")\n",
        "        return False\n",
        "\n",
        "def compare_sl_vs_rl(sl_actor, rl_actor, num_episodes=30):\n",
        "    \"\"\"Compare Supervised Learning vs Reinforcement Learning performance\"\"\"\n",
        "    \n",
        "    if sl_actor is None or rl_actor is None:\n",
        "        print(\"‚ùå Cannot compare - need both SL and RL actors\")\n",
        "        return None, None\n",
        "    \n",
        "    print(f\"‚öîÔ∏è  Comparing SL vs RL Performance ({num_episodes} episodes each)...\")\n",
        "    \n",
        "    # Evaluate SL model\n",
        "    print(\"\\\\nüìä Evaluating SL model...\")\n",
        "    sl_stats, sl_summary = evaluate_rl_agent(sl_actor, None, num_episodes, verbose=False)\n",
        "    \n",
        "    # Evaluate RL model  \n",
        "    print(\"\\\\nüìä Evaluating RL model...\")\n",
        "    rl_stats, rl_summary = evaluate_rl_agent(rl_actor, None, num_episodes, verbose=False)\n",
        "    \n",
        "    # Detailed comparison\n",
        "    print(f\"\\\\n‚öîÔ∏è  Detailed Performance Comparison:\")\n",
        "    print(f\"{'Metric':<25} | {'SL Model':<12} | {'RL Model':<12} | {'Improvement':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    metrics_comparison = [\n",
        "        ('Avg Catches', 'avg_catches', '{:.2f}'),\n",
        "        ('Success Rate', 'success_rate', '{:.1%}'),\n",
        "        ('Catch Efficiency', 'avg_efficiency', '{:.4f}'),\n",
        "        ('Avg Episode Length', 'avg_length', '{:.1f}'),\n",
        "        ('Timeout Rate', 'timeout_rate', '{:.1%}'),\n",
        "        ('Avg Reward', 'avg_reward', '{:.3f}'),\n",
        "        ('Total Catches', 'total_catches', '{:.0f}')\n",
        "    ]\n",
        "    \n",
        "    for metric_name, metric_key, format_str in metrics_comparison:\n",
        "        sl_val = sl_summary[metric_key]\n",
        "        rl_val = rl_summary[metric_key]\n",
        "        \n",
        "        # Calculate improvement (handle division by zero)\n",
        "        if sl_val != 0:\n",
        "            if metric_name in ['Timeout Rate']:  # Lower is better\n",
        "                improvement = ((sl_val - rl_val) / abs(sl_val)) * 100\n",
        "            else:  # Higher is better\n",
        "                improvement = ((rl_val - sl_val) / abs(sl_val)) * 100\n",
        "        else:\n",
        "            improvement = float('inf') if rl_val > 0 else 0\n",
        "        \n",
        "        sl_str = format_str.format(sl_val)\n",
        "        rl_str = format_str.format(rl_val)\n",
        "        \n",
        "        if improvement != float('inf'):\n",
        "            imp_str = f\"{improvement:+7.1f}%\"\n",
        "        else:\n",
        "            imp_str = \"    ‚àû%\"\n",
        "        \n",
        "        print(f\"{metric_name:<25} | {sl_str:<12} | {rl_str:<12} | {imp_str:<12}\")\n",
        "    \n",
        "    return sl_stats, rl_stats\n",
        "\n",
        "# Summary and instructions\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"üéØ ENHANCED RL TRAINING NOTEBOOK - COMPREHENSIVE IMPLEMENTATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ All critical issues from original notebook have been FIXED:\")\n",
        "print(\"\")\n",
        "print(\"üîß MAJOR FIXES IMPLEMENTED:\")\n",
        "print(\"  1. ‚úÖ Fixed StateManager integration (no more direct simulation access)\")\n",
        "print(\"  2. ‚úÖ Fixed stochastic actor policy with learnable exploration\") \n",
        "print(\"  3. ‚úÖ Implemented proper GAE with episode boundary handling\")\n",
        "print(\"  4. ‚úÖ Fixed memory-efficient experience buffer\")\n",
        "print(\"  5. ‚úÖ Enhanced feature extraction with error handling\")\n",
        "print(\"  6. ‚úÖ Comprehensive metrics and progress tracking\")\n",
        "print(\"  7. ‚úÖ Single source of truth configuration system\")\n",
        "print(\"  8. ‚úÖ Maintained multi-step reward attribution (as requested)\")\n",
        "print(\"\")\n",
        "print(\"üöÄ READY FOR FULL TRAINING:\")\n",
        "print(\"   # For full training, run:\")\n",
        "print(\"   # metrics = enhanced_rl_training(ppo_trainer, num_rollouts=500)\")\n",
        "print(\"\")\n",
        "print(\"üìä EVALUATION AND EXPORT:\")\n",
        "print(\"   # Evaluate trained model:\")\n",
        "print(\"   # eval_stats, eval_summary = evaluate_rl_agent(actor, critic, num_episodes=50)\")\n",
        "print(\"   # \")\n",
        "print(\"   # Plot training progress:\")\n",
        "print(\"   # plot_training_metrics(metrics)\")\n",
        "print(\"   #\")\n",
        "print(\"   # Export to JavaScript:\")\n",
        "print(\"   # export_rl_model(actor)\")\n",
        "print(\"\")\n",
        "print(\"‚öîÔ∏è  COMPARISON:\")\n",
        "print(\"   # Compare SL vs RL performance:\")\n",
        "print(\"   # sl_stats, rl_stats = compare_sl_vs_rl(sl_actor, rl_actor)\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
