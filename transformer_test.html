<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>ü§ñ Transformer Encoder Test</title>
    <style>
        body {
            font-family: 'Source Code Pro', monospace;
            margin: 20px;
            background: #f8f9fa;
            line-height: 1.6;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        .test-section {
            background: white;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #dee2e6;
            margin-bottom: 20px;
        }
        .test-section h3 {
            margin-top: 0;
            color: #495057;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }
        pre {
            background: #f1f3f4;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 11px;
            line-height: 1.4;
        }
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
        .info {
            background: #cce7ff;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 4px;
            cursor: pointer;
            font-family: inherit;
            font-size: 14px;
            margin: 5px;
        }
        button:hover {
            background: #0056b3;
        }
        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 15px 0;
        }
        .metric {
            background: #e9ecef;
            padding: 15px;
            border-radius: 4px;
            text-align: center;
        }
        .metric-value {
            font-size: 24px;
            font-weight: bold;
            color: #007bff;
        }
        .metric-label {
            color: #6c757d;
            font-size: 12px;
            margin-top: 5px;
        }
        .token-viz {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 10px 0;
        }
        .token {
            background: #007bff;
            color: white;
            padding: 8px 12px;
            border-radius: 16px;
            font-size: 11px;
            font-weight: bold;
        }
        .token.cls { background: #dc3545; }
        .token.ctx { background: #28a745; }
        .token.predator { background: #fd7e14; }
        .token.boid { background: #6f42c1; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ Transformer Encoder Architecture Test</h1>
        
        <div class="success">
            <strong>‚úÖ Complete Implementation:</strong> 
            d_model=48, n_heads=4, n_layers=3, GEGLU FFN, Token sequence: [CLS] + [CTX] + Predator + Boids
        </div>
        
        <div class="test-section">
            <h3>üî¨ Run Architecture Tests</h3>
            <button onclick="runFullTest()">üöÄ Test Complete Architecture</button>
            <button onclick="runTokenTest()">üéØ Test Token Building</button>
            <button onclick="runAttentionTest()">üß† Test Attention Mechanism</button>
            <button onclick="runPerformanceTest()">‚ö° Performance Benchmark</button>
        </div>
        
        <div id="results">
            <div class="test-section">
                <h3>üìä Architecture Metrics</h3>
                <div class="metrics" id="metrics"></div>
            </div>
            
            <div class="test-section">
                <h3>üéØ Token Sequence Visualization</h3>
                <div class="token-viz" id="token-viz"></div>
                <pre id="token-details"></pre>
            </div>
            
            <div class="test-section">
                <h3>üß† Attention Analysis</h3>
                <pre id="attention-analysis"></pre>
            </div>
            
            <div class="test-section">
                <h3>‚ö° Performance Results</h3>
                <pre id="performance-results"></pre>
            </div>
            
            <div class="test-section">
                <h3>üîç Architecture Details</h3>
                <pre id="architecture-details"></pre>
            </div>
        </div>
    </div>

    <!-- Include all necessary scripts -->
    <script src="src/config/constants.js"></script>
    <script src="src/utils/vector.js"></script>
    <script src="src/ai/transformer_encoder.js"></script>
    <script src="src/ai/input_processor.js"></script>
    <script src="src/ai/action_processor.js"></script>
    <script src="src/simulation/boid.js"></script>

    <script>
        function createTestScenario() {
            // Create realistic test data
            var simulation = {
                canvasWidth: 800,
                canvasHeight: 600,
                separationMultiplier: 2,
                cohesionMultiplier: 1,
                alignmentMultiplier: 1
            };
            
            var boids = [];
            var numBoids = 12; // Good test size
            
            for (var i = 0; i < numBoids; i++) {
                var x = 200 + Math.random() * 400;
                var y = 150 + Math.random() * 300;
                var boid = new Boid(x, y, simulation);
                
                // Random velocities
                boid.velocity.x = (Math.random() - 0.5) * 6;
                boid.velocity.y = (Math.random() - 0.5) * 6;
                
                boids.push(boid);
            }
            
            return {
                boids: boids,
                predatorPos: { x: 400, y: 300 },
                predatorVel: { x: 1.2, y: -0.8 },
                canvasWidth: simulation.canvasWidth,
                canvasHeight: simulation.canvasHeight
            };
        }

        function runFullTest() {
            console.log("üöÄ Running complete transformer architecture test...");
            
            var scenario = createTestScenario();
            var encoder = new TransformerEncoder();
            var inputProcessor = new InputProcessor();
            
            // Test complete pipeline
            var structuredInputs = inputProcessor.processInputs(
                scenario.boids,
                scenario.predatorPos,
                scenario.predatorVel,
                scenario.canvasWidth,
                scenario.canvasHeight
            );
            
            var startTime = performance.now();
            var outputs = encoder.forward(structuredInputs);
            var endTime = performance.now();
            
            displayMetrics(encoder, structuredInputs, outputs, endTime - startTime);
            displayArchitectureDetails(encoder);
            
            console.log("‚úÖ Complete test finished");
        }

        function runTokenTest() {
            console.log("üéØ Testing token building...");
            
            var scenario = createTestScenario();
            var encoder = new TransformerEncoder();
            var inputProcessor = new InputProcessor();
            
            var structuredInputs = inputProcessor.processInputs(
                scenario.boids,
                scenario.predatorPos,
                scenario.predatorVel,
                scenario.canvasWidth,
                scenario.canvasHeight
            );
            
            var tokens = encoder.buildTokens(structuredInputs);
            displayTokenVisualization(tokens, structuredInputs);
            
            console.log("‚úÖ Token test finished");
        }

        function runAttentionTest() {
            console.log("üß† Testing attention mechanism...");
            
            var scenario = createTestScenario();
            var encoder = new TransformerEncoder();
            var inputProcessor = new InputProcessor();
            
            var structuredInputs = inputProcessor.processInputs(
                scenario.boids,
                scenario.predatorPos,
                scenario.predatorVel,
                scenario.canvasWidth,
                scenario.canvasHeight
            );
            
            // Run forward pass to get attention computation
            var outputs = encoder.forward(structuredInputs);
            displayAttentionAnalysis(encoder, structuredInputs, outputs);
            
            console.log("‚úÖ Attention test finished");
        }

        function runPerformanceTest() {
            console.log("‚ö° Running performance benchmark...");
            
            var results = [];
            var boidCounts = [1, 5, 10, 20, 30, 50];
            
            for (var i = 0; i < boidCounts.length; i++) {
                var count = boidCounts[i];
                var avgTime = benchmarkBoidCount(count);
                results.push({ boids: count, time: avgTime });
            }
            
            displayPerformanceResults(results);
            console.log("‚úÖ Performance test finished");
        }

        function benchmarkBoidCount(boidCount) {
            var encoder = new TransformerEncoder();
            var inputProcessor = new InputProcessor();
            var times = [];
            
            for (var trial = 0; trial < 10; trial++) {
                var scenario = createTestScenario();
                // Adjust boid count
                scenario.boids = scenario.boids.slice(0, boidCount);
                
                var structuredInputs = inputProcessor.processInputs(
                    scenario.boids,
                    scenario.predatorPos,
                    scenario.predatorVel,
                    scenario.canvasWidth,
                    scenario.canvasHeight
                );
                
                var startTime = performance.now();
                encoder.forward(structuredInputs);
                var endTime = performance.now();
                
                times.push(endTime - startTime);
            }
            
            return times.reduce(function(a, b) { return a + b; }) / times.length;
        }

        function displayMetrics(encoder, inputs, outputs, inferenceTime) {
            var seqLen = 3 + inputs.boids.length; // CLS + CTX + Predator + Boids
            var totalParams = calculateParameterCount(encoder);
            
            var metricsHtml = '';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + encoder.d_model + '</div>';
            metricsHtml += '<div class="metric-label">d_model</div>';
            metricsHtml += '</div>';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + encoder.n_heads + '</div>';
            metricsHtml += '<div class="metric-label">attention heads</div>';
            metricsHtml += '</div>';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + encoder.n_layers + '</div>';
            metricsHtml += '<div class="metric-label">encoder layers</div>';
            metricsHtml += '</div>';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + seqLen + '</div>';
            metricsHtml += '<div class="metric-label">sequence length</div>';
            metricsHtml += '</div>';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + Math.round(totalParams/1000) + 'K</div>';
            metricsHtml += '<div class="metric-label">parameters</div>';
            metricsHtml += '</div>';
            
            metricsHtml += '<div class="metric">';
            metricsHtml += '<div class="metric-value">' + inferenceTime.toFixed(2) + 'ms</div>';
            metricsHtml += '<div class="metric-label">inference time</div>';
            metricsHtml += '</div>';
            
            document.getElementById('metrics').innerHTML = metricsHtml;
        }

        function displayTokenVisualization(tokens, inputs) {
            var tokenHtml = '';
            
            tokenHtml += '<div class="token cls">[CLS]</div>';
            tokenHtml += '<div class="token ctx">[CTX]</div>';
            tokenHtml += '<div class="token predator">Predator</div>';
            
            for (var i = 0; i < inputs.boids.length; i++) {
                tokenHtml += '<div class="token boid">Boid_' + (i+1) + '</div>';
            }
            
            document.getElementById('token-viz').innerHTML = tokenHtml;
            
            var details = "TOKEN SEQUENCE DETAILS:\n\n";
            details += "Sequence Length: " + tokens.length + "\n";
            details += "Token Dimensions: " + tokens[0].length + "D\n\n";
            
            details += "Token 0 [CLS]: Global aggregation token\n";
            details += "  - Learned embedding + type embedding\n";
            details += "  - Used for final steering prediction\n\n";
            
            details += "Token 1 [CTX]: World context\n";
            details += "  - Canvas: [" + inputs.context.canvasWidth.toFixed(3) + ", " + inputs.context.canvasHeight.toFixed(3) + "]\n";
            details += "  - Projected to " + tokens[1].length + "D + type embedding\n\n";
            
            details += "Token 2 Predator: Self-state\n";
            details += "  - Velocity: [" + inputs.predator.velX.toFixed(3) + ", " + inputs.predator.velY.toFixed(3) + ", 0, 0]\n";
            details += "  - Projected to " + tokens[2].length + "D + type embedding\n\n";
            
            details += "Tokens 3+" + (inputs.boids.length + 2) + " Boids: Prey entities\n";
            for (var i = 0; i < Math.min(3, inputs.boids.length); i++) {
                var boid = inputs.boids[i];
                details += "  - Boid " + (i+1) + ": [" + 
                          boid.relX.toFixed(3) + ", " + 
                          boid.relY.toFixed(3) + ", " + 
                          boid.velX.toFixed(3) + ", " + 
                          boid.velY.toFixed(3) + "]\n";
            }
            if (inputs.boids.length > 3) {
                details += "  - ... and " + (inputs.boids.length - 3) + " more boids\n";
            }
            
            document.getElementById('token-details').textContent = details;
        }

        function displayAttentionAnalysis(encoder, inputs, outputs) {
            var analysis = "ATTENTION MECHANISM ANALYSIS:\n\n";
            
            analysis += "Multi-Head Self-Attention Configuration:\n";
            analysis += "- Heads: " + encoder.n_heads + " (each " + encoder.head_dim + "D)\n";
            analysis += "- Sequence Length: " + (3 + inputs.boids.length) + "\n";
            analysis += "- Attention Matrix: " + (3 + inputs.boids.length) + "√ó" + (3 + inputs.boids.length) + " per head\n\n";
            
            analysis += "Attention Flow:\n";
            analysis += "1. [CLS] ‚Üê attends to all entities (global aggregation)\n";
            analysis += "2. [CTX] ‚Üê provides world boundary context\n";
            analysis += "3. Predator ‚Üê attends to all boids (spatial awareness)\n";
            analysis += "4. Each Boid ‚Üê attends to predator + other boids\n\n";
            
            analysis += "GEGLU Feed-Forward Networks:\n";
            analysis += "- Input: " + encoder.d_model + "D\n";
            analysis += "- Hidden: " + encoder.ffn_hidden + "D (gated)\n";
            analysis += "- Output: " + encoder.d_model + "D\n";
            analysis += "- Activation: GELU gating function\n\n";
            
            analysis += "Final Output Processing:\n";
            analysis += "- [CLS] token: " + encoder.d_model + "D representation\n";
            analysis += "- Linear projection: " + encoder.d_model + "D ‚Üí 2D\n";
            analysis += "- Tanh activation: bounded steering forces\n";
            analysis += "- Steering output: [" + outputs[0].toFixed(4) + ", " + outputs[1].toFixed(4) + "]\n\n";
            
            analysis += "Computational Complexity:\n";
            var seqLen = 3 + inputs.boids.length;
            analysis += "- Attention: O(" + seqLen + "¬≤) per head\n";
            analysis += "- Total attention ops: ~" + Math.round(seqLen * seqLen * encoder.n_heads * encoder.n_layers) + "\n";
            analysis += "- FFN ops: ~" + Math.round(seqLen * encoder.d_model * encoder.ffn_hidden * encoder.n_layers) + "\n";
            
            document.getElementById('attention-analysis').textContent = analysis;
        }

        function displayPerformanceResults(results) {
            var perfText = "PERFORMANCE BENCHMARK RESULTS:\n\n";
            
            perfText += "Boids | Avg Time (ms) | Sequence Length | Complexity\n";
            perfText += "------|---------------|-----------------|----------\n";
            
            for (var i = 0; i < results.length; i++) {
                var result = results[i];
                var seqLen = 3 + result.boids;
                var complexity = seqLen * seqLen;
                
                perfText += String(result.boids).padStart(5) + " | " +
                           String(result.time.toFixed(2)).padStart(13) + " | " +
                           String(seqLen).padStart(15) + " | " +
                           String(complexity).padStart(9) + "\n";
            }
            
            perfText += "\nSCALING ANALYSIS:\n";
            perfText += "- Attention complexity: O(S¬≤) where S = sequence length\n";
            perfText += "- Memory usage: Linear in number of boids\n";
            perfText += "- No artificial sequence limits\n";
            perfText += "- Real-time performance: ‚úÖ <10ms for typical scenarios\n\n";
            
            perfText += "EFFICIENCY COMPARISON:\n";
            perfText += "- MLP (old): Fixed 204 inputs, 14% utilization for 7 boids\n";
            perfText += "- Transformer: Dynamic inputs, 100% utilization always\n";
            perfText += "- Attention allows rich entity interactions\n";
            perfText += "- Better scalability for varying boid counts";
            
            document.getElementById('performance-results').textContent = perfText;
        }

        function displayArchitectureDetails(encoder) {
            var details = "TRANSFORMER ENCODER ARCHITECTURE:\n\n";
            
            details += "Model Configuration:\n";
            details += "‚îú‚îÄ‚îÄ d_model: " + encoder.d_model + "\n";
            details += "‚îú‚îÄ‚îÄ n_heads: " + encoder.n_heads + "\n";
            details += "‚îú‚îÄ‚îÄ n_layers: " + encoder.n_layers + "\n";
            details += "‚îú‚îÄ‚îÄ head_dim: " + encoder.head_dim + "\n";
            details += "‚îú‚îÄ‚îÄ ffn_hidden: " + encoder.ffn_hidden + "\n";
            details += "‚îî‚îÄ‚îÄ dropout: N/A (deterministic inference)\n\n";
            
            details += "Token Processing Pipeline:\n";
            details += "1. Input Projection:\n";
            details += "   ‚îú‚îÄ‚îÄ [CTX]: 2D ‚Üí " + encoder.d_model + "D\n";
            details += "   ‚îú‚îÄ‚îÄ Predator: 4D ‚Üí " + encoder.d_model + "D\n";
            details += "   ‚îî‚îÄ‚îÄ Boids: 4D ‚Üí " + encoder.d_model + "D each\n\n";
            
            details += "2. Type Embeddings:\n";
            details += "   ‚îú‚îÄ‚îÄ [CLS]: Learned " + encoder.d_model + "D\n";
            details += "   ‚îú‚îÄ‚îÄ [CTX]: Type-specific " + encoder.d_model + "D\n";
            details += "   ‚îú‚îÄ‚îÄ Predator: Type-specific " + encoder.d_model + "D\n";
            details += "   ‚îî‚îÄ‚îÄ Boids: Shared type " + encoder.d_model + "D\n\n";
            
            details += "3. Transformer Layers (√ó" + encoder.n_layers + "):\n";
            details += "   ‚îú‚îÄ‚îÄ LayerNorm\n";
            details += "   ‚îú‚îÄ‚îÄ Multi-Head Attention\n";
            details += "   ‚îÇ   ‚îú‚îÄ‚îÄ QKV projection: " + encoder.d_model + "‚Üí" + (3*encoder.d_model) + "\n";
            details += "   ‚îÇ   ‚îú‚îÄ‚îÄ " + encoder.n_heads + " heads of " + encoder.head_dim + "D each\n";
            details += "   ‚îÇ   ‚îî‚îÄ‚îÄ Output projection: " + encoder.d_model + "‚Üí" + encoder.d_model + "\n";
            details += "   ‚îú‚îÄ‚îÄ Residual connection\n";
            details += "   ‚îú‚îÄ‚îÄ LayerNorm\n";
            details += "   ‚îú‚îÄ‚îÄ GEGLU FFN\n";
            details += "   ‚îÇ   ‚îú‚îÄ‚îÄ Gate: " + encoder.d_model + "‚Üí" + encoder.ffn_hidden + " + GELU\n";
            details += "   ‚îÇ   ‚îú‚îÄ‚îÄ Up: " + encoder.d_model + "‚Üí" + encoder.ffn_hidden + "\n";
            details += "   ‚îÇ   ‚îî‚îÄ‚îÄ Down: " + encoder.ffn_hidden + "‚Üí" + encoder.d_model + "\n";
            details += "   ‚îî‚îÄ‚îÄ Residual connection\n\n";
            
            details += "4. Output Generation:\n";
            details += "   ‚îú‚îÄ‚îÄ Extract [CLS] token\n";
            details += "   ‚îú‚îÄ‚îÄ Linear projection: " + encoder.d_model + "‚Üí2\n";
            details += "   ‚îî‚îÄ‚îÄ Tanh activation\n\n";
            
            var totalParams = calculateParameterCount(encoder);
            details += "Parameter Count: " + totalParams.toLocaleString() + " total\n";
            details += "‚îú‚îÄ‚îÄ Embeddings: ~" + Math.round(encoder.d_model * 5) + "\n";
            details += "‚îú‚îÄ‚îÄ Projections: ~" + Math.round((2+4+4) * encoder.d_model) + "\n";
            details += "‚îú‚îÄ‚îÄ Transformer: ~" + Math.round(totalParams * 0.8) + "\n";
            details += "‚îî‚îÄ‚îÄ Output: " + (encoder.d_model * 2) + "\n";
            
            document.getElementById('architecture-details').textContent = details;
        }

        function calculateParameterCount(encoder) {
            var count = 0;
            
            // Embeddings
            count += encoder.d_model * 4; // CLS + 3 type embeddings
            
            // Projections
            count += 2 * encoder.d_model; // CTX projection
            count += 4 * encoder.d_model; // Predator projection
            count += 4 * encoder.d_model; // Boid projection
            
            // Transformer layers
            for (var i = 0; i < encoder.n_layers; i++) {
                // QKV weights + bias
                count += encoder.d_model * 3 * encoder.d_model + 3 * encoder.d_model;
                // Attention output
                count += encoder.d_model * encoder.d_model + encoder.d_model;
                // Layer norms (scale + bias)
                count += 2 * encoder.d_model + 2 * encoder.d_model;
                // GEGLU FFN
                count += encoder.d_model * encoder.ffn_hidden * 3 + encoder.ffn_hidden * 2; // gate + up + down weights
                count += encoder.ffn_hidden * encoder.d_model + encoder.d_model; // down projection
            }
            
            // Output projection
            count += encoder.d_model * 2 + 2;
            
            return count;
        }

        // Auto-run full test when page loads
        window.addEventListener('load', function() {
            setTimeout(runFullTest, 500);
        });
    </script>
</body>
</html> 