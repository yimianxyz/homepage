{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Codebase from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Repository information\n",
        "REPO_URL = \"https://github.com/yimianxyz/homepage.git\"\n",
        "BRANCH = \"neuro-predator\"\n",
        "REPO_DIR = \"homepage\"\n",
        "\n",
        "def download_codebase():\n",
        "    \"\"\"Download the codebase from GitHub if not already present\"\"\"\n",
        "    \n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(f\"Repository directory '{REPO_DIR}' already exists.\")\n",
        "        \n",
        "        # Check if it's the correct repository and branch\n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "            # Check current branch\n",
        "            result = subprocess.run(['git', 'branch', '--show-current'], \n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            current_branch = result.stdout.strip()\n",
        "            \n",
        "            if current_branch != BRANCH:\n",
        "                print(f\"Switching to branch '{BRANCH}'...\")\n",
        "                subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "            \n",
        "            # Pull latest changes\n",
        "            print(\"Updating repository...\")\n",
        "            subprocess.run(['git', 'pull', 'origin', BRANCH], check=True)\n",
        "            \n",
        "            print(f\"‚úÖ Repository updated successfully!\")\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error updating repository: {e}\")\n",
        "            print(\"Repository directory exists but may not be a valid git repository.\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "        \n",
        "        try:\n",
        "            # Clone the specific branch\n",
        "            subprocess.run(['git', 'clone', '-b', BRANCH, REPO_URL, REPO_DIR], check=True)\n",
        "            \n",
        "            print(f\"‚úÖ Repository cloned successfully!\")\n",
        "            \n",
        "            # Change to repository directory\n",
        "            os.chdir(REPO_DIR)\n",
        "            \n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error cloning repository: {e}\")\n",
        "            print(\"Make sure you have git installed and internet connection.\")\n",
        "            return False\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            return False\n",
        "    \n",
        "    # Verify key files exist\n",
        "    key_files = [\n",
        "        'config/constants.py',\n",
        "        'data_generation/generate_training_data.py',\n",
        "        'export_to_js.py',\n",
        "        'policy/transformer/transformer_policy.js',\n",
        "        'simulation/processors/input_processor.py'\n",
        "    ]\n",
        "    \n",
        "    missing_files = []\n",
        "    for file_path in key_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Some key files are missing:\")\n",
        "        for file_path in missing_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ All key files found!\")\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "# Download the codebase\n",
        "success = download_codebase()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Setup complete! You can now run the training notebook.\")\n",
        "    print(\"üìñ Repository structure:\")\n",
        "    \n",
        "    # Show repository structure\n",
        "    for root, dirs, files in os.walk('.'):\n",
        "        # Skip hidden directories and __pycache__\n",
        "        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
        "        level = root.replace('.', '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        \n",
        "        # Only show first level of files to avoid clutter\n",
        "        if level < 2:\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files:\n",
        "                if not file.startswith('.') and not file.endswith('.pyc'):\n",
        "                    print(f\"{subindent}{file}\")\n",
        "        \n",
        "        # Limit depth to avoid too much output\n",
        "        if level >= 2:\n",
        "            break\n",
        "            \n",
        "else:\n",
        "    print(\"‚ùå Setup failed. Please check the errors above and try again.\")\n",
        "    print(\"Manual setup: git clone -b neuro-predator https://github.com/yimianxyz/homepage.git\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Setup and Import Project Modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the correct directory and add to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'homepage':\n",
        "    print(f\"‚ö†Ô∏è  Warning: Current directory is '{project_root.name}', expected 'homepage'\")\n",
        "    print(\"Make sure the first cell downloaded the repository correctly.\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "try:\n",
        "    from config.constants import CONSTANTS\n",
        "    print(f\"‚úÖ Successfully imported simulation constants\")\n",
        "    print(f\"üìÅ Project root: {project_root}\")\n",
        "    print(f\"üîß Key constants: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}, BOID_MAX_SPEED={CONSTANTS.BOID_MAX_SPEED}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import constants: {e}\")\n",
        "    print(\"Make sure the repository was downloaded correctly in the first cell.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Architecture Configuration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Dict, List, Any, Tuple\n",
        "\n",
        "# Architecture constants - modify these as needed\n",
        "D_MODEL = 48\n",
        "N_HEADS = 4\n",
        "N_LAYERS = 2\n",
        "FFN_HIDDEN = 96\n",
        "DROPOUT = 0.1\n",
        "MAX_BOIDS = 50  # Maximum number of boids to handle\n",
        "\n",
        "# Training constants\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Architecture: d_model={D_MODEL}, n_heads={N_HEADS}, n_layers={N_LAYERS}, ffn_hidden={FFN_HIDDEN}\")\n",
        "print(f\"  Training: batch_size={BATCH_SIZE}, lr={LEARNING_RATE}, device={DEVICE}\")\n",
        "print(f\"  Max boids: {MAX_BOIDS}\")\n",
        "\n",
        "# Import project modules\n",
        "from config.constants import CONSTANTS\n",
        "print(f\"‚úì Simulation constants loaded: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Model Definition\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return x * torch.nn.functional.gelu(gate)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # GEGLU FFN with separate projections for export compatibility\n",
        "        self.ffn_gate_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_up_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_down_proj = nn.Linear(ffn_hidden, d_model)\n",
        "        \n",
        "    def forward(self, x, padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.self_attn(normed, normed, normed, key_padding_mask=padding_mask)\n",
        "        x = x + attn_out\n",
        "        \n",
        "        # FFN with residual\n",
        "        normed = self.norm2(x)\n",
        "        gate = torch.nn.functional.gelu(self.ffn_gate_proj(normed))\n",
        "        up = self.ffn_up_proj(normed)\n",
        "        ffn_out = self.ffn_down_proj(gate * up)\n",
        "        x = x + ffn_out\n",
        "        \n",
        "        return x\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, d_model=48, n_heads=4, n_layers=2, ffn_hidden=96, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "        \n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "        \n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "        \n",
        "        # Input projections\n",
        "        self.ctx_projection = nn.Linear(2, d_model)  # canvas_width, canvas_height\n",
        "        self.predator_projection = nn.Linear(4, d_model)  # velX, velY, 0, 0 (padded to 4D)\n",
        "        self.boid_projection = nn.Linear(4, d_model)  # relX, relY, velX, velY\n",
        "        \n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)  # predator action [x, y]\n",
        "        \n",
        "    def forward(self, structured_inputs, padding_mask=None):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "        \n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "        \n",
        "        # Build token sequences for each sample in batch\n",
        "        sequences = []\n",
        "        masks = []\n",
        "        \n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "            \n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "            \n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']], \n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "            \n",
        "            # Predator token - expand to 4D\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0], \n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "            \n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]  # CLS, CTX, Predator are not padding\n",
        "            \n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']], \n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "            \n",
        "            # Pad to max_boids + 3 (CLS + CTX + Predator)\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)  # Mark as padding\n",
        "            \n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "        \n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Create padding mask\n",
        "        if padding_mask is None:\n",
        "            padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "        \n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "        \n",
        "        # Extract CLS token and project to output\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        action = self.output_projection(cls_output)  # [batch_size, 2]\n",
        "        \n",
        "        # Apply tanh to ensure [-1, 1] range\n",
        "        action = torch.tanh(action)\n",
        "        \n",
        "        return action.squeeze(0) if batch_size == 1 else action\n",
        "\n",
        "print(\"‚úì Model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Class\n",
        "class BoidsDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        print(f\"Loading dataset from {data_path}...\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        self.samples = data['samples']\n",
        "        self.metadata = data['metadata']\n",
        "        \n",
        "        print(f\"‚úì Loaded {len(self.samples)} samples\")\n",
        "        print(f\"  Dataset metadata: {self.metadata['total_samples']} total samples\")\n",
        "        print(f\"  Valid targets: {self.metadata['statistics']['valid_target_percentage']:.1f}%\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Input is the structured format\n",
        "        inputs = sample['input']\n",
        "        \n",
        "        # Output is the action [x, y]\n",
        "        target = torch.tensor(sample['output'], dtype=torch.float32)\n",
        "        \n",
        "        return inputs, target\n",
        "\n",
        "print(\"‚úì Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Initialization\n",
        "def create_model():\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"‚úì Model created:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Architecture: {D_MODEL}√ó{N_HEADS}√ó{N_LAYERS}√ó{FFN_HIDDEN}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create fresh model\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Model from Checkpoint (optional)\n",
        "def load_checkpoint_and_update_arch(checkpoint_path, optimizer=None):\n",
        "    global D_MODEL, N_HEADS, N_LAYERS, FFN_HIDDEN, MAX_BOIDS, model\n",
        "    \n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    \n",
        "    # Extract and update architecture parameters from checkpoint\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        D_MODEL = arch.get('d_model', D_MODEL)\n",
        "        N_HEADS = arch.get('n_heads', N_HEADS)\n",
        "        N_LAYERS = arch.get('n_layers', N_LAYERS)\n",
        "        FFN_HIDDEN = arch.get('ffn_hidden', FFN_HIDDEN)\n",
        "        MAX_BOIDS = arch.get('max_boids', MAX_BOIDS)\n",
        "        \n",
        "        print(f\"‚úì Updated architecture from checkpoint:\")\n",
        "        print(f\"  d_model: {D_MODEL}\")\n",
        "        print(f\"  n_heads: {N_HEADS}\")\n",
        "        print(f\"  n_layers: {N_LAYERS}\")\n",
        "        print(f\"  ffn_hidden: {FFN_HIDDEN}\")\n",
        "        print(f\"  max_boids: {MAX_BOIDS}\")\n",
        "    else:\n",
        "        print(\"Warning: No architecture info in checkpoint, using current values\")\n",
        "    \n",
        "    # Recreate model with correct architecture\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Load model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Update optimizer if provided\n",
        "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    print(f\"‚úì Loaded checkpoint from epoch {epoch}, best val loss: {best_val_loss:.6f}\")\n",
        "    return epoch, best_val_loss, model\n",
        "\n",
        "# Uncomment to load from checkpoint\n",
        "# checkpoint_path = \"checkpoints/best_model.pt\"\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     start_epoch, best_val_loss, model = load_checkpoint_and_update_arch(checkpoint_path)\n",
        "#     print(f\"‚úì Model recreated with checkpoint architecture\")\n",
        "# else:\n",
        "#     print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "#     start_epoch, best_val_loss = 0, float('inf')\n",
        "\n",
        "start_epoch, best_val_loss = 0, float('inf')\n",
        "print(f\"‚úì Starting from epoch {start_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Training Data (if needed)\n",
        "def generate_training_data(num_samples=10000, output_path=\"training_data.json\"):\n",
        "    print(f\"Generating {num_samples} training samples...\")\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"data_generation/generate_training_data.py\", \n",
        "        \"--samples\", str(num_samples), \n",
        "        \"--output\", output_path\n",
        "    ], capture_output=True, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úì Training data generated: {output_path}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Error generating training data:\")\n",
        "        print(result.stderr)\n",
        "        return False\n",
        "\n",
        "# Load Training Data\n",
        "data_path = \"training_data.json\"\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Training data not found at {data_path}\")\n",
        "    print(\"Generating training data...\")\n",
        "    success = generate_training_data(50000, data_path)\n",
        "    if not success:\n",
        "        print(\"Failed to generate training data\")\n",
        "        train_loader = val_loader = None\n",
        "    \n",
        "if os.path.exists(data_path):\n",
        "    try:\n",
        "        dataset = BoidsDataset(data_path)\n",
        "        \n",
        "        # Split into train/val\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        \n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "        \n",
        "        print(f\"‚úì Data loaded:\")\n",
        "        print(f\"  Train samples: {len(train_dataset)}\")\n",
        "        print(f\"  Val samples: {len(val_dataset)}\")\n",
        "        print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        train_loader = val_loader = None\n",
        "else:\n",
        "    train_loader = val_loader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Setup and Functions\n",
        "def setup_training(model):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    print(f\"‚úì Training setup:\")\n",
        "    print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "    print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
        "    print(f\"  Criterion: MSELoss\")\n",
        "    \n",
        "    return optimizer, scheduler, criterion\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        targets = targets.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.6f}')\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(val_loader)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            targets = targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, val_loss, is_best=False):\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    \n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN,\n",
        "            'max_boids': MAX_BOIDS\n",
        "        },\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Save epoch checkpoint\n",
        "    epoch_path = f\"checkpoints/model_epoch_{epoch}.pt\"\n",
        "    torch.save(checkpoint, epoch_path)\n",
        "    print(f\"‚úì Saved checkpoint: {epoch_path}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if is_best:\n",
        "        best_path = \"checkpoints/best_model.pt\"\n",
        "        torch.save(checkpoint, best_path)\n",
        "        print(f\"‚úì Saved best model: {best_path}\")\n",
        "    \n",
        "    return epoch_path\n",
        "\n",
        "# Setup training\n",
        "optimizer, scheduler, criterion = setup_training(model)\n",
        "print(\"‚úì Training components ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=50):\n",
        "    global best_val_loss\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{start_epoch + num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch+1)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss = validate_epoch(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        is_best = val_loss < best_val_loss\n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "            print(f\"üéØ New best validation loss: {best_val_loss:.6f}\")\n",
        "        \n",
        "        # Save every 5 epochs or if best\n",
        "        if (epoch + 1) % 5 == 0 or is_best:\n",
        "            save_checkpoint(model, optimizer, epoch + 1, val_loss, is_best)\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Run training (modify NUM_EPOCHS as needed)\n",
        "if train_loader is not None and val_loader is not None:\n",
        "    NUM_EPOCHS = 20  # Adjust as needed\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, NUM_EPOCHS)\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot start training: data not loaded\")\n",
        "    train_losses = val_losses = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Progress Visualization and Model Evaluation\n",
        "def plot_training_progress(train_losses, val_losses):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Val Loss', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_losses, label='Val Loss', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation MSE Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üìä Training Summary:\")\n",
        "    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"  Final val loss: {val_losses[-1]:.6f}\")\n",
        "    print(f\"  Best val loss: {min(val_losses):.6f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, num_samples=100):\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for inputs, batch_targets in val_loader:\n",
        "            batch_targets = batch_targets.to(DEVICE)\n",
        "            batch_outputs = model(inputs)\n",
        "            \n",
        "            predictions.extend(batch_outputs.cpu().numpy())\n",
        "            targets.extend(batch_targets.cpu().numpy())\n",
        "            \n",
        "            count += len(batch_targets)\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "    \n",
        "    predictions = np.array(predictions[:num_samples])\n",
        "    targets = np.array(targets[:num_samples])\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = np.mean((predictions - targets) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "    \n",
        "    print(f\"üìà Evaluation on {len(predictions)} samples:\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    \n",
        "    # Plot predictions vs targets\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(targets[:, 0], predictions[:, 0], alpha=0.5, color='blue')\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--', label='Perfect prediction')\n",
        "    plt.xlabel('Target X')\n",
        "    plt.ylabel('Predicted X')\n",
        "    plt.title('X Component Prediction')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(targets[:, 1], predictions[:, 1], alpha=0.5, color='green')\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--', label='Perfect prediction')\n",
        "    plt.xlabel('Target Y')\n",
        "    plt.ylabel('Predicted Y')\n",
        "    plt.title('Y Component Prediction')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return mse, mae\n",
        "\n",
        "# Plot training progress and evaluate model\n",
        "if train_losses is not None and val_losses is not None:\n",
        "    plot_training_progress(train_losses, val_losses)\n",
        "\n",
        "if val_loader is not None:\n",
        "    mse, mae = evaluate_model(model, val_loader)\n",
        "else:\n",
        "    print(\"‚ùå Cannot evaluate model: validation data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Export and Testing\n",
        "def export_to_js(model, output_path=\"policy/transformer/models/trained_model.js\"):\n",
        "    print(f\"üîÑ Exporting model to JavaScript format...\")\n",
        "    \n",
        "    # Save PyTorch checkpoint first\n",
        "    checkpoint_path = \"checkpoints/export_checkpoint.pt\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': 'export',\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"‚úì Saved checkpoint for export: {checkpoint_path}\")\n",
        "    \n",
        "    # Use export_to_js.py script\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"export_to_js.py\", \n",
        "        \"--checkpoint\", checkpoint_path, \n",
        "        \"--output\", output_path\n",
        "    ], capture_output=True, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ Model exported to: {output_path}\")\n",
        "        print(\"üéâ You can now use this model in the browser simulation!\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(f\"‚ùå Export failed:\")\n",
        "        print(result.stderr)\n",
        "    \n",
        "    return result.returncode == 0\n",
        "\n",
        "def test_model_inference(model):\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a test input\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "    \n",
        "    print(f\"üß™ Test inference:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Output: [{output[0]:.4f}, {output[1]:.4f}]\")\n",
        "    print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "    print(f\"  ‚úì Output is properly bounded in [-1, 1]\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Test the model\n",
        "test_output = test_model_inference(model)\n",
        "\n",
        "# Export the trained model\n",
        "export_success = export_to_js(model)\n",
        "\n",
        "if export_success:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ Model trained and exported to JavaScript\")\n",
        "    print(\"‚úÖ Ready to use in the browser simulation\")\n",
        "    print(\"‚úÖ Check the playground.html to test your model\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n‚ùå Export failed - model trained but not exported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Architecture Configuration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Architecture constants - modify these as needed\n",
        "D_MODEL = 48\n",
        "N_HEADS = 4\n",
        "N_LAYERS = 2\n",
        "FFN_HIDDEN = 96\n",
        "DROPOUT = 0.1\n",
        "MAX_BOIDS = 50  # Maximum number of boids to handle\n",
        "\n",
        "# Training constants\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Architecture: d_model={D_MODEL}, n_heads={N_HEADS}, n_layers={N_LAYERS}, ffn_hidden={FFN_HIDDEN}\")\n",
        "print(f\"  Training: batch_size={BATCH_SIZE}, lr={LEARNING_RATE}, device={DEVICE}\")\n",
        "print(f\"  Max boids: {MAX_BOIDS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to Python path\n",
        "project_root = Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "from config.constants import CONSTANTS\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Simulation constants loaded: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Model Definition\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return x * torch.nn.functional.gelu(gate)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # GEGLU FFN with separate projections for export compatibility\n",
        "        self.ffn_gate_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_up_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_down_proj = nn.Linear(ffn_hidden, d_model)\n",
        "        \n",
        "    def forward(self, x, padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.self_attn(normed, normed, normed, key_padding_mask=padding_mask)\n",
        "        x = x + attn_out\n",
        "        \n",
        "        # FFN with residual\n",
        "        normed = self.norm2(x)\n",
        "        gate = torch.nn.functional.gelu(self.ffn_gate_proj(normed))\n",
        "        up = self.ffn_up_proj(normed)\n",
        "        ffn_out = self.ffn_down_proj(gate * up)\n",
        "        x = x + ffn_out\n",
        "        \n",
        "        return x\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, d_model=48, n_heads=4, n_layers=2, ffn_hidden=96, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "        \n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "        \n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "        \n",
        "        # Input projections\n",
        "        self.ctx_projection = nn.Linear(2, d_model)  # canvas_width, canvas_height\n",
        "        self.predator_projection = nn.Linear(4, d_model)  # velX, velY, 0, 0 (padded to 4D)\n",
        "        self.boid_projection = nn.Linear(4, d_model)  # relX, relY, velX, velY\n",
        "        \n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)  # predator action [x, y]\n",
        "        \n",
        "    def forward(self, structured_inputs, padding_mask=None):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "        \n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "        \n",
        "        # Build token sequences for each sample in batch\n",
        "        sequences = []\n",
        "        masks = []\n",
        "        \n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "            \n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "            \n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']], \n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "            \n",
        "            # Predator token - expand to 4D\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0], \n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "            \n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]  # CLS, CTX, Predator are not padding\n",
        "            \n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']], \n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "            \n",
        "            # Pad to max_boids + 3 (CLS + CTX + Predator)\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)  # Mark as padding\n",
        "            \n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "        \n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Create padding mask\n",
        "        if padding_mask is None:\n",
        "            padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "        \n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "        \n",
        "        # Extract CLS token and project to output\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        action = self.output_projection(cls_output)  # [batch_size, 2]\n",
        "        \n",
        "        # Apply tanh to ensure [-1, 1] range\n",
        "        action = torch.tanh(action)\n",
        "        \n",
        "        return action.squeeze(0) if batch_size == 1 else action\n",
        "\n",
        "print(\"Model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Class\n",
        "class BoidsDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        print(f\"Loading dataset from {data_path}...\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        self.samples = data['samples']\n",
        "        self.metadata = data['metadata']\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "        print(f\"Dataset metadata: {self.metadata['total_samples']} total samples\")\n",
        "        print(f\"Valid targets: {self.metadata['statistics']['valid_target_percentage']:.1f}%\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Input is the structured format\n",
        "        inputs = sample['input']\n",
        "        \n",
        "        # Output is the action [x, y]\n",
        "        target = torch.tensor(sample['output'], dtype=torch.float32)\n",
        "        \n",
        "        return inputs, target\n",
        "\n",
        "print(\"Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Initialization\n",
        "def create_model():\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"Model created:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Architecture: {D_MODEL}√ó{N_HEADS}√ó{N_LAYERS}√ó{FFN_HIDDEN}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create fresh model\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Model from Checkpoint (optional)\n",
        "def load_checkpoint_and_update_arch(checkpoint_path, optimizer=None):\n",
        "    global D_MODEL, N_HEADS, N_LAYERS, FFN_HIDDEN, MAX_BOIDS, model\n",
        "    \n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    \n",
        "    # Extract and update architecture parameters from checkpoint\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        D_MODEL = arch.get('d_model', D_MODEL)\n",
        "        N_HEADS = arch.get('n_heads', N_HEADS)\n",
        "        N_LAYERS = arch.get('n_layers', N_LAYERS)\n",
        "        FFN_HIDDEN = arch.get('ffn_hidden', FFN_HIDDEN)\n",
        "        MAX_BOIDS = arch.get('max_boids', MAX_BOIDS)\n",
        "        \n",
        "        print(f\"Updated architecture from checkpoint:\")\n",
        "        print(f\"  d_model: {D_MODEL}\")\n",
        "        print(f\"  n_heads: {N_HEADS}\")\n",
        "        print(f\"  n_layers: {N_LAYERS}\")\n",
        "        print(f\"  ffn_hidden: {FFN_HIDDEN}\")\n",
        "        print(f\"  max_boids: {MAX_BOIDS}\")\n",
        "    else:\n",
        "        print(\"Warning: No architecture info in checkpoint, using current values\")\n",
        "    \n",
        "    # Recreate model with correct architecture\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    # Load model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    # Update optimizer if provided\n",
        "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    print(f\"Loaded checkpoint from epoch {epoch}, best val loss: {best_val_loss:.6f}\")\n",
        "    return epoch, best_val_loss, model\n",
        "\n",
        "# Uncomment to load from checkpoint\n",
        "# checkpoint_path = \"checkpoints/best_model.pt\"\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     start_epoch, best_val_loss, model = load_checkpoint_and_update_arch(checkpoint_path)\n",
        "#     print(f\"Model recreated with checkpoint architecture\")\n",
        "# else:\n",
        "#     print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "#     start_epoch, best_val_loss = 0, float('inf')\n",
        "\n",
        "start_epoch, best_val_loss = 0, float('inf')\n",
        "print(f\"Starting from epoch {start_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Training Data\n",
        "data_path = \"training_data.json\"  # Modify path as needed\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    dataset = BoidsDataset(data_path)\n",
        "    \n",
        "    # Split into train/val\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"Data loaded:\")\n",
        "    print(f\"  Train samples: {len(train_dataset)}\")\n",
        "    print(f\"  Val samples: {len(val_dataset)}\")\n",
        "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"Training data not found at {data_path}\")\n",
        "    print(\"Please generate training data first using data_generation/generate_training_data.py\")\n",
        "    train_loader = val_loader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "def setup_training(model):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    print(f\"Training setup:\")\n",
        "    print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "    print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
        "    print(f\"  Criterion: MSELoss\")\n",
        "    \n",
        "    return optimizer, scheduler, criterion\n",
        "\n",
        "optimizer, scheduler, criterion = setup_training(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Functions\n",
        "def train_epoch(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        targets = targets.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.6f}')\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(val_loader)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            targets = targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "print(\"Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Checkpoint Function\n",
        "def save_checkpoint(model, optimizer, epoch, val_loss, is_best=False):\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    \n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN,\n",
        "            'max_boids': MAX_BOIDS\n",
        "        },\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # Save epoch checkpoint\n",
        "    epoch_path = f\"checkpoints/model_epoch_{epoch}.pt\"\n",
        "    torch.save(checkpoint, epoch_path)\n",
        "    print(f\"Saved checkpoint: {epoch_path}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if is_best:\n",
        "        best_path = \"checkpoints/best_model.pt\"\n",
        "        torch.save(checkpoint, best_path)\n",
        "        print(f\"Saved best model: {best_path}\")\n",
        "    \n",
        "    return epoch_path\n",
        "\n",
        "print(\"Checkpoint function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=50):\n",
        "    global best_val_loss\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{start_epoch + num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch+1)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss = validate_epoch(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        is_best = val_loss < best_val_loss\n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "            print(f\"New best validation loss: {best_val_loss:.6f}\")\n",
        "        \n",
        "        # Save every 5 epochs or if best\n",
        "        if (epoch + 1) % 5 == 0 or is_best:\n",
        "            save_checkpoint(model, optimizer, epoch + 1, val_loss, is_best)\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Run training (modify num_epochs as needed)\n",
        "if train_loader is not None and val_loader is not None:\n",
        "    NUM_EPOCHS = 20  # Adjust as needed\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, NUM_EPOCHS)\n",
        "else:\n",
        "    print(\"Cannot start training: data not loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Training Progress\n",
        "def plot_training_progress(train_losses, val_losses):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation MSE Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"Final val loss: {val_losses[-1]:.6f}\")\n",
        "    print(f\"Best val loss: {min(val_losses):.6f}\")\n",
        "\n",
        "# Plot if training was completed\n",
        "if 'train_losses' in locals() and 'val_losses' in locals():\n",
        "    plot_training_progress(train_losses, val_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "def evaluate_model(model, val_loader, num_samples=100):\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for inputs, batch_targets in val_loader:\n",
        "            batch_targets = batch_targets.to(DEVICE)\n",
        "            batch_outputs = model(inputs)\n",
        "            \n",
        "            predictions.extend(batch_outputs.cpu().numpy())\n",
        "            targets.extend(batch_targets.cpu().numpy())\n",
        "            \n",
        "            count += len(batch_targets)\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "    \n",
        "    predictions = np.array(predictions[:num_samples])\n",
        "    targets = np.array(targets[:num_samples])\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = np.mean((predictions - targets) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "    \n",
        "    print(f\"Evaluation on {len(predictions)} samples:\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    \n",
        "    # Plot predictions vs targets\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(targets[:, 0], predictions[:, 0], alpha=0.5)\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--')\n",
        "    plt.xlabel('Target X')\n",
        "    plt.ylabel('Predicted X')\n",
        "    plt.title('X Component')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(targets[:, 1], predictions[:, 1], alpha=0.5)\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--')\n",
        "    plt.xlabel('Target Y')\n",
        "    plt.ylabel('Predicted Y')\n",
        "    plt.title('Y Component')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return mse, mae\n",
        "\n",
        "# Evaluate if data is available\n",
        "if val_loader is not None:\n",
        "    mse, mae = evaluate_model(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Model to JavaScript\n",
        "def export_to_js(model, output_path=\"policy/transformer/models/trained_model.js\"):\n",
        "    print(f\"Exporting model to JavaScript format...\")\n",
        "    \n",
        "    # Save PyTorch checkpoint first\n",
        "    checkpoint_path = \"checkpoints/export_checkpoint.pt\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': 'export',\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Saved checkpoint for export: {checkpoint_path}\")\n",
        "    \n",
        "    # Use export_to_js.py script\n",
        "    os.system(f\"python export_to_js.py --checkpoint {checkpoint_path} --output {output_path}\")\n",
        "    \n",
        "    print(f\"Model exported to: {output_path}\")\n",
        "    print(\"You can now use this model in the browser simulation!\")\n",
        "\n",
        "# Export the trained model\n",
        "export_to_js(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Test of Model\n",
        "def test_model_inference(model):\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a test input\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "    \n",
        "    print(f\"Test inference:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Output: [{output[0]:.4f}, {output[1]:.4f}]\")\n",
        "    print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Test the model\n",
        "test_output = test_model_inference(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Training Data (if needed)\n",
        "def generate_training_data(num_samples=10000, output_path=\"training_data.json\"):\n",
        "    print(f\"Generating {num_samples} training samples...\")\n",
        "    os.system(f\"python data_generation/generate_training_data.py --samples {num_samples} --output {output_path}\")\n",
        "    print(f\"Training data generated: {output_path}\")\n",
        "\n",
        "# Uncomment to generate training data\n",
        "# generate_training_data(50000, \"training_data_50k.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ARCHITECTURE CONFIGURATION ===\n",
        "# Configure your model architecture here\n",
        "ARCH_CONFIG = {\n",
        "    'd_model': 48,        # Model dimension\n",
        "    'n_heads': 4,         # Number of attention heads  \n",
        "    'n_layers': 3,        # Number of transformer layers\n",
        "    'ffn_hidden': 96,     # Feed-forward hidden dimension\n",
        "    'dropout': 0.1,       # Dropout rate\n",
        "    'max_boids': 50,      # Maximum number of boids in sequence\n",
        "}\n",
        "\n",
        "# === TRAINING CONFIGURATION ===\n",
        "TRAIN_CONFIG = {\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_epochs': 100,\n",
        "    'warmup_steps': 1000,\n",
        "    'weight_decay': 1e-5,\n",
        "    'grad_clip': 1.0,\n",
        "    'save_every': 10,     # Save checkpoint every N epochs\n",
        "    'eval_every': 5,      # Evaluate every N epochs\n",
        "}\n",
        "\n",
        "# === PATHS CONFIGURATION ===\n",
        "PATHS = {\n",
        "    'data_file': 'training_data.json',           # Training data file\n",
        "    'checkpoint_dir': 'checkpoints',             # Directory for saving checkpoints\n",
        "    'best_model': 'checkpoints/best_model.pt',   # Best model checkpoint\n",
        "    'latest_model': 'checkpoints/latest_model.pt', # Latest model checkpoint\n",
        "}\n",
        "\n",
        "print(f\"Architecture: {ARCH_CONFIG['d_model']}√ó{ARCH_CONFIG['n_heads']}√ó{ARCH_CONFIG['n_layers']}√ó{ARCH_CONFIG['ffn_hidden']}\")\n",
        "print(f\"Max sequence length: 3 + {ARCH_CONFIG['max_boids']} = {3 + ARCH_CONFIG['max_boids']}\")\n",
        "print(f\"Training config: batch={TRAIN_CONFIG['batch_size']}, lr={TRAIN_CONFIG['learning_rate']}, epochs={TRAIN_CONFIG['num_epochs']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BoidsDataset(Dataset):\n",
        "    def __init__(self, data_file: str, max_boids: int = 50):\n",
        "        print(f\"Loading dataset from {data_file}...\")\n",
        "        \n",
        "        with open(data_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        self.samples = data['samples']\n",
        "        self.metadata = data['metadata']\n",
        "        self.max_boids = max_boids\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples):,} samples\")\n",
        "        print(f\"Metadata: {self.metadata.get('total_samples', 'N/A')} total samples\")\n",
        "        print(f\"Valid targets: {self.metadata.get('statistics', {}).get('valid_target_percentage', 'N/A')}%\")\n",
        "        \n",
        "        # Filter out samples with too many boids\n",
        "        original_count = len(self.samples)\n",
        "        self.samples = [s for s in self.samples if len(s['input']['boids']) <= max_boids]\n",
        "        filtered_count = len(self.samples)\n",
        "        \n",
        "        if filtered_count < original_count:\n",
        "            print(f\"Filtered out {original_count - filtered_count} samples with >{max_boids} boids\")\n",
        "            print(f\"Final dataset size: {filtered_count:,} samples\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Extract structured input\n",
        "        input_data = sample['input']\n",
        "        context = input_data['context']\n",
        "        predator = input_data['predator']\n",
        "        boids = input_data['boids']\n",
        "        \n",
        "        # Extract target output\n",
        "        target = torch.tensor(sample['output'], dtype=torch.float32)\n",
        "        \n",
        "        # Build input sequence: [CLS] + [CTX] + [PREDATOR] + [BOID1, BOID2, ...]\n",
        "        sequence = []\n",
        "        \n",
        "        # CLS token (learnable, will be added by model)\n",
        "        # CTX token\n",
        "        sequence.append([context['canvasWidth'], context['canvasHeight']])\n",
        "        \n",
        "        # PREDATOR token  \n",
        "        sequence.append([predator['velX'], predator['velY']])\n",
        "        \n",
        "        # BOID tokens\n",
        "        for boid in boids:\n",
        "            sequence.append([boid['relX'], boid['relY'], boid['velX'], boid['velY']])\n",
        "        \n",
        "        # Convert to tensor and pad\n",
        "        if len(sequence) == 0:\n",
        "            # Edge case: no context, predator, or boids\n",
        "            sequence = [[0, 0]]  # Dummy context\n",
        "        \n",
        "        # Pad sequence to consistent length for batching\n",
        "        max_seq_len = 2 + self.max_boids  # CTX + PREDATOR + max_boids\n",
        "        while len(sequence) < max_seq_len:\n",
        "            sequence.append([0, 0, 0, 0])  # Pad with dummy boid tokens\n",
        "        \n",
        "        sequence = sequence[:max_seq_len]  # Truncate if too long\n",
        "        \n",
        "        # Create input tensor\n",
        "        input_tensor = torch.zeros(max_seq_len, 4)  # All tokens have 4 features max\n",
        "        for i, token in enumerate(sequence):\n",
        "            input_tensor[i, :len(token)] = torch.tensor(token, dtype=torch.float32)\n",
        "        \n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        attention_mask = torch.zeros(max_seq_len)\n",
        "        actual_seq_len = 2 + len(boids)  # CTX + PREDATOR + actual_boids\n",
        "        attention_mask[:actual_seq_len] = 1\n",
        "        \n",
        "        # Create token type IDs\n",
        "        token_types = torch.zeros(max_seq_len, dtype=torch.long)\n",
        "        token_types[0] = 1  # CTX token\n",
        "        token_types[1] = 2  # PREDATOR token  \n",
        "        token_types[2:2+len(boids)] = 3  # BOID tokens\n",
        "        # Padding tokens remain 0\n",
        "        \n",
        "        return {\n",
        "            'input': input_tensor,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_types': token_types,\n",
        "            'target': target,\n",
        "            'num_boids': len(boids)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, n_layers: int, ffn_hidden: int, \n",
        "                 dropout: float = 0.1, max_boids: int = 50):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.max_seq_len = 2 + max_boids  # CTX + PREDATOR + max_boids\n",
        "        \n",
        "        # Token embeddings (CLS is learnable parameter)\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "        \n",
        "        # Type embeddings for different token types\n",
        "        self.type_embeddings = nn.Embedding(4, d_model)  # 0=PAD, 1=CTX, 2=PREDATOR, 3=BOID\n",
        "        \n",
        "        # Input projections for different token types\n",
        "        self.ctx_projection = nn.Linear(2, d_model)     # Canvas dimensions\n",
        "        self.predator_projection = nn.Linear(2, d_model) # Velocity only (first 2 dims)\n",
        "        self.boid_projection = nn.Linear(4, d_model)     # Relative pos + velocity\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1 + self.max_seq_len, d_model))\n",
        "        \n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ffn_hidden,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_layers = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)\n",
        "        \n",
        "        # Layer norm\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        # Initialize embeddings\n",
        "        nn.init.normal_(self.cls_embedding, std=0.02)\n",
        "        nn.init.normal_(self.type_embeddings.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_encoding, std=0.02)\n",
        "        \n",
        "        # Initialize projections\n",
        "        for module in [self.ctx_projection, self.predator_projection, \n",
        "                      self.boid_projection, self.output_projection]:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, input_tokens, attention_mask, token_types):\n",
        "        batch_size, seq_len, _ = input_tokens.shape\n",
        "        \n",
        "        # Process different token types\n",
        "        embeddings = torch.zeros(batch_size, seq_len, self.d_model, device=input_tokens.device)\n",
        "        \n",
        "        # CTX tokens (type 1)\n",
        "        ctx_mask = (token_types == 1)\n",
        "        if ctx_mask.any():\n",
        "            ctx_tokens = input_tokens[ctx_mask][:, :2]  # First 2 dims\n",
        "            embeddings[ctx_mask] = self.ctx_projection(ctx_tokens)\n",
        "        \n",
        "        # PREDATOR tokens (type 2)\n",
        "        pred_mask = (token_types == 2)\n",
        "        if pred_mask.any():\n",
        "            pred_tokens = input_tokens[pred_mask][:, :2]  # First 2 dims (velocity)\n",
        "            embeddings[pred_mask] = self.predator_projection(pred_tokens)\n",
        "        \n",
        "        # BOID tokens (type 3)\n",
        "        boid_mask = (token_types == 3)\n",
        "        if boid_mask.any():\n",
        "            boid_tokens = input_tokens[boid_mask]  # All 4 dims\n",
        "            embeddings[boid_mask] = self.boid_projection(boid_tokens)\n",
        "        \n",
        "        # Add type embeddings\n",
        "        embeddings = embeddings + self.type_embeddings(token_types)\n",
        "        \n",
        "        # Prepend CLS token\n",
        "        cls_tokens = self.cls_embedding.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)\n",
        "        embeddings = torch.cat([cls_tokens, embeddings], dim=1)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        seq_len_with_cls = embeddings.shape[1]\n",
        "        embeddings = embeddings + self.positional_encoding[:seq_len_with_cls].unsqueeze(0)\n",
        "        \n",
        "        # Apply layer norm and dropout\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        # Create attention mask for CLS + sequence\n",
        "        attention_mask_with_cls = torch.cat([\n",
        "            torch.ones(batch_size, 1, device=attention_mask.device),  # CLS is always attended\n",
        "            attention_mask\n",
        "        ], dim=1)\n",
        "        \n",
        "        # Convert attention mask to transformer format (True = ignore)\n",
        "        transformer_mask = (attention_mask_with_cls == 0)\n",
        "        \n",
        "        # Apply transformer\n",
        "        transformer_output = self.transformer_layers(\n",
        "            embeddings, \n",
        "            src_key_padding_mask=transformer_mask\n",
        "        )\n",
        "        \n",
        "        # Use CLS token output for prediction\n",
        "        cls_output = transformer_output[:, 0]  # First token is CLS\n",
        "        \n",
        "        # Project to action space\n",
        "        action_logits = self.output_projection(cls_output)\n",
        "        \n",
        "        # Apply tanh to ensure output is in [-1, 1] range\n",
        "        action = torch.tanh(action_logits)\n",
        "        \n",
        "        return action\n",
        "\n",
        "print(f\"Model will have {3 + ARCH_CONFIG['max_boids']} max sequence length (CLS + CTX + PREDATOR + {ARCH_CONFIG['max_boids']} boids)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
