{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIJaceDEmv3i",
        "outputId": "794d5294-a535-4370-9f9d-97d6be34b1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository from https://github.com/yimianxyz/homepage.git (branch: neuro-predator)...\n",
            "‚úÖ Repository cloned successfully!\n",
            "‚úÖ All key files found!\n",
            "üìÅ Working directory: /content/homepage\n",
            "\n",
            "üéâ Setup complete! You can now run the training notebook.\n",
            "üìñ Repository structure:\n",
            "./\n",
            "  LICENSE\n",
            "  export_to_js.py\n",
            "  transformer_training.ipynb\n",
            "  playground.css\n",
            "  index.html\n",
            "  styles.css\n",
            "  shared.css\n",
            "  playground.html\n",
            "  data_generation/\n",
            "    generate_training_data.py\n",
            "  simulation/\n",
            "    __init__.py\n",
            "    random_state_generator/\n"
          ]
        }
      ],
      "source": [
        "# Download Codebase from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Repository information\n",
        "REPO_URL = \"https://github.com/yimianxyz/homepage.git\"\n",
        "BRANCH = \"neuro-predator\"\n",
        "REPO_DIR = \"homepage\"\n",
        "\n",
        "def download_codebase():\n",
        "    \"\"\"Download the codebase from GitHub if not already present\"\"\"\n",
        "\n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(f\"Repository directory '{REPO_DIR}' already exists.\")\n",
        "\n",
        "        # Check if it's the correct repository and branch\n",
        "        try:\n",
        "            os.chdir(REPO_DIR)\n",
        "\n",
        "            # Check current branch\n",
        "            result = subprocess.run(['git', 'branch', '--show-current'],\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            current_branch = result.stdout.strip()\n",
        "\n",
        "            if current_branch != BRANCH:\n",
        "                print(f\"Switching to branch '{BRANCH}'...\")\n",
        "                subprocess.run(['git', 'checkout', BRANCH], check=True)\n",
        "\n",
        "            # Pull latest changes\n",
        "            print(\"Updating repository...\")\n",
        "            subprocess.run(['git', 'pull', 'origin', BRANCH], check=True)\n",
        "\n",
        "            print(f\"‚úÖ Repository updated successfully!\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error updating repository: {e}\")\n",
        "            print(\"Repository directory exists but may not be a valid git repository.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "\n",
        "        try:\n",
        "            # Clone the specific branch\n",
        "            subprocess.run(['git', 'clone', '-b', BRANCH, REPO_URL, REPO_DIR], check=True)\n",
        "\n",
        "            print(f\"‚úÖ Repository cloned successfully!\")\n",
        "\n",
        "            # Change to repository directory\n",
        "            os.chdir(REPO_DIR)\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error cloning repository: {e}\")\n",
        "            print(\"Make sure you have git installed and internet connection.\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Verify key files exist\n",
        "    key_files = [\n",
        "        'config/constants.py',\n",
        "        'data_generation/generate_training_data.py',\n",
        "        'export_to_js.py',\n",
        "        'policy/transformer/transformer_policy.js',\n",
        "        'simulation/processors/input_processor.py'\n",
        "    ]\n",
        "\n",
        "    missing_files = []\n",
        "    for file_path in key_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_path)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Some key files are missing:\")\n",
        "        for file_path in missing_files:\n",
        "            print(f\"  - {file_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"‚úÖ All key files found!\")\n",
        "    print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Download the codebase\n",
        "success = download_codebase()\n",
        "\n",
        "if success:\n",
        "    print(\"\\nüéâ Setup complete! You can now run the training notebook.\")\n",
        "    print(\"üìñ Repository structure:\")\n",
        "\n",
        "    # Show repository structure\n",
        "    for root, dirs, files in os.walk('.'):\n",
        "        # Skip hidden directories and __pycache__\n",
        "        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n",
        "        level = root.replace('.', '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "        # Only show first level of files to avoid clutter\n",
        "        if level < 2:\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files:\n",
        "                if not file.startswith('.') and not file.endswith('.pyc'):\n",
        "                    print(f\"{subindent}{file}\")\n",
        "\n",
        "        # Limit depth to avoid too much output\n",
        "        if level >= 2:\n",
        "            break\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Setup failed. Please check the errors above and try again.\")\n",
        "    print(\"Manual setup: git clone -b neuro-predator https://github.com/yimianxyz/homepage.git\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzoYtbxPmv3k",
        "outputId": "81ef995f-5ec7-49c4-8209-020e95b202f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully imported simulation constants\n",
            "üìÅ Project root: /content/homepage\n",
            "üîß Key constants: MAX_DISTANCE=2000, BOID_MAX_SPEED=3.5\n"
          ]
        }
      ],
      "source": [
        "# Verify Setup and Import Project Modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure we're in the correct directory and add to Python path\n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'homepage':\n",
        "    print(f\"‚ö†Ô∏è  Warning: Current directory is '{project_root.name}', expected 'homepage'\")\n",
        "    print(\"Make sure the first cell downloaded the repository correctly.\")\n",
        "\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "try:\n",
        "    from config.constants import CONSTANTS\n",
        "    print(f\"‚úÖ Successfully imported simulation constants\")\n",
        "    print(f\"üìÅ Project root: {project_root}\")\n",
        "    print(f\"üîß Key constants: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}, BOID_MAX_SPEED={CONSTANTS.BOID_MAX_SPEED}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import constants: {e}\")\n",
        "    print(\"Make sure the repository was downloaded correctly in the first cell.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlzaOaSQmv3l",
        "outputId": "3f238564-28fb-4781-dd9d-20d9e1504c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Architecture: d_model=128, n_heads=8, n_layers=4, ffn_hidden=512\n",
            "  Training: batch_size=256, lr=0.0001, device=cuda\n",
            "  Max boids: 50\n",
            "‚úì Simulation constants loaded: MAX_DISTANCE=2000\n"
          ]
        }
      ],
      "source": [
        "# Architecture Configuration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Dict, List, Any, Tuple\n",
        "\n",
        "# Architecture constants - modify these as needed\n",
        "D_MODEL = 128\n",
        "N_HEADS = 8\n",
        "N_LAYERS = 4\n",
        "FFN_HIDDEN = 512\n",
        "DROPOUT = 0.1\n",
        "MAX_BOIDS = 50  # Maximum number of boids to handle\n",
        "\n",
        "# Training constants\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Architecture: d_model={D_MODEL}, n_heads={N_HEADS}, n_layers={N_LAYERS}, ffn_hidden={FFN_HIDDEN}\")\n",
        "print(f\"  Training: batch_size={BATCH_SIZE}, lr={LEARNING_RATE}, device={DEVICE}\")\n",
        "print(f\"  Max boids: {MAX_BOIDS}\")\n",
        "\n",
        "# Import project modules\n",
        "from config.constants import CONSTANTS\n",
        "print(f\"‚úì Simulation constants loaded: MAX_DISTANCE={CONSTANTS.MAX_DISTANCE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZUTXzzbmv3l",
        "outputId": "9ae5a63d-d80c-4b90-c88d-4a563191cfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model architecture defined\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model Definition\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return x * torch.nn.functional.gelu(gate)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ffn_hidden, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # GEGLU FFN with separate projections for export compatibility\n",
        "        self.ffn_gate_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_up_proj = nn.Linear(d_model, ffn_hidden)\n",
        "        self.ffn_down_proj = nn.Linear(ffn_hidden, d_model)\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        normed = self.norm1(x)\n",
        "        attn_out, _ = self.self_attn(normed, normed, normed, key_padding_mask=padding_mask)\n",
        "        x = x + attn_out\n",
        "\n",
        "        # FFN with residual\n",
        "        normed = self.norm2(x)\n",
        "        gate = torch.nn.functional.gelu(self.ffn_gate_proj(normed))\n",
        "        up = self.ffn_up_proj(normed)\n",
        "        ffn_out = self.ffn_down_proj(gate * up)\n",
        "        x = x + ffn_out\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, d_model=48, n_heads=4, n_layers=2, ffn_hidden=96, max_boids=50, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.ffn_hidden = ffn_hidden\n",
        "        self.max_boids = max_boids\n",
        "\n",
        "        # CLS token embedding\n",
        "        self.cls_embedding = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "        # Type embeddings\n",
        "        self.type_embeddings = nn.ParameterDict({\n",
        "            'cls': nn.Parameter(torch.randn(d_model)),\n",
        "            'ctx': nn.Parameter(torch.randn(d_model)),\n",
        "            'predator': nn.Parameter(torch.randn(d_model)),\n",
        "            'boid': nn.Parameter(torch.randn(d_model))\n",
        "        })\n",
        "\n",
        "        # Input projections\n",
        "        self.ctx_projection = nn.Linear(2, d_model)  # canvas_width, canvas_height\n",
        "        self.predator_projection = nn.Linear(4, d_model)  # velX, velY, 0, 0 (padded to 4D)\n",
        "        self.boid_projection = nn.Linear(4, d_model)  # relX, relY, velX, velY\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, n_heads, ffn_hidden, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, 2)  # predator action [x, y]\n",
        "\n",
        "    def forward(self, structured_inputs, padding_mask=None):\n",
        "        batch_size = len(structured_inputs) if isinstance(structured_inputs, list) else 1\n",
        "\n",
        "        # Handle single sample vs batch\n",
        "        if isinstance(structured_inputs, dict):\n",
        "            structured_inputs = [structured_inputs]\n",
        "            batch_size = 1\n",
        "\n",
        "        # Build token sequences for each sample in batch\n",
        "        sequences = []\n",
        "        masks = []\n",
        "\n",
        "        for sample in structured_inputs:\n",
        "            tokens = []\n",
        "\n",
        "            # CLS token\n",
        "            cls_token = self.cls_embedding + self.type_embeddings['cls']\n",
        "            tokens.append(cls_token)\n",
        "\n",
        "            # Context token\n",
        "            ctx_input = torch.tensor([sample['context']['canvasWidth'], sample['context']['canvasHeight']],\n",
        "                                   dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            ctx_token = self.ctx_projection(ctx_input) + self.type_embeddings['ctx']\n",
        "            tokens.append(ctx_token)\n",
        "\n",
        "            # Predator token - expand to 4D\n",
        "            predator_input = torch.tensor([sample['predator']['velX'], sample['predator']['velY'], 0.0, 0.0],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "            predator_token = self.predator_projection(predator_input) + self.type_embeddings['predator']\n",
        "            tokens.append(predator_token)\n",
        "\n",
        "            # Boid tokens\n",
        "            sample_mask = [False, False, False]  # CLS, CTX, Predator are not padding\n",
        "\n",
        "            for boid in sample['boids']:\n",
        "                boid_input = torch.tensor([boid['relX'], boid['relY'], boid['velX'], boid['velY']],\n",
        "                                        dtype=torch.float32, device=self.cls_embedding.device)\n",
        "                boid_token = self.boid_projection(boid_input) + self.type_embeddings['boid']\n",
        "                tokens.append(boid_token)\n",
        "                sample_mask.append(False)\n",
        "\n",
        "            # Pad to max_boids + 3 (CLS + CTX + Predator)\n",
        "            while len(tokens) < self.max_boids + 3:\n",
        "                padding_token = torch.zeros(self.d_model, device=self.cls_embedding.device)\n",
        "                tokens.append(padding_token)\n",
        "                sample_mask.append(True)  # Mark as padding\n",
        "\n",
        "            sequences.append(torch.stack(tokens))\n",
        "            masks.append(sample_mask)\n",
        "\n",
        "        # Stack sequences\n",
        "        x = torch.stack(sequences)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Create padding mask\n",
        "        if padding_mask is None:\n",
        "            padding_mask = torch.tensor(masks, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, padding_mask)\n",
        "\n",
        "        # Extract CLS token and project to output\n",
        "        cls_output = x[:, 0]  # [batch_size, d_model]\n",
        "        action = self.output_projection(cls_output)  # [batch_size, 2]\n",
        "\n",
        "        # Apply tanh to ensure [-1, 1] range\n",
        "        action = torch.tanh(action)\n",
        "\n",
        "        return action.squeeze(0) if batch_size == 1 else action\n",
        "\n",
        "print(\"‚úì Model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiCGQt3Kmv3l",
        "outputId": "02433dc7-22f1-486d-b9a1-e030dd6dbbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "# Dataset Class\n",
        "class BoidsDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        print(f\"Loading dataset from {data_path}...\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.samples = data['samples']\n",
        "        self.metadata = data['metadata']\n",
        "\n",
        "        print(f\"‚úì Loaded {len(self.samples)} samples\")\n",
        "        print(f\"  Dataset metadata: {self.metadata['total_samples']} total samples\")\n",
        "        print(f\"  Valid targets: {self.metadata['statistics']['valid_target_percentage']:.1f}%\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Input is the structured format\n",
        "        inputs = sample['input']\n",
        "\n",
        "        # Output is the action [x, y]\n",
        "        target = torch.tensor(sample['output'], dtype=torch.float32)\n",
        "\n",
        "        return inputs, target\n",
        "\n",
        "print(\"‚úì Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7stEGQWmv3m",
        "outputId": "efebdf4d-4ecb-473a-f4e8-94b6b9da8a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model created:\n",
            "  Total parameters: 1,059,842\n",
            "  Trainable parameters: 1,059,842\n",
            "  Architecture: 128√ó8√ó4√ó512\n"
          ]
        }
      ],
      "source": [
        "# Model Initialization\n",
        "def create_model():\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"‚úì Model created:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Architecture: {D_MODEL}√ó{N_HEADS}√ó{N_LAYERS}√ó{FFN_HIDDEN}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create fresh model\n",
        "model = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDJNKwpFmv3m",
        "outputId": "d853614f-26ae-42db-8d08-3709e5ee8629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Starting from epoch 0\n"
          ]
        }
      ],
      "source": [
        "# Load Model from Checkpoint (optional)\n",
        "def load_checkpoint_and_update_arch(checkpoint_path, optimizer=None):\n",
        "    global D_MODEL, N_HEADS, N_LAYERS, FFN_HIDDEN, MAX_BOIDS, model\n",
        "\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    # Extract and update architecture parameters from checkpoint\n",
        "    if 'architecture' in checkpoint:\n",
        "        arch = checkpoint['architecture']\n",
        "        D_MODEL = arch.get('d_model', D_MODEL)\n",
        "        N_HEADS = arch.get('n_heads', N_HEADS)\n",
        "        N_LAYERS = arch.get('n_layers', N_LAYERS)\n",
        "        FFN_HIDDEN = arch.get('ffn_hidden', FFN_HIDDEN)\n",
        "        MAX_BOIDS = arch.get('max_boids', MAX_BOIDS)\n",
        "\n",
        "        print(f\"‚úì Updated architecture from checkpoint:\")\n",
        "        print(f\"  d_model: {D_MODEL}\")\n",
        "        print(f\"  n_heads: {N_HEADS}\")\n",
        "        print(f\"  n_layers: {N_LAYERS}\")\n",
        "        print(f\"  ffn_hidden: {FFN_HIDDEN}\")\n",
        "        print(f\"  max_boids: {MAX_BOIDS}\")\n",
        "    else:\n",
        "        print(\"Warning: No architecture info in checkpoint, using current values\")\n",
        "\n",
        "    # Recreate model with correct architecture\n",
        "    model = TransformerPredictor(\n",
        "        d_model=D_MODEL,\n",
        "        n_heads=N_HEADS,\n",
        "        n_layers=N_LAYERS,\n",
        "        ffn_hidden=FFN_HIDDEN,\n",
        "        max_boids=MAX_BOIDS,\n",
        "        dropout=DROPOUT\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Load model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Update optimizer if provided\n",
        "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "\n",
        "    print(f\"‚úì Loaded checkpoint from epoch {epoch}, best val loss: {best_val_loss:.6f}\")\n",
        "    return epoch, best_val_loss, model\n",
        "\n",
        "# Uncomment to load from checkpoint\n",
        "# checkpoint_path = \"checkpoints/best_model.pt\"\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     start_epoch, best_val_loss, model = load_checkpoint_and_update_arch(checkpoint_path)\n",
        "#     print(f\"‚úì Model recreated with checkpoint architecture\")\n",
        "# else:\n",
        "#     print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "#     start_epoch, best_val_loss = 0, float('inf')\n",
        "\n",
        "start_epoch, best_val_loss = 0, float('inf')\n",
        "print(f\"‚úì Starting from epoch {start_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZo5vugNmv3m",
        "outputId": "f2f5e884-29ba-4aab-df73-9665e127b8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data not found at training_data.json\n",
            "Generating training data...\n",
            "Generating 1000000 training samples...\n",
            "‚úì Training data generated: training_data.json\n",
            "Loading dataset from training_data.json...\n",
            "‚úì Loaded 1000000 samples\n",
            "  Dataset metadata: 1000000 total samples\n",
            "  Valid targets: 100.0%\n",
            "‚úì Data loaded:\n",
            "  Train samples: 800000\n",
            "  Val samples: 200000\n",
            "  Batch size: 256\n"
          ]
        }
      ],
      "source": [
        "# Generate Training Data (if needed)\n",
        "def generate_training_data(num_samples=10000, output_path=\"training_data.json\"):\n",
        "    print(f\"Generating {num_samples} training samples...\")\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"data_generation/generate_training_data.py\",\n",
        "        \"--samples\", str(num_samples),\n",
        "        \"--output\", output_path\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úì Training data generated: {output_path}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Error generating training data:\")\n",
        "        print(result.stderr)\n",
        "        return False\n",
        "\n",
        "# Load Training Data\n",
        "data_path = \"training_data.json\"\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Training data not found at {data_path}\")\n",
        "    print(\"Generating training data...\")\n",
        "    success = generate_training_data(1000000, data_path)\n",
        "    if not success:\n",
        "        print(\"Failed to generate training data\")\n",
        "        train_loader = val_loader = None\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    try:\n",
        "        dataset = BoidsDataset(data_path)\n",
        "\n",
        "        # Split into train/val\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "        print(f\"‚úì Data loaded:\")\n",
        "        print(f\"  Train samples: {len(train_dataset)}\")\n",
        "        print(f\"  Val samples: {len(val_dataset)}\")\n",
        "        print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data: {e}\")\n",
        "        train_loader = val_loader = None\n",
        "else:\n",
        "    train_loader = val_loader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO5o-9eBmv3m",
        "outputId": "119a9a04-db92-4057-b813-2e24f72ef9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Training setup:\n",
            "  Optimizer: AdamW (lr=0.0001, weight_decay=0.01)\n",
            "  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\n",
            "  Criterion: MSELoss\n",
            "‚úì Training components ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Training Setup and Functions\n",
        "def setup_training(model):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    print(f\"‚úì Training setup:\")\n",
        "    print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "    print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
        "    print(f\"  Criterion: MSELoss\")\n",
        "\n",
        "    return optimizer, scheduler, criterion\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.6f}')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            targets = targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, val_loss, is_best=False):\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN,\n",
        "            'max_boids': MAX_BOIDS\n",
        "        },\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Save epoch checkpoint\n",
        "    epoch_path = f\"checkpoints/model_epoch_{epoch}.pt\"\n",
        "    torch.save(checkpoint, epoch_path)\n",
        "    print(f\"‚úì Saved checkpoint: {epoch_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    if is_best:\n",
        "        best_path = \"checkpoints/best_model.pt\"\n",
        "        torch.save(checkpoint, best_path)\n",
        "        print(f\"‚úì Saved best model: {best_path}\")\n",
        "\n",
        "    return epoch_path\n",
        "\n",
        "# Setup training\n",
        "optimizer, scheduler, criterion = setup_training(model)\n",
        "print(\"‚úì Training components ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "830ebmO9mv3m",
        "outputId": "6a49140b-93fc-4cae-950d-e54b04f5fbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from training_data.json...\n",
            "‚úì Loaded 1000000 samples\n",
            "  Dataset metadata: 1000000 total samples\n",
            "  Valid targets: 100.0%\n",
            "Data loaded:\n",
            "  Train samples: 800000\n",
            "  Val samples: 200000\n",
            "  Batch size: 256\n",
            "  Custom collate function: ‚úì\n",
            "üöÄ Starting training for 20 epochs...\n",
            "\n",
            "Epoch 1/20\n",
            "--------------------------------------------------\n",
            "Epoch 1, Batch 0/3125, Loss: 0.949517\n",
            "Epoch 1, Batch 100/3125, Loss: 0.495001\n",
            "Epoch 1, Batch 200/3125, Loss: 0.526856\n",
            "Epoch 1, Batch 300/3125, Loss: 0.496496\n",
            "Epoch 1, Batch 400/3125, Loss: 0.482994\n",
            "Epoch 1, Batch 500/3125, Loss: 0.496084\n",
            "Epoch 1, Batch 600/3125, Loss: 0.496468\n",
            "Epoch 1, Batch 700/3125, Loss: 0.504417\n",
            "Epoch 1, Batch 800/3125, Loss: 0.488186\n",
            "Epoch 1, Batch 900/3125, Loss: 0.492478\n",
            "Epoch 1, Batch 1000/3125, Loss: 0.389856\n",
            "Epoch 1, Batch 1100/3125, Loss: 0.160098\n",
            "Epoch 1, Batch 1200/3125, Loss: 0.155775\n",
            "Epoch 1, Batch 1300/3125, Loss: 0.103763\n",
            "Epoch 1, Batch 1400/3125, Loss: 0.081833\n",
            "Epoch 1, Batch 1500/3125, Loss: 0.071068\n",
            "Epoch 1, Batch 1600/3125, Loss: 0.067944\n",
            "Epoch 1, Batch 1700/3125, Loss: 0.066367\n",
            "Epoch 1, Batch 1800/3125, Loss: 0.051043\n",
            "Epoch 1, Batch 1900/3125, Loss: 0.038764\n",
            "Epoch 1, Batch 2000/3125, Loss: 0.120861\n",
            "Epoch 1, Batch 2100/3125, Loss: 0.089190\n",
            "Epoch 1, Batch 2200/3125, Loss: 0.081514\n",
            "Epoch 1, Batch 2300/3125, Loss: 0.113384\n",
            "Epoch 1, Batch 2400/3125, Loss: 0.048662\n",
            "Epoch 1, Batch 2500/3125, Loss: 0.080424\n",
            "Epoch 1, Batch 2600/3125, Loss: 0.066259\n",
            "Epoch 1, Batch 2700/3125, Loss: 0.083571\n",
            "Epoch 1, Batch 2800/3125, Loss: 0.043363\n",
            "Epoch 1, Batch 2900/3125, Loss: 0.078650\n",
            "Epoch 1, Batch 3000/3125, Loss: 0.043429\n",
            "Epoch 1, Batch 3100/3125, Loss: 0.080271\n",
            "Train Loss: 0.222123, Val Loss: 0.058573\n",
            "üéØ New best validation loss: 0.058573\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_1.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 2/20\n",
            "--------------------------------------------------\n",
            "Epoch 2, Batch 0/3125, Loss: 0.058426\n",
            "Epoch 2, Batch 100/3125, Loss: 0.056784\n",
            "Epoch 2, Batch 200/3125, Loss: 0.078757\n",
            "Epoch 2, Batch 300/3125, Loss: 0.129968\n",
            "Epoch 2, Batch 400/3125, Loss: 0.103547\n",
            "Epoch 2, Batch 500/3125, Loss: 0.077395\n",
            "Epoch 2, Batch 600/3125, Loss: 0.042207\n",
            "Epoch 2, Batch 700/3125, Loss: 0.044008\n",
            "Epoch 2, Batch 800/3125, Loss: 0.087438\n",
            "Epoch 2, Batch 900/3125, Loss: 0.050776\n",
            "Epoch 2, Batch 1000/3125, Loss: 0.031529\n",
            "Epoch 2, Batch 1100/3125, Loss: 0.105783\n",
            "Epoch 2, Batch 1200/3125, Loss: 0.058674\n",
            "Epoch 2, Batch 1300/3125, Loss: 0.060004\n",
            "Epoch 2, Batch 1400/3125, Loss: 0.025889\n",
            "Epoch 2, Batch 1500/3125, Loss: 0.043343\n",
            "Epoch 2, Batch 1600/3125, Loss: 0.027632\n",
            "Epoch 2, Batch 1700/3125, Loss: 0.048198\n",
            "Epoch 2, Batch 1800/3125, Loss: 0.048107\n",
            "Epoch 2, Batch 1900/3125, Loss: 0.104558\n",
            "Epoch 2, Batch 2000/3125, Loss: 0.023633\n",
            "Epoch 2, Batch 2100/3125, Loss: 0.030550\n",
            "Epoch 2, Batch 2200/3125, Loss: 0.045285\n",
            "Epoch 2, Batch 2300/3125, Loss: 0.044829\n",
            "Epoch 2, Batch 2400/3125, Loss: 0.034032\n",
            "Epoch 2, Batch 2500/3125, Loss: 0.037512\n",
            "Epoch 2, Batch 2600/3125, Loss: 0.045055\n",
            "Epoch 2, Batch 2700/3125, Loss: 0.061495\n",
            "Epoch 2, Batch 2800/3125, Loss: 0.025971\n",
            "Epoch 2, Batch 2900/3125, Loss: 0.055265\n",
            "Epoch 2, Batch 3000/3125, Loss: 0.072290\n",
            "Epoch 2, Batch 3100/3125, Loss: 0.095331\n",
            "Train Loss: 0.050621, Val Loss: 0.038041\n",
            "üéØ New best validation loss: 0.038041\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_2.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 3/20\n",
            "--------------------------------------------------\n",
            "Epoch 3, Batch 0/3125, Loss: 0.040152\n",
            "Epoch 3, Batch 100/3125, Loss: 0.047185\n",
            "Epoch 3, Batch 200/3125, Loss: 0.061252\n",
            "Epoch 3, Batch 300/3125, Loss: 0.060327\n",
            "Epoch 3, Batch 400/3125, Loss: 0.057377\n",
            "Epoch 3, Batch 500/3125, Loss: 0.072252\n",
            "Epoch 3, Batch 600/3125, Loss: 0.020373\n",
            "Epoch 3, Batch 700/3125, Loss: 0.039889\n",
            "Epoch 3, Batch 800/3125, Loss: 0.088439\n",
            "Epoch 3, Batch 900/3125, Loss: 0.033683\n",
            "Epoch 3, Batch 1000/3125, Loss: 0.079012\n",
            "Epoch 3, Batch 1100/3125, Loss: 0.015756\n",
            "Epoch 3, Batch 1200/3125, Loss: 0.020048\n",
            "Epoch 3, Batch 1300/3125, Loss: 0.030248\n",
            "Epoch 3, Batch 1400/3125, Loss: 0.043898\n",
            "Epoch 3, Batch 1500/3125, Loss: 0.029543\n",
            "Epoch 3, Batch 1600/3125, Loss: 0.058820\n",
            "Epoch 3, Batch 1700/3125, Loss: 0.024672\n",
            "Epoch 3, Batch 1800/3125, Loss: 0.018654\n",
            "Epoch 3, Batch 1900/3125, Loss: 0.033939\n",
            "Epoch 3, Batch 2000/3125, Loss: 0.077094\n",
            "Epoch 3, Batch 2100/3125, Loss: 0.017348\n",
            "Epoch 3, Batch 2200/3125, Loss: 0.049799\n",
            "Epoch 3, Batch 2300/3125, Loss: 0.020878\n",
            "Epoch 3, Batch 2400/3125, Loss: 0.027479\n",
            "Epoch 3, Batch 2500/3125, Loss: 0.038067\n",
            "Epoch 3, Batch 2600/3125, Loss: 0.046681\n",
            "Epoch 3, Batch 2700/3125, Loss: 0.041060\n",
            "Epoch 3, Batch 2800/3125, Loss: 0.033415\n",
            "Epoch 3, Batch 2900/3125, Loss: 0.033470\n",
            "Epoch 3, Batch 3000/3125, Loss: 0.040875\n",
            "Epoch 3, Batch 3100/3125, Loss: 0.028146\n",
            "Train Loss: 0.039606, Val Loss: 0.031418\n",
            "üéØ New best validation loss: 0.031418\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_3.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 4/20\n",
            "--------------------------------------------------\n",
            "Epoch 4, Batch 0/3125, Loss: 0.037921\n",
            "Epoch 4, Batch 100/3125, Loss: 0.031843\n",
            "Epoch 4, Batch 200/3125, Loss: 0.030357\n",
            "Epoch 4, Batch 300/3125, Loss: 0.060145\n",
            "Epoch 4, Batch 400/3125, Loss: 0.080247\n",
            "Epoch 4, Batch 500/3125, Loss: 0.033791\n",
            "Epoch 4, Batch 600/3125, Loss: 0.022505\n",
            "Epoch 4, Batch 700/3125, Loss: 0.034957\n",
            "Epoch 4, Batch 800/3125, Loss: 0.025821\n",
            "Epoch 4, Batch 900/3125, Loss: 0.036693\n",
            "Epoch 4, Batch 1000/3125, Loss: 0.037358\n",
            "Epoch 4, Batch 1100/3125, Loss: 0.040443\n",
            "Epoch 4, Batch 1200/3125, Loss: 0.040163\n",
            "Epoch 4, Batch 1300/3125, Loss: 0.018275\n",
            "Epoch 4, Batch 1400/3125, Loss: 0.034382\n",
            "Epoch 4, Batch 1500/3125, Loss: 0.010208\n",
            "Epoch 4, Batch 1600/3125, Loss: 0.101704\n",
            "Epoch 4, Batch 1700/3125, Loss: 0.033660\n",
            "Epoch 4, Batch 1800/3125, Loss: 0.074205\n",
            "Epoch 4, Batch 1900/3125, Loss: 0.020827\n",
            "Epoch 4, Batch 2000/3125, Loss: 0.051861\n",
            "Epoch 4, Batch 2100/3125, Loss: 0.027505\n",
            "Epoch 4, Batch 2200/3125, Loss: 0.043237\n",
            "Epoch 4, Batch 2300/3125, Loss: 0.015746\n",
            "Epoch 4, Batch 2400/3125, Loss: 0.022928\n",
            "Epoch 4, Batch 2500/3125, Loss: 0.016155\n",
            "Epoch 4, Batch 2600/3125, Loss: 0.043813\n",
            "Epoch 4, Batch 2700/3125, Loss: 0.032488\n",
            "Epoch 4, Batch 2800/3125, Loss: 0.027635\n",
            "Epoch 4, Batch 2900/3125, Loss: 0.047442\n",
            "Epoch 4, Batch 3000/3125, Loss: 0.035396\n",
            "Epoch 4, Batch 3100/3125, Loss: 0.021076\n",
            "Train Loss: 0.035605, Val Loss: 0.022057\n",
            "üéØ New best validation loss: 0.022057\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_4.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 5/20\n",
            "--------------------------------------------------\n",
            "Epoch 5, Batch 0/3125, Loss: 0.012320\n",
            "Epoch 5, Batch 100/3125, Loss: 0.037316\n",
            "Epoch 5, Batch 200/3125, Loss: 0.028232\n",
            "Epoch 5, Batch 300/3125, Loss: 0.024666\n",
            "Epoch 5, Batch 400/3125, Loss: 0.066968\n",
            "Epoch 5, Batch 500/3125, Loss: 0.028618\n",
            "Epoch 5, Batch 600/3125, Loss: 0.030727\n",
            "Epoch 5, Batch 700/3125, Loss: 0.017385\n",
            "Epoch 5, Batch 800/3125, Loss: 0.043676\n",
            "Epoch 5, Batch 900/3125, Loss: 0.064890\n",
            "Epoch 5, Batch 1000/3125, Loss: 0.038274\n",
            "Epoch 5, Batch 1100/3125, Loss: 0.023033\n",
            "Epoch 5, Batch 1200/3125, Loss: 0.029137\n",
            "Epoch 5, Batch 1300/3125, Loss: 0.044232\n",
            "Epoch 5, Batch 1400/3125, Loss: 0.036847\n",
            "Epoch 5, Batch 1500/3125, Loss: 0.013842\n",
            "Epoch 5, Batch 1600/3125, Loss: 0.036576\n",
            "Epoch 5, Batch 1700/3125, Loss: 0.030197\n",
            "Epoch 5, Batch 1800/3125, Loss: 0.034841\n",
            "Epoch 5, Batch 1900/3125, Loss: 0.022233\n",
            "Epoch 5, Batch 2000/3125, Loss: 0.020519\n",
            "Epoch 5, Batch 2100/3125, Loss: 0.025498\n",
            "Epoch 5, Batch 2200/3125, Loss: 0.023537\n",
            "Epoch 5, Batch 2300/3125, Loss: 0.030870\n",
            "Epoch 5, Batch 2400/3125, Loss: 0.038568\n",
            "Epoch 5, Batch 2500/3125, Loss: 0.040532\n",
            "Epoch 5, Batch 2600/3125, Loss: 0.041929\n",
            "Epoch 5, Batch 2700/3125, Loss: 0.011739\n",
            "Epoch 5, Batch 2800/3125, Loss: 0.014247\n",
            "Epoch 5, Batch 2900/3125, Loss: 0.049803\n",
            "Epoch 5, Batch 3000/3125, Loss: 0.015548\n",
            "Epoch 5, Batch 3100/3125, Loss: 0.050517\n",
            "Train Loss: 0.031245, Val Loss: 0.063851\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_5.pt\n",
            "\n",
            "Epoch 6/20\n",
            "--------------------------------------------------\n",
            "Epoch 6, Batch 0/3125, Loss: 0.066440\n",
            "Epoch 6, Batch 100/3125, Loss: 0.018503\n",
            "Epoch 6, Batch 200/3125, Loss: 0.052100\n",
            "Epoch 6, Batch 300/3125, Loss: 0.039068\n",
            "Epoch 6, Batch 400/3125, Loss: 0.035369\n",
            "Epoch 6, Batch 500/3125, Loss: 0.047597\n",
            "Epoch 6, Batch 600/3125, Loss: 0.012118\n",
            "Epoch 6, Batch 700/3125, Loss: 0.014348\n",
            "Epoch 6, Batch 800/3125, Loss: 0.017051\n",
            "Epoch 6, Batch 900/3125, Loss: 0.021257\n",
            "Epoch 6, Batch 1000/3125, Loss: 0.020401\n",
            "Epoch 6, Batch 1100/3125, Loss: 0.025900\n",
            "Epoch 6, Batch 1200/3125, Loss: 0.022826\n",
            "Epoch 6, Batch 1300/3125, Loss: 0.046633\n",
            "Epoch 6, Batch 1400/3125, Loss: 0.017501\n",
            "Epoch 6, Batch 1500/3125, Loss: 0.017342\n",
            "Epoch 6, Batch 1600/3125, Loss: 0.034662\n",
            "Epoch 6, Batch 1700/3125, Loss: 0.035193\n",
            "Epoch 6, Batch 1800/3125, Loss: 0.026230\n",
            "Epoch 6, Batch 1900/3125, Loss: 0.034672\n",
            "Epoch 6, Batch 2000/3125, Loss: 0.025259\n",
            "Epoch 6, Batch 2100/3125, Loss: 0.033635\n",
            "Epoch 6, Batch 2200/3125, Loss: 0.032831\n",
            "Epoch 6, Batch 2300/3125, Loss: 0.014412\n",
            "Epoch 6, Batch 2400/3125, Loss: 0.028154\n",
            "Epoch 6, Batch 2500/3125, Loss: 0.034035\n",
            "Epoch 6, Batch 2600/3125, Loss: 0.031486\n",
            "Epoch 6, Batch 2700/3125, Loss: 0.026085\n",
            "Epoch 6, Batch 2800/3125, Loss: 0.026887\n",
            "Epoch 6, Batch 2900/3125, Loss: 0.014758\n",
            "Epoch 6, Batch 3000/3125, Loss: 0.026710\n",
            "Epoch 6, Batch 3100/3125, Loss: 0.047109\n",
            "Train Loss: 0.029421, Val Loss: 0.041409\n",
            "\n",
            "Epoch 7/20\n",
            "--------------------------------------------------\n",
            "Epoch 7, Batch 0/3125, Loss: 0.024137\n",
            "Epoch 7, Batch 100/3125, Loss: 0.065915\n",
            "Epoch 7, Batch 200/3125, Loss: 0.023854\n",
            "Epoch 7, Batch 300/3125, Loss: 0.019450\n",
            "Epoch 7, Batch 400/3125, Loss: 0.018861\n",
            "Epoch 7, Batch 500/3125, Loss: 0.020153\n",
            "Epoch 7, Batch 600/3125, Loss: 0.105465\n",
            "Epoch 7, Batch 700/3125, Loss: 0.032340\n",
            "Epoch 7, Batch 800/3125, Loss: 0.034932\n",
            "Epoch 7, Batch 900/3125, Loss: 0.030704\n",
            "Epoch 7, Batch 1000/3125, Loss: 0.035371\n",
            "Epoch 7, Batch 1100/3125, Loss: 0.025654\n",
            "Epoch 7, Batch 1200/3125, Loss: 0.045477\n",
            "Epoch 7, Batch 1300/3125, Loss: 0.015006\n",
            "Epoch 7, Batch 1400/3125, Loss: 0.016577\n",
            "Epoch 7, Batch 1500/3125, Loss: 0.047740\n",
            "Epoch 7, Batch 1600/3125, Loss: 0.025999\n",
            "Epoch 7, Batch 1700/3125, Loss: 0.009336\n",
            "Epoch 7, Batch 1800/3125, Loss: 0.010236\n",
            "Epoch 7, Batch 1900/3125, Loss: 0.020528\n",
            "Epoch 7, Batch 2000/3125, Loss: 0.022880\n",
            "Epoch 7, Batch 2100/3125, Loss: 0.037671\n",
            "Epoch 7, Batch 2200/3125, Loss: 0.032058\n",
            "Epoch 7, Batch 2300/3125, Loss: 0.049317\n",
            "Epoch 7, Batch 2400/3125, Loss: 0.016598\n",
            "Epoch 7, Batch 2500/3125, Loss: 0.022909\n",
            "Epoch 7, Batch 2600/3125, Loss: 0.013601\n",
            "Epoch 7, Batch 2700/3125, Loss: 0.023953\n",
            "Epoch 7, Batch 2800/3125, Loss: 0.015473\n",
            "Epoch 7, Batch 2900/3125, Loss: 0.021719\n",
            "Epoch 7, Batch 3000/3125, Loss: 0.037612\n",
            "Epoch 7, Batch 3100/3125, Loss: 0.042302\n",
            "Train Loss: 0.028011, Val Loss: 0.017905\n",
            "üéØ New best validation loss: 0.017905\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_7.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 8/20\n",
            "--------------------------------------------------\n",
            "Epoch 8, Batch 0/3125, Loss: 0.007450\n",
            "Epoch 8, Batch 100/3125, Loss: 0.025135\n",
            "Epoch 8, Batch 200/3125, Loss: 0.011351\n",
            "Epoch 8, Batch 300/3125, Loss: 0.026037\n",
            "Epoch 8, Batch 400/3125, Loss: 0.025113\n",
            "Epoch 8, Batch 500/3125, Loss: 0.018579\n",
            "Epoch 8, Batch 600/3125, Loss: 0.026169\n",
            "Epoch 8, Batch 700/3125, Loss: 0.046216\n",
            "Epoch 8, Batch 800/3125, Loss: 0.027773\n",
            "Epoch 8, Batch 900/3125, Loss: 0.029897\n",
            "Epoch 8, Batch 1000/3125, Loss: 0.011663\n",
            "Epoch 8, Batch 1100/3125, Loss: 0.028648\n",
            "Epoch 8, Batch 1200/3125, Loss: 0.013056\n",
            "Epoch 8, Batch 1300/3125, Loss: 0.041887\n",
            "Epoch 8, Batch 1400/3125, Loss: 0.018196\n",
            "Epoch 8, Batch 1500/3125, Loss: 0.026279\n",
            "Epoch 8, Batch 1600/3125, Loss: 0.030061\n",
            "Epoch 8, Batch 1700/3125, Loss: 0.016186\n",
            "Epoch 8, Batch 1800/3125, Loss: 0.066170\n",
            "Epoch 8, Batch 1900/3125, Loss: 0.016187\n",
            "Epoch 8, Batch 2000/3125, Loss: 0.026382\n",
            "Epoch 8, Batch 2100/3125, Loss: 0.046011\n",
            "Epoch 8, Batch 2200/3125, Loss: 0.023776\n",
            "Epoch 8, Batch 2300/3125, Loss: 0.038834\n",
            "Epoch 8, Batch 2400/3125, Loss: 0.027032\n",
            "Epoch 8, Batch 2500/3125, Loss: 0.017011\n",
            "Epoch 8, Batch 2600/3125, Loss: 0.024432\n",
            "Epoch 8, Batch 2700/3125, Loss: 0.030328\n",
            "Epoch 8, Batch 2800/3125, Loss: 0.016996\n",
            "Epoch 8, Batch 2900/3125, Loss: 0.007244\n",
            "Epoch 8, Batch 3000/3125, Loss: 0.035418\n",
            "Epoch 8, Batch 3100/3125, Loss: 0.030292\n",
            "Train Loss: 0.024605, Val Loss: 0.029194\n",
            "\n",
            "Epoch 9/20\n",
            "--------------------------------------------------\n",
            "Epoch 9, Batch 0/3125, Loss: 0.039663\n",
            "Epoch 9, Batch 100/3125, Loss: 0.039109\n",
            "Epoch 9, Batch 200/3125, Loss: 0.024527\n",
            "Epoch 9, Batch 300/3125, Loss: 0.019796\n",
            "Epoch 9, Batch 400/3125, Loss: 0.018767\n",
            "Epoch 9, Batch 500/3125, Loss: 0.021261\n",
            "Epoch 9, Batch 600/3125, Loss: 0.008843\n",
            "Epoch 9, Batch 700/3125, Loss: 0.015428\n",
            "Epoch 9, Batch 800/3125, Loss: 0.018649\n",
            "Epoch 9, Batch 900/3125, Loss: 0.018440\n",
            "Epoch 9, Batch 1000/3125, Loss: 0.029662\n",
            "Epoch 9, Batch 1100/3125, Loss: 0.027889\n",
            "Epoch 9, Batch 1200/3125, Loss: 0.012600\n",
            "Epoch 9, Batch 1300/3125, Loss: 0.019169\n",
            "Epoch 9, Batch 1400/3125, Loss: 0.026998\n",
            "Epoch 9, Batch 1500/3125, Loss: 0.024643\n",
            "Epoch 9, Batch 1600/3125, Loss: 0.011672\n",
            "Epoch 9, Batch 1700/3125, Loss: 0.020393\n",
            "Epoch 9, Batch 1800/3125, Loss: 0.005032\n",
            "Epoch 9, Batch 1900/3125, Loss: 0.025515\n",
            "Epoch 9, Batch 2000/3125, Loss: 0.034663\n",
            "Epoch 9, Batch 2100/3125, Loss: 0.019841\n",
            "Epoch 9, Batch 2200/3125, Loss: 0.017580\n",
            "Epoch 9, Batch 2300/3125, Loss: 0.022315\n",
            "Epoch 9, Batch 2400/3125, Loss: 0.014593\n",
            "Epoch 9, Batch 2500/3125, Loss: 0.021121\n",
            "Epoch 9, Batch 2600/3125, Loss: 0.019188\n",
            "Epoch 9, Batch 2700/3125, Loss: 0.026889\n",
            "Epoch 9, Batch 2800/3125, Loss: 0.031269\n",
            "Epoch 9, Batch 2900/3125, Loss: 0.019632\n",
            "Epoch 9, Batch 3000/3125, Loss: 0.014292\n",
            "Epoch 9, Batch 3100/3125, Loss: 0.023299\n",
            "Train Loss: 0.023475, Val Loss: 0.015973\n",
            "üéØ New best validation loss: 0.015973\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_9.pt\n",
            "‚úì Saved best model: checkpoints/best_model.pt\n",
            "\n",
            "Epoch 10/20\n",
            "--------------------------------------------------\n",
            "Epoch 10, Batch 0/3125, Loss: 0.010241\n",
            "Epoch 10, Batch 100/3125, Loss: 0.010293\n",
            "Epoch 10, Batch 200/3125, Loss: 0.061596\n",
            "Epoch 10, Batch 300/3125, Loss: 0.027205\n",
            "Epoch 10, Batch 400/3125, Loss: 0.007927\n",
            "Epoch 10, Batch 500/3125, Loss: 0.029649\n",
            "Epoch 10, Batch 600/3125, Loss: 0.012650\n",
            "Epoch 10, Batch 700/3125, Loss: 0.015696\n",
            "Epoch 10, Batch 800/3125, Loss: 0.022892\n",
            "Epoch 10, Batch 900/3125, Loss: 0.005287\n",
            "Epoch 10, Batch 1000/3125, Loss: 0.052196\n",
            "Epoch 10, Batch 1100/3125, Loss: 0.008237\n",
            "Epoch 10, Batch 1200/3125, Loss: 0.026764\n",
            "Epoch 10, Batch 1300/3125, Loss: 0.018436\n",
            "Epoch 10, Batch 1400/3125, Loss: 0.013389\n",
            "Epoch 10, Batch 1500/3125, Loss: 0.012231\n",
            "Epoch 10, Batch 1600/3125, Loss: 0.023574\n",
            "Epoch 10, Batch 1700/3125, Loss: 0.013797\n",
            "Epoch 10, Batch 1800/3125, Loss: 0.024348\n",
            "Epoch 10, Batch 1900/3125, Loss: 0.041822\n",
            "Epoch 10, Batch 2000/3125, Loss: 0.006946\n",
            "Epoch 10, Batch 2100/3125, Loss: 0.026819\n",
            "Epoch 10, Batch 2200/3125, Loss: 0.013698\n",
            "Epoch 10, Batch 2300/3125, Loss: 0.039223\n",
            "Epoch 10, Batch 2400/3125, Loss: 0.028997\n",
            "Epoch 10, Batch 2500/3125, Loss: 0.011620\n",
            "Epoch 10, Batch 2600/3125, Loss: 0.036230\n",
            "Epoch 10, Batch 2700/3125, Loss: 0.050919\n",
            "Epoch 10, Batch 2800/3125, Loss: 0.023486\n",
            "Epoch 10, Batch 2900/3125, Loss: 0.028960\n",
            "Epoch 10, Batch 3000/3125, Loss: 0.015258\n",
            "Epoch 10, Batch 3100/3125, Loss: 0.028011\n",
            "Train Loss: 0.022675, Val Loss: 0.025124\n",
            "‚úì Saved checkpoint: checkpoints/model_epoch_10.pt\n",
            "\n",
            "Epoch 11/20\n",
            "--------------------------------------------------\n",
            "Epoch 11, Batch 0/3125, Loss: 0.029540\n",
            "Epoch 11, Batch 100/3125, Loss: 0.010064\n",
            "Epoch 11, Batch 200/3125, Loss: 0.015551\n",
            "Epoch 11, Batch 300/3125, Loss: 0.033820\n",
            "Epoch 11, Batch 400/3125, Loss: 0.031395\n",
            "Epoch 11, Batch 500/3125, Loss: 0.069009\n",
            "Epoch 11, Batch 600/3125, Loss: 0.013906\n",
            "Epoch 11, Batch 700/3125, Loss: 0.023012\n",
            "Epoch 11, Batch 800/3125, Loss: 0.016373\n",
            "Epoch 11, Batch 900/3125, Loss: 0.017769\n",
            "Epoch 11, Batch 1000/3125, Loss: 0.015409\n",
            "Epoch 11, Batch 1100/3125, Loss: 0.018338\n",
            "Epoch 11, Batch 1200/3125, Loss: 0.023409\n",
            "Epoch 11, Batch 1300/3125, Loss: 0.013882\n",
            "Epoch 11, Batch 1400/3125, Loss: 0.022225\n",
            "Epoch 11, Batch 1500/3125, Loss: 0.022906\n",
            "Epoch 11, Batch 1600/3125, Loss: 0.046529\n",
            "Epoch 11, Batch 1700/3125, Loss: 0.037440\n",
            "Epoch 11, Batch 1800/3125, Loss: 0.020295\n",
            "Epoch 11, Batch 1900/3125, Loss: 0.014363\n",
            "Epoch 11, Batch 2000/3125, Loss: 0.004493\n",
            "Epoch 11, Batch 2100/3125, Loss: 0.007023\n",
            "Epoch 11, Batch 2200/3125, Loss: 0.036397\n",
            "Epoch 11, Batch 2300/3125, Loss: 0.016976\n",
            "Epoch 11, Batch 2400/3125, Loss: 0.011999\n",
            "Epoch 11, Batch 2500/3125, Loss: 0.008620\n",
            "Epoch 11, Batch 2600/3125, Loss: 0.012426\n",
            "Epoch 11, Batch 2700/3125, Loss: 0.045261\n",
            "Epoch 11, Batch 2800/3125, Loss: 0.018267\n",
            "Epoch 11, Batch 2900/3125, Loss: 0.012787\n",
            "Epoch 11, Batch 3000/3125, Loss: 0.015919\n",
            "Epoch 11, Batch 3100/3125, Loss: 0.032361\n",
            "Train Loss: 0.021990, Val Loss: 0.025115\n",
            "\n",
            "Epoch 12/20\n",
            "--------------------------------------------------\n",
            "Epoch 12, Batch 0/3125, Loss: 0.028062\n",
            "Epoch 12, Batch 100/3125, Loss: 0.011305\n",
            "Epoch 12, Batch 200/3125, Loss: 0.013637\n",
            "Epoch 12, Batch 300/3125, Loss: 0.095609\n",
            "Epoch 12, Batch 400/3125, Loss: 0.026027\n",
            "Epoch 12, Batch 500/3125, Loss: 0.005448\n",
            "Epoch 12, Batch 600/3125, Loss: 0.024624\n",
            "Epoch 12, Batch 700/3125, Loss: 0.007911\n",
            "Epoch 12, Batch 800/3125, Loss: 0.010546\n",
            "Epoch 12, Batch 900/3125, Loss: 0.028612\n",
            "Epoch 12, Batch 1000/3125, Loss: 0.020431\n",
            "Epoch 12, Batch 1100/3125, Loss: 0.015128\n",
            "Epoch 12, Batch 1200/3125, Loss: 0.055887\n",
            "Epoch 12, Batch 1300/3125, Loss: 0.010631\n",
            "Epoch 12, Batch 1400/3125, Loss: 0.025053\n",
            "Epoch 12, Batch 1500/3125, Loss: 0.016245\n",
            "Epoch 12, Batch 1600/3125, Loss: 0.021383\n",
            "Epoch 12, Batch 1700/3125, Loss: 0.013872\n",
            "Epoch 12, Batch 1800/3125, Loss: 0.026485\n",
            "Epoch 12, Batch 1900/3125, Loss: 0.023597\n",
            "Epoch 12, Batch 2000/3125, Loss: 0.012572\n",
            "Epoch 12, Batch 2100/3125, Loss: 0.017140\n",
            "Epoch 12, Batch 2200/3125, Loss: 0.010265\n",
            "Epoch 12, Batch 2300/3125, Loss: 0.012883\n",
            "Epoch 12, Batch 2400/3125, Loss: 0.008166\n",
            "Epoch 12, Batch 2500/3125, Loss: 0.029819\n",
            "Epoch 12, Batch 2600/3125, Loss: 0.015674\n",
            "Epoch 12, Batch 2700/3125, Loss: 0.032500\n",
            "Epoch 12, Batch 2800/3125, Loss: 0.009142\n",
            "Epoch 12, Batch 2900/3125, Loss: 0.037825\n",
            "Epoch 12, Batch 3000/3125, Loss: 0.008977\n",
            "Epoch 12, Batch 3100/3125, Loss: 0.019964\n",
            "Train Loss: 0.021300, Val Loss: 0.034842\n",
            "\n",
            "Epoch 13/20\n",
            "--------------------------------------------------\n",
            "Epoch 13, Batch 0/3125, Loss: 0.030139\n",
            "Epoch 13, Batch 100/3125, Loss: 0.031938\n",
            "Epoch 13, Batch 200/3125, Loss: 0.026783\n",
            "Epoch 13, Batch 300/3125, Loss: 0.020207\n",
            "Epoch 13, Batch 400/3125, Loss: 0.030153\n",
            "Epoch 13, Batch 500/3125, Loss: 0.040175\n",
            "Epoch 13, Batch 600/3125, Loss: 0.008823\n",
            "Epoch 13, Batch 700/3125, Loss: 0.032108\n",
            "Epoch 13, Batch 800/3125, Loss: 0.016042\n",
            "Epoch 13, Batch 900/3125, Loss: 0.047892\n",
            "Epoch 13, Batch 1000/3125, Loss: 0.022557\n",
            "Epoch 13, Batch 1100/3125, Loss: 0.007735\n",
            "Epoch 13, Batch 1200/3125, Loss: 0.020591\n",
            "Epoch 13, Batch 1300/3125, Loss: 0.038704\n",
            "Epoch 13, Batch 1400/3125, Loss: 0.031572\n",
            "Epoch 13, Batch 1500/3125, Loss: 0.017118\n",
            "Epoch 13, Batch 1600/3125, Loss: 0.017799\n",
            "Epoch 13, Batch 1700/3125, Loss: 0.010482\n",
            "Epoch 13, Batch 1800/3125, Loss: 0.029738\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function for structured inputs with variable-length boids\"\"\"\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for sample_input, sample_target in batch:\n",
        "        inputs.append(sample_input)  # Keep structured format\n",
        "        targets.append(sample_target)\n",
        "\n",
        "    # Stack targets into a tensor\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, num_epochs=50):\n",
        "    global best_val_loss\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{start_epoch + num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch+1)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validate\n",
        "        val_loss = validate_epoch(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        is_best = val_loss < best_val_loss\n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "            print(f\"üéØ New best validation loss: {best_val_loss:.6f}\")\n",
        "\n",
        "        # Save every 5 epochs or if best\n",
        "        if (epoch + 1) % 5 == 0 or is_best:\n",
        "            save_checkpoint(model, optimizer, epoch + 1, val_loss, is_best)\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Create data loaders with custom collate function\n",
        "if os.path.exists(data_path):\n",
        "    dataset = BoidsDataset(data_path)\n",
        "\n",
        "    # Split into train/val\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders with custom collate function\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                             num_workers=0, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                           num_workers=0, collate_fn=custom_collate_fn)\n",
        "\n",
        "    print(f\"Data loaded:\")\n",
        "    print(f\"  Train samples: {len(train_dataset)}\")\n",
        "    print(f\"  Val samples: {len(val_dataset)}\")\n",
        "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"  Custom collate function: ‚úì\")\n",
        "\n",
        "# Run training (modify NUM_EPOCHS as needed)\n",
        "if train_loader is not None and val_loader is not None:\n",
        "    NUM_EPOCHS = 20  # Adjust as needed\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, NUM_EPOCHS)\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot start training: data not loaded\")\n",
        "    train_losses = val_losses = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8fjmSa0mv3n"
      },
      "outputs": [],
      "source": [
        "# Training Progress Visualization and Model Evaluation\n",
        "def plot_training_progress(train_losses, val_losses):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Val Loss', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_losses, label='Val Loss', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation MSE Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"üìä Training Summary:\")\n",
        "    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"  Final val loss: {val_losses[-1]:.6f}\")\n",
        "    print(f\"  Best val loss: {min(val_losses):.6f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, num_samples=100):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for inputs, batch_targets in val_loader:\n",
        "            batch_targets = batch_targets.to(DEVICE)\n",
        "            batch_outputs = model(inputs)\n",
        "\n",
        "            predictions.extend(batch_outputs.cpu().numpy())\n",
        "            targets.extend(batch_targets.cpu().numpy())\n",
        "\n",
        "            count += len(batch_targets)\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "    predictions = np.array(predictions[:num_samples])\n",
        "    targets = np.array(targets[:num_samples])\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = np.mean((predictions - targets) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - targets))\n",
        "\n",
        "    print(f\"üìà Evaluation on {len(predictions)} samples:\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "\n",
        "    # Plot predictions vs targets\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(targets[:, 0], predictions[:, 0], alpha=0.5, color='blue')\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--', label='Perfect prediction')\n",
        "    plt.xlabel('Target X')\n",
        "    plt.ylabel('Predicted X')\n",
        "    plt.title('X Component Prediction')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(targets[:, 1], predictions[:, 1], alpha=0.5, color='green')\n",
        "    plt.plot([-1, 1], [-1, 1], 'r--', label='Perfect prediction')\n",
        "    plt.xlabel('Target Y')\n",
        "    plt.ylabel('Predicted Y')\n",
        "    plt.title('Y Component Prediction')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mse, mae\n",
        "\n",
        "# Plot training progress and evaluate model\n",
        "if train_losses is not None and val_losses is not None:\n",
        "    plot_training_progress(train_losses, val_losses)\n",
        "\n",
        "if val_loader is not None:\n",
        "    mse, mae = evaluate_model(model, val_loader)\n",
        "else:\n",
        "    print(\"‚ùå Cannot evaluate model: validation data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZTPleNCmv3n"
      },
      "outputs": [],
      "source": [
        "# Model Export and Testing\n",
        "def export_to_js(model, output_path=\"policy/transformer/models/trained_model.js\"):\n",
        "    print(f\"üîÑ Exporting model to JavaScript format...\")\n",
        "\n",
        "    # Save PyTorch checkpoint first\n",
        "    checkpoint_path = \"checkpoints/export_checkpoint.pt\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': 'export',\n",
        "        'architecture': {\n",
        "            'd_model': D_MODEL,\n",
        "            'n_heads': N_HEADS,\n",
        "            'n_layers': N_LAYERS,\n",
        "            'ffn_hidden': FFN_HIDDEN\n",
        "        }\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"‚úì Saved checkpoint for export: {checkpoint_path}\")\n",
        "\n",
        "    # Use export_to_js.py script\n",
        "    import subprocess\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"export_to_js.py\",\n",
        "        \"--checkpoint\", checkpoint_path,\n",
        "        \"--output\", output_path\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ Model exported to: {output_path}\")\n",
        "        print(\"üéâ You can now use this model in the browser simulation!\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(f\"‚ùå Export failed:\")\n",
        "        print(result.stderr)\n",
        "\n",
        "    return result.returncode == 0\n",
        "\n",
        "def test_model_inference(model):\n",
        "    model.eval()\n",
        "\n",
        "    # Create a test input\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "\n",
        "    print(f\"üß™ Test inference:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Output: [{output[0]:.4f}, {output[1]:.4f}]\")\n",
        "    print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "    print(f\"  ‚úì Output is properly bounded in [-1, 1]\")\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test the model\n",
        "test_output = test_model_inference(model)\n",
        "\n",
        "# Export the trained model\n",
        "export_success = export_to_js(model)\n",
        "\n",
        "if export_success:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ Model trained and exported to JavaScript\")\n",
        "    print(\"‚úÖ Ready to use in the browser simulation\")\n",
        "    print(\"‚úÖ Check the playground.html to test your model\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n‚ùå Export failed - model trained but not exported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jTu5atsmv3o"
      },
      "outputs": [],
      "source": [
        "# Quick Test of Model\n",
        "def test_model_inference(model):\n",
        "    model.eval()\n",
        "\n",
        "    # Create a test input\n",
        "    test_input = {\n",
        "        'context': {'canvasWidth': 0.8, 'canvasHeight': 0.6},\n",
        "        'predator': {'velX': 0.1, 'velY': -0.2},\n",
        "        'boids': [\n",
        "            {'relX': 0.1, 'relY': 0.3, 'velX': 0.5, 'velY': -0.1},\n",
        "            {'relX': -0.2, 'relY': 0.1, 'velX': -0.3, 'velY': 0.4}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "\n",
        "    print(f\"Test inference:\")\n",
        "    print(f\"  Input: {len(test_input['boids'])} boids\")\n",
        "    print(f\"  Output: [{output[0]:.4f}, {output[1]:.4f}]\")\n",
        "    print(f\"  Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test the model\n",
        "test_output = test_model_inference(model)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}