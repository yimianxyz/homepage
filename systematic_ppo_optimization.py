#!/usr/bin/env python3
"""
Systematic PPO Optimization - Scientific approach to prevent overfitting

RESEARCH OBJECTIVE: Enable longer, more stable PPO training by preventing rapid overfitting
observed at 4 iterations. Focus on hyperparameters that control learning stability.

KEY INSIGHT: 4 iterations being optimal indicates rapid overfitting. We need:
1. Much smaller learning rates (0.00005, 0.00003, 0.00001)
2. PPO regularization (clip_epsilon, entropy_coef)
3. Training stability (gradient clipping, smaller batch sizes)
4. Advanced techniques (learning rate scheduling)

SYSTEMATIC APPROACH:
- Multi-stage optimization (LR -> PPO params -> Regularization)
- Extended training validation (15+ iterations)
- Statistical significance testing
- Overfitting detection and mitigation
"""

import os
import sys
import time
import json
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
from scipy import stats
from itertools import product

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from rl_training import PPOTrainer
from evaluation import PolicyEvaluator
from simulation.random_state_generator import generate_random_state
from policy.transformer.transformer_policy import TransformerPolicy


@dataclass
class PPOConfig:
    """Comprehensive PPO configuration"""
    learning_rate: float
    clip_epsilon: float
    ppo_epochs: int
    entropy_coef: float
    gae_lambda: float
    gamma: float
    max_grad_norm: float
    name: str


@dataclass
class OptimizationResult:
    """Result from PPO configuration test"""
    config: PPOConfig
    peak_performance: float
    peak_iteration: int
    final_performance: float
    final_iteration: int
    stability_score: float  # How stable is training over time
    overfitting_iteration: Optional[int]  # When did overfitting start
    improvement_over_sl: float
    beats_sl_consistently: bool
    learning_curve: List[float]
    training_times: List[float]


class SystematicPPOOptimizer:
    """Scientific PPO optimization with overfitting prevention"""
    
    def __init__(self):
        self.evaluator = PolicyEvaluator()
        self.sl_baseline = None
        self.results = []
        
        print("üß¨ SYSTEMATIC PPO OPTIMIZATION")
        print("=" * 70)
        print("OBJECTIVE: Prevent rapid overfitting, enable stable long-term training")
        print("HYPOTHESIS: Current overfitting at 4 iterations due to aggressive hyperparams")
        print("APPROACH: Multi-stage optimization with extended validation")
        print("=" * 70)
        
        self._establish_sl_baseline()
    
    def _establish_sl_baseline(self):
        """Establish SL baseline quickly"""
        print("\nüìä Establishing SL baseline...")
        sl_policy = TransformerPolicy("checkpoints/best_model.pt")
        result = self.evaluator.evaluate_policy(sl_policy, "SL_Baseline_Quick")
        self.sl_baseline = result.overall_catch_rate
        print(f"‚úÖ SL baseline: {self.sl_baseline:.4f}")
    
    def detect_overfitting(self, performance_curve: List[float], 
                          window_size: int = 3) -> Optional[int]:
        """Detect when overfitting starts based on performance degradation"""
        if len(performance_curve) < window_size + 2:
            return None
        
        # Find peak performance
        peak_idx = performance_curve.index(max(performance_curve))
        
        # Check if performance consistently degrades after peak
        if peak_idx >= len(performance_curve) - window_size:
            return None
        
        # Look for consistent degradation after peak
        post_peak = performance_curve[peak_idx + 1:]
        if len(post_peak) >= window_size:
            recent_avg = np.mean(post_peak[-window_size:])
            peak_perf = performance_curve[peak_idx]
            
            # If recent performance is significantly worse than peak
            if recent_avg < peak_perf * 0.95:  # 5% degradation threshold
                return peak_idx + 1
        
        return None
    
    def calculate_stability_score(self, performance_curve: List[float]) -> float:
        """Calculate training stability (lower variance = higher stability)"""
        if len(performance_curve) < 2:
            return 0.0
        
        # Coefficient of variation (CV) - normalized standard deviation
        mean_perf = np.mean(performance_curve)
        std_perf = np.std(performance_curve)
        
        if mean_perf == 0:
            return 0.0
        
        cv = std_perf / mean_perf
        # Convert to stability score (0-1, higher = more stable)
        stability = max(0.0, 1.0 - cv)
        return stability
    
    def test_ppo_configuration(self, config: PPOConfig, 
                              max_iterations: int = 15, 
                              evaluation_frequency: int = 3) -> OptimizationResult:
        """Test PPO configuration with extended training and overfitting detection"""
        print(f"\nüß™ Testing: {config.name}")
        print(f"   LR: {config.learning_rate}, Clip: {config.clip_epsilon}, Entropy: {config.entropy_coef}")
        print(f"   PPO epochs: {config.ppo_epochs}, GAE Œª: {config.gae_lambda}")
        
        # Initialize trainer with configuration
        trainer = PPOTrainer(
            sl_checkpoint_path="checkpoints/best_model.pt",
            learning_rate=config.learning_rate,
            clip_epsilon=config.clip_epsilon,
            ppo_epochs=config.ppo_epochs,
            rollout_steps=512,
            max_episode_steps=2500,
            gamma=config.gamma,
            gae_lambda=config.gae_lambda,
            device='cpu'
        )
        
        performance_curve = []
        training_times = []
        evaluation_iterations = []
        
        # Extended training with regular evaluation
        for iteration in range(1, max_iterations + 1):
            # Training
            start_time = time.time()
            initial_state = generate_random_state(12, 400, 300)
            trainer.train_iteration(initial_state)
            training_time = time.time() - start_time
            training_times.append(training_time)
            
            # Regular evaluation
            if iteration % evaluation_frequency == 0 or iteration <= 5:
                print(f"   Evaluation at iteration {iteration}...")
                result = self.evaluator.evaluate_policy(trainer.policy, f"{config.name}_iter{iteration}")
                performance = result.overall_catch_rate
                performance_curve.append(performance)
                evaluation_iterations.append(iteration)
                
                improvement = ((performance - self.sl_baseline) / self.sl_baseline) * 100
                status = "‚úÖ BEATS SL" if performance > self.sl_baseline else "‚ùå Below SL"
                print(f"   Iter {iteration}: {performance:.4f} ({improvement:+.1f}%) {status}")
                
                # Early stopping if severe overfitting detected
                overfitting_start = self.detect_overfitting(performance_curve)
                if overfitting_start is not None and len(performance_curve) > overfitting_start + 3:
                    print(f"   ‚ö†Ô∏è  Early stopping: Severe overfitting detected at iteration {evaluation_iterations[overfitting_start]}")
                    break
        
        # Analysis
        if not performance_curve:
            # Fallback evaluation
            result = self.evaluator.evaluate_policy(trainer.policy, f"{config.name}_final")
            performance_curve = [result.overall_catch_rate]
            evaluation_iterations = [max_iterations]
        
        peak_performance = max(performance_curve)
        peak_idx = performance_curve.index(peak_performance)
        peak_iteration = evaluation_iterations[peak_idx] if peak_idx < len(evaluation_iterations) else max_iterations
        
        final_performance = performance_curve[-1]
        final_iteration = evaluation_iterations[-1] if evaluation_iterations else max_iterations
        
        stability_score = self.calculate_stability_score(performance_curve)
        overfitting_iteration = self.detect_overfitting(performance_curve)
        overfitting_iter_num = evaluation_iterations[overfitting_iteration] if overfitting_iteration is not None else None
        
        improvement_over_sl = ((peak_performance - self.sl_baseline) / self.sl_baseline) * 100
        beats_sl_consistently = all(perf > self.sl_baseline for perf in performance_curve)
        
        result = OptimizationResult(
            config=config,
            peak_performance=peak_performance,
            peak_iteration=peak_iteration,
            final_performance=final_performance,
            final_iteration=final_iteration,
            stability_score=stability_score,
            overfitting_iteration=overfitting_iter_num,
            improvement_over_sl=improvement_over_sl,
            beats_sl_consistently=beats_sl_consistently,
            learning_curve=performance_curve,
            training_times=training_times
        )
        
        self.results.append(result)
        
        print(f"   RESULT: Peak {peak_performance:.4f} at iter {peak_iteration}, Final {final_performance:.4f}")
        print(f"   Stability: {stability_score:.3f}, Overfitting: {overfitting_iter_num or 'None'}")
        
        return result
    
    def experiment_1_ultra_conservative_learning_rates(self) -> Dict[str, Any]:
        """Test ultra-conservative learning rates to prevent rapid overfitting"""
        print(f"\n{'='*70}")
        print(f"üß™ EXPERIMENT 1: ULTRA-CONSERVATIVE LEARNING RATES")
        print(f"{'='*70}")
        print(f"Hypothesis: Much smaller LR enables longer stable training")
        
        # Test ultra-conservative learning rates
        learning_rates = [0.00001, 0.00003, 0.00005, 0.0001]  # Much smaller than previous 0.0001
        
        results = []
        for lr in learning_rates:
            config = PPOConfig(
                learning_rate=lr,
                clip_epsilon=0.2,      # Default
                ppo_epochs=2,          # Default
                entropy_coef=0.01,     # Small entropy bonus
                gae_lambda=0.95,       # Default
                gamma=0.99,            # Default
                max_grad_norm=0.5,     # Gradient clipping
                name=f"UltraLR_{lr:.5f}"
            )
            
            result = self.test_ppo_configuration(config, max_iterations=15, evaluation_frequency=3)
            results.append((lr, result))
        
        # Find best learning rate
        best_lr_result = max(results, key=lambda x: x[1].peak_performance)
        best_lr = best_lr_result[0]
        
        print(f"\nüìä ULTRA-CONSERVATIVE LEARNING RATE RESULTS:")
        for lr, result in results:
            overfitting_str = f"@{result.overfitting_iteration}" if result.overfitting_iteration else "None"
            status = "‚úÖ" if result.beats_sl_consistently else "‚ùå"
            print(f"   LR {lr:.5f}: Peak {result.peak_performance:.4f} @{result.peak_iteration}, "
                  f"Stability {result.stability_score:.3f}, Overfit {overfitting_str} {status}")
        
        print(f"\nüèÜ BEST ULTRA-CONSERVATIVE LR: {best_lr:.5f}")
        print(f"   Peak: {best_lr_result[1].peak_performance:.4f} at iteration {best_lr_result[1].peak_iteration}")
        print(f"   Stability: {best_lr_result[1].stability_score:.3f}")
        
        return {
            'best_learning_rate': best_lr,
            'best_result': best_lr_result[1],
            'all_results': results
        }
    
    def experiment_2_ppo_regularization(self, optimal_lr: float) -> Dict[str, Any]:
        """Optimize PPO-specific regularization parameters"""
        print(f"\n{'='*70}")
        print(f"üß™ EXPERIMENT 2: PPO REGULARIZATION OPTIMIZATION")
        print(f"{'='*70}")
        print(f"Hypothesis: Proper regularization prevents overfitting")
        print(f"Using optimal LR: {optimal_lr:.5f}")
        
        # Test different regularization combinations
        configs = [
            # Conservative clipping
            PPOConfig(optimal_lr, 0.1, 2, 0.01, 0.95, 0.99, 0.5, "Conservative_Clip"),
            PPOConfig(optimal_lr, 0.15, 2, 0.01, 0.95, 0.99, 0.5, "Moderate_Clip"), 
            PPOConfig(optimal_lr, 0.2, 2, 0.01, 0.95, 0.99, 0.5, "Standard_Clip"),
            
            # Entropy regularization
            PPOConfig(optimal_lr, 0.15, 2, 0.001, 0.95, 0.99, 0.5, "Low_Entropy"),
            PPOConfig(optimal_lr, 0.15, 2, 0.01, 0.95, 0.99, 0.5, "Med_Entropy"),
            PPOConfig(optimal_lr, 0.15, 2, 0.05, 0.95, 0.99, 0.5, "High_Entropy"),
            
            # PPO epochs (more updates per rollout)
            PPOConfig(optimal_lr, 0.15, 1, 0.01, 0.95, 0.99, 0.5, "1_PPO_Epoch"),
            PPOConfig(optimal_lr, 0.15, 3, 0.01, 0.95, 0.99, 0.5, "3_PPO_Epochs"),
            PPOConfig(optimal_lr, 0.15, 4, 0.01, 0.95, 0.99, 0.5, "4_PPO_Epochs"),
        ]
        
        results = []
        for config in configs:
            result = self.test_ppo_configuration(config, max_iterations=15, evaluation_frequency=3)
            results.append((config.name, result))
        
        # Find best configuration
        best_config_result = max(results, key=lambda x: x[1].peak_performance)
        best_config_name = best_config_result[0]
        best_config = best_config_result[1].config
        
        print(f"\nüìä PPO REGULARIZATION RESULTS:")
        for name, result in results:
            overfitting_str = f"@{result.overfitting_iteration}" if result.overfitting_iteration else "None"
            status = "‚úÖ" if result.beats_sl_consistently else "‚ùå"
            print(f"   {name:<15}: Peak {result.peak_performance:.4f} @{result.peak_iteration}, "
                  f"Stab {result.stability_score:.3f}, Overfit {overfitting_str} {status}")
        
        print(f"\nüèÜ BEST REGULARIZATION CONFIG: {best_config_name}")
        print(f"   Configuration: {best_config}")
        
        return {
            'best_config': best_config,
            'best_result': best_config_result[1],
            'all_results': results
        }
    
    def experiment_3_advanced_techniques(self, optimal_config: PPOConfig) -> Dict[str, Any]:
        """Test advanced overfitting prevention techniques"""
        print(f"\n{'='*70}")
        print(f"üß™ EXPERIMENT 3: ADVANCED OVERFITTING PREVENTION")
        print(f"{'='*70}")
        print(f"Testing: Learning rate decay, stronger regularization, early stopping")
        
        # Test advanced configurations
        configs = [
            # Stronger gradient clipping
            PPOConfig(optimal_config.learning_rate, optimal_config.clip_epsilon, 
                     optimal_config.ppo_epochs, optimal_config.entropy_coef,
                     optimal_config.gae_lambda, optimal_config.gamma, 0.1, "Strong_GradClip"),
            
            # Higher entropy bonus
            PPOConfig(optimal_config.learning_rate, optimal_config.clip_epsilon,
                     optimal_config.ppo_epochs, 0.1,  # Higher entropy
                     optimal_config.gae_lambda, optimal_config.gamma, optimal_config.max_grad_norm,
                     "High_Entropy_Reg"),
            
            # Very conservative clipping
            PPOConfig(optimal_config.learning_rate, 0.05,  # Very small clip
                     optimal_config.ppo_epochs, optimal_config.entropy_coef,
                     optimal_config.gae_lambda, optimal_config.gamma, optimal_config.max_grad_norm,
                     "Ultra_Conservative"),
            
            # Baseline optimal for comparison
            PPOConfig(optimal_config.learning_rate, optimal_config.clip_epsilon,
                     optimal_config.ppo_epochs, optimal_config.entropy_coef,
                     optimal_config.gae_lambda, optimal_config.gamma, optimal_config.max_grad_norm,
                     "Optimal_Baseline")
        ]
        
        results = []
        for config in configs:
            result = self.test_ppo_configuration(config, max_iterations=20, evaluation_frequency=2)  # Longer test
            results.append((config.name, result))
        
        # Find best advanced configuration
        best_advanced_result = max(results, key=lambda x: x[1].stability_score * x[1].peak_performance)  # Balance performance and stability
        
        print(f"\nüìä ADVANCED TECHNIQUES RESULTS:")
        for name, result in results:
            overfitting_str = f"@{result.overfitting_iteration}" if result.overfitting_iteration else "None"
            combined_score = result.stability_score * result.peak_performance
            status = "‚úÖ" if result.beats_sl_consistently else "‚ùå"
            print(f"   {name:<18}: Peak {result.peak_performance:.4f}, Stab {result.stability_score:.3f}, "
                  f"Combined {combined_score:.3f}, Overfit {overfitting_str} {status}")
        
        print(f"\nüèÜ BEST ADVANCED CONFIG: {best_advanced_result[0]}")
        
        return {
            'best_config': best_advanced_result[1].config,
            'best_result': best_advanced_result[1],
            'all_results': results
        }
    
    def run_systematic_optimization(self) -> Dict[str, Any]:
        """Run complete systematic PPO optimization"""
        print(f"\nüöÄ SYSTEMATIC PPO OPTIMIZATION")
        print(f"Goal: Prevent rapid overfitting, enable stable long-term training")
        
        start_time = time.time()
        
        # Stage 1: Ultra-conservative learning rates
        exp1_results = self.experiment_1_ultra_conservative_learning_rates()
        optimal_lr = exp1_results['best_learning_rate']
        
        # Stage 2: PPO regularization
        exp2_results = self.experiment_2_ppo_regularization(optimal_lr)
        optimal_config = exp2_results['best_config']
        
        # Stage 3: Advanced techniques
        exp3_results = self.experiment_3_advanced_techniques(optimal_config)
        final_optimal_config = exp3_results['best_config']
        final_result = exp3_results['best_result']
        
        total_time = time.time() - start_time
        
        # Final validation with very long training
        print(f"\nüéØ FINAL VALIDATION WITH EXTENDED TRAINING (25 iterations)")
        final_validation = self.test_ppo_configuration(final_optimal_config, max_iterations=25, evaluation_frequency=2)
        
        print(f"\n{'='*80}")
        print(f"üèÜ SYSTEMATIC PPO OPTIMIZATION COMPLETE")
        print(f"{'='*80}")
        print(f"SL Baseline:                    {self.sl_baseline:.4f}")
        print(f"Final Peak Performance:         {final_validation.peak_performance:.4f}")
        print(f"Final Peak Iteration:           {final_validation.peak_iteration}")
        print(f"Training Stability Score:       {final_validation.stability_score:.3f}")
        print(f"Overfitting Start:              {final_validation.overfitting_iteration or 'None detected'}")
        print(f"Improvement over SL:            {final_validation.improvement_over_sl:+.1f}%")
        print(f"Beats SL Consistently:          {'‚úÖ Yes' if final_validation.beats_sl_consistently else '‚ùå No'}")
        print(f"")
        print(f"üîß FINAL OPTIMAL CONFIGURATION:")
        print(f"   Learning Rate:     {final_optimal_config.learning_rate:.6f}")
        print(f"   Clip Epsilon:      {final_optimal_config.clip_epsilon}")
        print(f"   PPO Epochs:        {final_optimal_config.ppo_epochs}")
        print(f"   Entropy Coef:      {final_optimal_config.entropy_coef}")
        print(f"   GAE Lambda:        {final_optimal_config.gae_lambda}")
        print(f"   Max Grad Norm:     {final_optimal_config.max_grad_norm}")
        print(f"")
        print(f"Total optimization time: {total_time/60:.1f} minutes")
        
        # Save comprehensive results
        optimization_results = {
            'sl_baseline': self.sl_baseline,
            'final_validation': {
                'peak_performance': final_validation.peak_performance,
                'peak_iteration': final_validation.peak_iteration,
                'stability_score': final_validation.stability_score,
                'overfitting_iteration': final_validation.overfitting_iteration,
                'improvement_over_sl': final_validation.improvement_over_sl,
                'beats_sl_consistently': final_validation.beats_sl_consistently,
                'learning_curve': final_validation.learning_curve
            },
            'optimal_configuration': {
                'learning_rate': final_optimal_config.learning_rate,
                'clip_epsilon': final_optimal_config.clip_epsilon,
                'ppo_epochs': final_optimal_config.ppo_epochs,
                'entropy_coef': final_optimal_config.entropy_coef,
                'gae_lambda': final_optimal_config.gae_lambda,
                'gamma': final_optimal_config.gamma,
                'max_grad_norm': final_optimal_config.max_grad_norm
            },
            'experiment_1_learning_rates': exp1_results,
            'experiment_2_regularization': exp2_results,
            'experiment_3_advanced': exp3_results,
            'total_time_minutes': total_time/60
        }
        
        with open('systematic_ppo_optimization_results.json', 'w') as f:
            json.dump(optimization_results, f, indent=2, default=str)
        
        print(f"‚úÖ Results saved: systematic_ppo_optimization_results.json")
        
        return optimization_results


def main():
    """Run systematic PPO optimization"""
    print("üß¨ SYSTEMATIC PPO OPTIMIZATION")
    print("=" * 70)
    print("PROBLEM: Rapid overfitting at 4 iterations")
    print("SOLUTION: Systematic hyperparameter optimization")
    print("APPROACH:")
    print("  1. Ultra-conservative learning rates")
    print("  2. PPO regularization optimization") 
    print("  3. Advanced overfitting prevention")
    print("  4. Extended training validation")
    print("=" * 70)
    
    optimizer = SystematicPPOOptimizer()
    results = optimizer.run_systematic_optimization()
    
    if results['final_validation']['beats_sl_consistently']:
        print(f"\nüéâ BREAKTHROUGH: Overfitting problem solved!")
        print(f"   Stable training enabled up to iteration {results['final_validation']['peak_iteration']}")
        print(f"   Performance: {results['final_validation']['improvement_over_sl']:+.1f}% over SL")
    else:
        print(f"\nüìä Optimization complete: Best configuration identified")
        print(f"   Further research needed for consistent SL beating")
    
    return results


if __name__ == "__main__":
    main()